{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "059756b7",
      "metadata": {},
      "source": [
        "# Deploy on LangGraph Cloud\n",
        "\n",
        "- Author: [JoonHo Kim](https://github.com/jhboyo)\n",
        "- Design: []()\n",
        "- Peer Review :\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/06-DocumentLoader/04-CSV-Loader.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/06-DocumentLoader/04-CSV-Loader.ipynb)\n",
        "\n",
        "\n",
        "## Overview\n",
        "**LangGraph Cloud** is a cloud-based framework designed to simplify the development, deployment, and management of graph-based workflows for AI applications. It extends the functionality of LangGraph by providing a scalable, distributed, and user-friendly environment to build complex AI agents, workflows, and pipelines.\n",
        "\n",
        "With **LangGraph Cloud**, you can:\n",
        "- Handle large workloads with horizontally-scaling servers, task queues, and built-in persistence\n",
        "- Debug agent failure modes and quickly iterate in a visual playground-like studio\n",
        "- Deploy in one-click and get integrated tracing & monitoring in LangSmith\n",
        "\n",
        "This tutorial will guide you through the key features and components of LangGraph Cloud, including:\n",
        "- Setting up **LangGraph Cloud**: How to create an account, configure your workspace, and deploy your first workflow.\n",
        "- Building AI Workflows: Defining and deploying workflows using LangGraph Cloud.\n",
        "- Managing and monitoring workflows: Tools for debugging, logging, and performance optimization.\n",
        "- Integration with External Services: Connecting LangGraph Cloud to APIs, and other tools.\n",
        "\n",
        "By the end of this tutorial, you will be equipped with the knowledge to effectively utilize **LangGraph Cloud** for building and managing AI workflows in a scalable and efficient manner.\n",
        "\n",
        "Now, let's dive in and explore how to boost performance with **LangGraph Cloud**! üöÄ\n",
        "\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Setting up a new repository on GitHub](#setting-up-a-new-repository-on-github)\n",
        "- [Deployment to LangGraph Cloud](#deployment-to-langgraph-cloud)\n",
        "- [Using LangGraph Studio on the web](#using-langgraph-studio-on-the-web)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Deploy on LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/quick_start/#deploy-to-langgraph-cloud)\n",
        "- [React Agent](https://github.com/langchain-ai/react-agent)\n",
        "- [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#cloud-studio)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "923a83b4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8e8ad1ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
            "  File \"/var/folders/yj/cyt3mklj4xq6yzx3zrch7yz80000gp/T/ipykernel_57942/3784546254.py\", line 1, in <module>\n",
            "    get_ipython().run_line_magic('pip', 'install -qU langchain-opentutorial')\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2480, in run_line_magic\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/magics/packaging.py\", line 105, in pip\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 657, in system_piped\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/utils/_process_posix.py\", line 125, in system\n",
            "ModuleNotFoundError: No module named 'pexpect'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
            "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
            "  File \"/Users/joonhokim/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-nHTobcyW-py3.11/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
            "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -qU langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446f6eba",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"ANTHROPIC_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"false\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"LangGraph-Cloud\",\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70361e4a",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langchain\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain_experimental\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain-fireworks\",\n",
        "        \"langgraph\",\n",
        "        \"tavily-python\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5877653",
      "metadata": {},
      "source": [
        "\n",
        "## Prerequisites\n",
        "Before we start, ensure we have the following:\n",
        "\n",
        "- [GitHub Account](https://github.com/join)\n",
        "- [LangSmith API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key/)\n",
        "- [Anthropic API key](https://console.anthropic.com/settings/keys)\n",
        "- [Tavily API key](https://app.tavily.com/home)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16944702",
      "metadata": {},
      "source": [
        "## Setting up a new repository on GitHub\n",
        "\n",
        "To deploy a **LangGraph** application on **LangGraph Cloud**, your application's code must be stored in a **GitHub** repository. \n",
        "You can deploy any **LangGraph** applications to **LangGraph Cloud** with ease. \n",
        "\n",
        "For this guide, we'll use the pre-built **Python** [`ReAct Agent`](https://github.com/langchain-ai/react-agent) template. You can go to **GitHub** and fork the repository.\n",
        "This [`ReAct Agent`](https://github.com/langchain-ai/react-agent) application requires API keys from **Anthropic** and **Tavily**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d692d28f",
      "metadata": {},
      "source": [
        "## Deployment to LangGraph Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69985a2a",
      "metadata": {},
      "source": [
        "1. After logging in **[LangSmith](https://smith.langchain.com)**, you can click **LangGraph Platform** menu at the bottom of the left sidebar.\n",
        "\n",
        "\n",
        "    <img src=\"./assets/12-LangGraph-cloud-sidebar-menu.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Click **+ New Deployment** button at the bottom of the page and then you can follow the steps belowfor creating a new deployment.\n",
        "    \n",
        "    * Select **Github react-agent** repository from the drop-down menu.\n",
        "    * Write the deployment name in **Name** field.\n",
        "    * Select Git branch. `main` is default.\n",
        "    * Langgraph config file is `langgraph.json` as default. You can select another file.\n",
        "    * Select **Development type**.\n",
        "    * Write **Environment Variables**. In this tutorial, we will use **ANTHROPIC_API_KEY** and **TAVILY_API_KEY**.\n",
        "    * Click **Submit** button at the upper right corner. It takes a few minutes to build the application.\n",
        "\n",
        "    <img src=\"./assets/12-LangGraph-cloud-create-deployment-1.png\" width=\"1150\">\n",
        "        \n",
        "\n",
        "\n",
        "3. Now you can see the deployment status on the **Overview** section.\n",
        "\n",
        "    <img src=\"./assets/12-LangGraph-cloud-deployment-status.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d38567d",
      "metadata": {},
      "source": [
        "## Using LangGraph Studio on the web"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e081f153",
      "metadata": {},
      "source": [
        "Once your application is deployed, you can test it in **LangGraph Studio** in **LangGraph Platform**.\n",
        "\n",
        "You can find the **LangGraph Studio** text and **Endpoint URL** at the bottom of the page. Let's click the **LangGraph Studio** text top copy the clipboard.\n",
        "\n",
        "<img src=\"./assets/12-LangGraph-cloud-langgraph-platform.png\" width=\"1150\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77254676",
      "metadata": {},
      "source": [
        "Now you can test your LangGraph application in **LangGraph Studio** on the web.\n",
        "\n",
        "<img src=\"./assets/12-LangGraph-cloud-langgraph-studio.png\" width=\"1150\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dded6a30",
      "metadata": {},
      "source": [
        "##  Testing the API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0415f5c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "949b65e4",
      "metadata": {},
      "source": [
        "If you append `/docs` to the end of the **Endpoint URL** and enter it in a web browser, you can check the web API.\n",
        "\n",
        "<img src=\"./assets/12-LangGraph-cloud-web-api-1.png\" width=\"650\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c9afc5",
      "metadata": {},
      "source": [
        "Now, you can refer to this document and use API testing tools like Postman or Scalar to conduct tests.\n",
        "\n",
        "`GET https://{{endpoint_url}}threads/{{thread_id}}/history`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1883a36",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea774db",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Creating an AI message object with multiple tool calls\n",
        "message_with_multiple_tool_calls = AIMessage(\n",
        "    content=\"\",\n",
        "    tool_calls=[\n",
        "        {\n",
        "            \"name\": \"search_news\",\n",
        "            \"args\": {\"query\": \"Tesla new model\"},\n",
        "            \"id\": \"tool_call_id_1\",\n",
        "            \"type\": \"tool_call\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"python_code_interpreter\",\n",
        "            \"args\": {\"code\": \"print(1+2+3+4)\"},\n",
        "            \"id\": \"tool_call_id_2\",\n",
        "            \"type\": \"tool_call\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Invoke multiple tool calls with created message\n",
        "tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d079b5b9",
      "metadata": {},
      "source": [
        "## Using with LLMs\n",
        "\n",
        " To use chat models with tool calling, we need to first ensure that the model is aware of the available tools. \n",
        " LangChain provides various chat models of different providers such as `OpenAI GPT`, `Anthropic Claude`, `Google Gemini` and more.\n",
        " You can visit [LangChain ChatModels](https://python.langchain.com/docs/integrations/chat/) for more details.\n",
        " \n",
        "In this tutorial, We do this by calling `.bind_tools` method on `ChatOpenAI` model.\n",
        "This can be done by calling the `.bind_tools` method on the `ChatOpenAI` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af53996f",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize LLM model and bind tools\n",
        "model_with_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4d2811",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "model_with_tools.invoke(\n",
        "    \"Write Python code to print the first 5 prime numbers.\"\n",
        ").tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13a07e4",
      "metadata": {},
      "source": [
        "As you can see, the AI message generated by the chat model already has `tool_calls` populated, so we can just pass it directly to `ToolNode`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5dc7d5",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Processing messages through ToolNode and generating tool-based responses from LLM model\n",
        "tool_node.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            model_with_tools.invoke(\n",
        "                \"Write Python code to print the first 5 prime numbers.\"\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1fb0371",
      "metadata": {},
      "source": [
        "## Using with Agent\n",
        "\n",
        "Next, let's explore how to use `ToolNode` within a `LangGraph`' graph.\n",
        "\n",
        "We will set up an [`ReAct Agent`](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-implementation)'s graph implementation. This agent takes a query as input and repeatedly invokes tools until it gathers enough information to resolve the query.\n",
        "\n",
        "Before we start, let's define a function to visualize the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd4e3ba",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        " # Graph visualization\n",
        "def visualize_graph(graph, xray=False):\n",
        "    \"\"\"\n",
        "    Visualizes and displays a CompiledStateGraph object.\n",
        "    \n",
        "    This function converts and displays the given graph object as a PNG image in Mermaid format if it is an instance of CompiledStateGraph.\n",
        "\n",
        "    Args:\n",
        "        graph: The graph object to visualize. It must be an instance of CompiledStateGraph.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        Exception: Outputs an exception if an error occurs during graph visualization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(graph, CompiledStateGraph):\n",
        "            display(\n",
        "                Image(\n",
        "                    graph.get_graph(xray=xray).draw_mermaid_png(\n",
        "                        background_color=\"white\",\n",
        "                        node_colors=None,\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Visualize Graph Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6d4b13",
      "metadata": {},
      "source": [
        "The `ToolNode` and `OpenAI` model will be used together with the tools we just defined.\n",
        "\n",
        "Let's build a graph with the following steps.\n",
        "1. Use LLM model to process messages and generate responses, return responses with tool calls.\n",
        "2. Initialize workflow graph based on message state.\n",
        "3. Define the two nodes we will cycle between agent and tools.\n",
        "4. Connect the workflow starting point to the agent node.\n",
        "5. Set up conditional branching from the agent node, connecting to a tool node or an endpoint.\n",
        "6. Set up circular edges between the tool node and the agent node.\n",
        "7. Connect the agent node to the end point.\n",
        "8. Compile the defined workflow graph and create an executable application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1920a041",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Import types for LangGraph workflow state and message processing\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "\n",
        "\n",
        "# 1. Use LLM model to process messages and generate responses, return responses with tool calls\n",
        "def call_model(state: MessagesState):\n",
        "    messages = state[\"messages\"]\n",
        "    response = model_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# 2. Initialize workflow graph based on message state\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# 3. Define the two nodes we will cycle between agent and tools\n",
        "# Add agent and tools nodes to the workflow graph\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# 4. Connect the workflow starting point to the agent node\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# 5.Set up conditional branching from the agent node, connecting to a tool node or an endpoint\n",
        "workflow.add_conditional_edges(\"agent\", tools_condition)\n",
        "\n",
        "# 6. Set up circular edges between the tool node and the agent node\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# 7. Connect the agent node to the end point\n",
        "workflow.add_edge(\"agent\", END)\n",
        "\n",
        "# 8. Compile the defined workflow graph and create an executable application\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7b9e45",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "visualize_graph(app)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e14778c",
      "metadata": {},
      "source": [
        "Let's try to run the graph with different queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8e843e",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "for chunk in app.stream(\n",
        "    {\"messages\": [(\"human\", \"Write Python code to print the first 5 prime numbers.\")]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    # Print the last message\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ce609d",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Search query\n",
        "for chunk in app.stream(\n",
        "    {\"messages\": [(\"human\", \"What is most played BTS song?\")]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c5c75d",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ Jupyter <a href='command:jupyter.viewOutput'>Î°úÍ∑∏</a>Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî."
          ]
        }
      ],
      "source": [
        "# Query that does not require tool call\n",
        "for chunk in app.stream(\n",
        "    {\"messages\": [(\"human\", \"Hi, How's it going?\")]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d39c30b4",
      "metadata": {},
      "source": [
        "`ToolNode` can also handle errors that occur during tool execution.\n",
        "\n",
        "You can enable/disable this feature by setting `handle_tool_errors=True` (enabled by default)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-nHTobcyW-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
