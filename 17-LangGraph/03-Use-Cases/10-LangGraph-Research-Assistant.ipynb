{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# LangGraph : Research Assistant with STORM\n",
        "\n",
        "- Author: [Secludor](https://github.com/Secludor)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Research is often a labor-intensive task delegated to analysts, but AI holds tremendous potential to revolutionize this process. This tutorial explores **how to construct a customized AI-powered research and report generation workflow** using `LangGraph`, incorporating key concepts from Stanford's STORM framework.\n",
        "\n",
        "### Why This Approach?\n",
        "The STORM methodology has demonstrated significant improvements in research quality through two key innovations:\n",
        "- Outline creation through querying similar topics enhances coverage.\n",
        "- Multi-perspective conversation simulation increases reference usage and information density.\n",
        "\n",
        "The translation is accurate but can be enhanced for better clarity and technical precision. Here's the reviewed version:\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Core Themes**\n",
        "- Memory: State management and persistence across interactions.\n",
        "- Human-in-the-loop: Interactive feedback and validation mechanisms.\n",
        "- Controllability: Fine-grained control over agent workflows.\n",
        "\n",
        "**Research Framework**\n",
        "- Research Automation Objective: Building customized research processes tailored to user requirements.\n",
        "- Source Management: Strategic selection and integration of research input sources.\n",
        "- Planning Framework: Topic definition and AI analyst team assembly.\n",
        "\n",
        "### Process Implementation\n",
        "\n",
        "**Execution Flow**\n",
        "- LLM Integration: Conducting comprehensive expert AI interviews.\n",
        "- Parallel Processing: Simultaneous information gathering and interview execution.\n",
        "- Output Synthesis: Integration of research findings into comprehensive reports.\n",
        "\n",
        "**Technical Implementation**\n",
        "- Environment Setup: Configuration of runtime environment and API authentication.\n",
        "- Analyst Development: Human-supervised analyst creation and validation process.\n",
        "- Interview Management: Systematic question generation and response collection.\n",
        "- Parallel Processing: Implementation of Map-Reduce for interview parallelization.\n",
        "- Report Generation: Structured composition of introductory and concluding sections.\n",
        "\n",
        "\n",
        "AI has significant potential to support these research processes. However, research requires customization. Raw LLM outputs are often not suitable for real decision-making workflows.\n",
        "\n",
        "A customized AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflow is a promising solution to address this issue.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Utilities](#utilities)\n",
        "- [Analysts Generation : Human in the Loop](#analysts-generation--human-in-the-loop)\n",
        "- [Interview Execution](#interview-execution)\n",
        "- [Report Writing](#report-writing)\n",
        "\n",
        "### References\n",
        "\n",
        "- [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag)\n",
        "- [LangGraph `Send()`](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)\n",
        "- [LangGraph - Multi-Agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/#custom-multi-agent-workflow)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d6fac1c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "# set the project name same as the title\n",
        "set_env(\n",
        "    {\n",
        "        \"LANGCHAIN_PROJECT\": \"LangGraph-Research-Assistant\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Utilities\n",
        "\n",
        "These are the materials needed for the practice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c569a10",
      "metadata": {},
      "source": [
        "### `get_model_name`\n",
        "for getting recent model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d905fdfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class LLMs(Enum):\n",
        "    GPT4o_MINI = \"gpt-4o-mini\"\n",
        "    GPT4o = \"gpt-4o\"\n",
        "    GPT4 = GPT4o_MINI\n",
        "\n",
        "    O1_PREVIEW = \"o1-preview\"\n",
        "    O1_MINI = \"o1-mini\"\n",
        "    O1 = O1_MINI\n",
        "\n",
        "    CLAUDE_SONNET = \"claude-3-5-sonnet-20241022\"\n",
        "    CLAUDE_HAIKU = \"claude-3-5-haiku-20241022\"\n",
        "    CLAUDE = CLAUDE_SONNET\n",
        "\n",
        "    UPSTAGE_SOLAR_MINI = \"solar-mini\"\n",
        "    UPSTAGE_SOLAR_PRO = \"solar-pro\"\n",
        "    UPSTAGE = UPSTAGE_SOLAR_PRO\n",
        "\n",
        "\n",
        "class Embeddings(Enum):\n",
        "    OPENAI_EMBEDDING_SMALL = \"text-embedding-3-small\"\n",
        "    OPENAI_EMBEDDING_LARGE = \"text-embedding-3-large\"\n",
        "    OPENAI_EMBEDDING = OPENAI_EMBEDDING_SMALL\n",
        "\n",
        "    UPSTAGE_EMBEDDING_QUERY = \"embedding-query\"\n",
        "    UPSTAGE_EMBEDDING_PASSAGE = \"embedding-passage\"\n",
        "    UPSTAGE_EMBEDDING = UPSTAGE_EMBEDDING_PASSAGE\n",
        "\n",
        "\n",
        "def get_model_name(model: LLMs | Embeddings) -> str:\n",
        "    \"\"\"\n",
        "    Extracts and returns the final value of the model from the Enum class.\n",
        "\n",
        "    Args:\n",
        "    model (LLMs | Embeddings): An instance of the LLMs or Embeddings Enum class.\n",
        "\n",
        "    Returns:\n",
        "    str: The final value of the model. Returns None if the Enum is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # value가 Enum 멤버인 경우 최종 값을 반환\n",
        "        current_value = model.value\n",
        "        while isinstance(current_value, Enum):\n",
        "            current_value = current_value.value\n",
        "        return current_value\n",
        "    except AttributeError:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652f3f62",
      "metadata": {},
      "source": [
        "### `visualize_graph`\n",
        "for visualizing graph structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "69cb77da",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class NodeStyles:\n",
        "    default: str = (\n",
        "        \"fill:#45C4B0, fill-opacity:0.3, color:#23260F, stroke:#45C4B0, stroke-width:1px, font-weight:bold, line-height:1.2\"  # 기본 색상\n",
        "    )\n",
        "    first: str = (\n",
        "        \"fill:#45C4B0, fill-opacity:0.1, color:#23260F, stroke:#45C4B0, stroke-width:1px, font-weight:normal, font-style:italic, stroke-dasharray:2,2\"  # 점선 테두리\n",
        "    )\n",
        "    last: str = (\n",
        "        \"fill:#45C4B0, fill-opacity:1, color:#000000, stroke:#45C4B0, stroke-width:1px, font-weight:normal, font-style:italic, stroke-dasharray:2,2\"  # 점선 테두리\n",
        "    )\n",
        "\n",
        "\n",
        "def visualize_graph(graph, xray=False):\n",
        "    \"\"\"\n",
        "    Visualizes and displays the CompiledStateGraph object.\n",
        "\n",
        "    This function converts the given graph object into a PNG image in Mermaid format and displays it, provided the graph is an instance of CompiledStateGraph.\n",
        "\n",
        "    Args:\n",
        "    graph: The graph object to visualize. It must be an instance of CompiledStateGraph.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "\n",
        "    Raises:\n",
        "    Exception: Raises an exception if an error occurs during the graph visualization process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(graph, CompiledStateGraph):\n",
        "            display(\n",
        "                Image(\n",
        "                    graph.get_graph(xray=xray).draw_mermaid_png(\n",
        "                        background_color=\"white\",\n",
        "                        node_colors=NodeStyles(),\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Visualize Graph Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc51d6cb",
      "metadata": {},
      "source": [
        "### `random_uuid` , `invoke_graph`\n",
        "- `random_uuid` : for generating a random UUID (Universally Unique Identifier) and returns it as a string.\n",
        "- `invoke_graph` : for streaming and displays the results of executing a CompiledStateGraph instance in a visually appealing format. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3fcd0c04",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Callable\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "import uuid\n",
        "\n",
        "\n",
        "def random_uuid():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "\n",
        "def invoke_graph(\n",
        "    graph: CompiledStateGraph,\n",
        "    inputs: dict,\n",
        "    config: RunnableConfig,\n",
        "    node_names: List[str] = [],\n",
        "    callback: Callable = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    A function that streams and outputs the results of executing the LangGraph app in a visually appealing way.\n",
        "\n",
        "    Args:\n",
        "        graph (CompiledStateGraph): The compiled LangGraph object to execute.\n",
        "        inputs (dict): A dictionary of input values to pass to the graph.\n",
        "        config (RunnableConfig): The execution configuration.\n",
        "        node_names (List[str], optional): A list of node names to output. Defaults to an empty list.\n",
        "        callback (Callable, optional): A callback function for processing each chunk. Defaults to None.\n",
        "            The callback function receives a dictionary in the format {\"node\": str, \"content\": str} as an argument.\n",
        "\n",
        "    Returns:\n",
        "        None: The function only outputs the streaming results and does not return a value.\n",
        "    \"\"\"\n",
        "\n",
        "    def format_namespace(namespace):\n",
        "        return namespace[-1].split(\":\")[0] if len(namespace) > 0 else \"root graph\"\n",
        "\n",
        "    # Include output from subgraphs by setting subgraphs=True\n",
        "    for namespace, chunk in graph.stream(\n",
        "        inputs, config, stream_mode=\"updates\", subgraphs=True\n",
        "    ):\n",
        "        for node_name, node_chunk in chunk.items():\n",
        "            # Filter only if node_names is not empty\n",
        "            if len(node_names) > 0 and node_name not in node_names:\n",
        "                continue\n",
        "\n",
        "            # Execute the callback function if it exists\n",
        "            if callback is not None:\n",
        "                callback({\"node\": node_name, \"content\": node_chunk})\n",
        "            # Default output if no callback is provided\n",
        "            else:\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                formatted_namespace = format_namespace(namespace)\n",
        "                if formatted_namespace == \"root graph\":\n",
        "                    print(f\"🔄 Node: \\033[1;36m{node_name}\\033[0m 🔄\")\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"🔄 Node: \\033[1;36m{node_name}\\033[0m in [\\033[1;33m{formatted_namespace}\\033[0m] 🔄\"\n",
        "                    )\n",
        "                print(\"- \" * 25)\n",
        "\n",
        "                # 노드의 청크 데이터 출력\n",
        "                if isinstance(node_chunk, dict):\n",
        "                    for k, v in node_chunk.items():\n",
        "                        if isinstance(v, BaseMessage):\n",
        "                            v.pretty_print()\n",
        "                        elif isinstance(v, list):\n",
        "                            for list_item in v:\n",
        "                                if isinstance(list_item, BaseMessage):\n",
        "                                    list_item.pretty_print()\n",
        "                                else:\n",
        "                                    print(list_item)\n",
        "                        elif isinstance(v, dict):\n",
        "                            for node_chunk_key, node_chunk_value in node_chunk.items():\n",
        "                                print(f\"{node_chunk_key}:\\n{node_chunk_value}\")\n",
        "                        else:\n",
        "                            print(f\"\\033[1;32m{k}\\033[0m:\\n{v}\")\n",
        "                else:\n",
        "                    if node_chunk is not None:\n",
        "                        for item in node_chunk:\n",
        "                            print(item)\n",
        "                print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c683ef59",
      "metadata": {},
      "source": [
        "### `TabilySearch`\n",
        "\n",
        "This code defines a tool for performing search queries using the Tavily Search API. It includes input validation, formatting of search results, and the ability to customize search parameters.\n",
        "**Methods** :\n",
        "- `__init__` : Initializes the TavilySearch instance, setting up the API client and input parameters.\n",
        "- `_run` : Implements the base tool's run method, calling the search method and returning results.\n",
        "- `search` : Performs the actual search using the Tavily API, taking various optional parameters to customize the query. It formats the output based on user preferences.\n",
        "- `get_search_context` : Retrieves relevant context based on a search query, returning a JSON string that includes search results formatted as specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "83d75ebb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "from tavily import TavilyClient\n",
        "from typing import Literal, Sequence, Optional\n",
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class TavilySearchInput(BaseModel):\n",
        "    \"\"\"Input for the Tavily tool.\"\"\"\n",
        "\n",
        "    query: str = Field(description=\"Search Query\")\n",
        "\n",
        "\n",
        "def format_search_result(result: dict, include_raw_content: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Utility functions for formatting search results.\n",
        "\n",
        "    Args:\n",
        "        result (dict): Original search results\n",
        "\n",
        "    Returns:\n",
        "        str: XML formatted search results\n",
        "    \"\"\"\n",
        "    # use json.dumps() to treat non-english encoding\n",
        "    title = json.dumps(result[\"title\"], ensure_ascii=False)[1:-1]\n",
        "    content = json.dumps(result[\"content\"], ensure_ascii=False)[1:-1]\n",
        "    raw_content = \"\"\n",
        "    if (\n",
        "        include_raw_content\n",
        "        and \"raw_content\" in result\n",
        "        and result[\"raw_content\"] is not None\n",
        "        and len(result[\"raw_content\"].strip()) > 0\n",
        "    ):\n",
        "        raw_content = f\"<raw>{result['raw_content']}</raw>\"\n",
        "\n",
        "    return f\"<document><title>{title}</title><url>{result['url']}</url><content>{content}</content>{raw_content}</document>\"\n",
        "\n",
        "\n",
        "class TavilySearch(BaseTool):\n",
        "    \"\"\"\n",
        "    Tool that queries the Tavily Search API and gets back json\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"tavily_web_search\"\n",
        "    description: str = (\n",
        "        \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n",
        "        \"Useful for when you need to answer questions about current events. \"\n",
        "        \"Input should be a search query. [IMPORTANT] Input(query) should be over 5 characters.\"\n",
        "    )\n",
        "    args_schema: type[BaseModel] = TavilySearchInput\n",
        "    client: TavilyClient = None\n",
        "    include_domains: list = []\n",
        "    exclude_domains: list = []\n",
        "    max_results: int = 3\n",
        "    topic: Literal[\"general\", \"news\"] = \"general\"\n",
        "    days: int = 3\n",
        "    search_depth: Literal[\"basic\", \"advanced\"] = \"basic\"\n",
        "    include_answer: bool = False\n",
        "    include_raw_content: bool = True\n",
        "    include_images: bool = False\n",
        "    format_output: bool = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: Optional[str] = None,\n",
        "        include_domains: list = [],\n",
        "        exclude_domains: list = [],\n",
        "        max_results: int = 3,\n",
        "        topic: Literal[\"general\", \"news\"] = \"general\",\n",
        "        days: int = 3,\n",
        "        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n",
        "        include_answer: bool = False,\n",
        "        include_raw_content: bool = True,\n",
        "        include_images: bool = False,\n",
        "        format_output: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize TavilySearch Class\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Tavily API key\n",
        "            include_domains (list): A list of domains to include in the search.\n",
        "            exclude_domains (list): A list of domains to exclude from the search.\n",
        "            max_results (int): The maximum number of search results.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if api_key is None:\n",
        "            api_key = os.environ.get(\"TAVILY_API_KEY\", None)\n",
        "\n",
        "        if api_key is None:\n",
        "            raise ValueError(\"Tavily API key is not set.\")\n",
        "\n",
        "        self.client = TavilyClient(api_key=api_key)\n",
        "        self.include_domains = include_domains\n",
        "        self.exclude_domains = exclude_domains\n",
        "        self.max_results = max_results\n",
        "        self.topic = topic\n",
        "        self.days = days\n",
        "        self.search_depth = search_depth\n",
        "        self.include_answer = include_answer\n",
        "        self.include_raw_content = include_raw_content\n",
        "        self.include_images = include_images\n",
        "        self.format_output = format_output\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"define BaseTool's _run method\"\"\"\n",
        "        results = self.search(query)\n",
        "        return results\n",
        "        # return json.dumps(results, ensure_ascii=False)\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        query: str,\n",
        "        search_depth: Literal[\"basic\", \"advanced\"] = None,\n",
        "        topic: Literal[\"general\", \"news\"] = None,\n",
        "        days: int = None,\n",
        "        max_results: int = None,\n",
        "        include_domains: Sequence[str] = None,\n",
        "        exclude_domains: Sequence[str] = None,\n",
        "        include_answer: bool = None,\n",
        "        include_raw_content: bool = None,\n",
        "        include_images: bool = None,\n",
        "        format_output: bool = None,\n",
        "        **kwargs,\n",
        "    ) -> list:\n",
        "        \"\"\"\n",
        "\n",
        "        Performs a search and returns the results.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search query.\n",
        "            search_depth (str): The depth of the search (\"basic\" or \"advanced\").\n",
        "            topic (str): The topic of the search (\"general\" or \"news\").\n",
        "            days (int): The range of dates to search.\n",
        "            max_results (int): The maximum number of search results.\n",
        "            include_domains (list): A list of domains to include in the search.\n",
        "            exclude_domains (list): A list of domains to exclude from the search.\n",
        "            include_answer (bool): Whether to include an answer.\n",
        "            include_raw_content (bool): Whether to include raw content.\n",
        "            include_images (bool): Whether to include images.\n",
        "            format_output (bool): Whether to format the results.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of search results.\n",
        "        \"\"\"\n",
        "        # set init values\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"search_depth\": search_depth or self.search_depth,\n",
        "            \"topic\": topic or self.topic,\n",
        "            \"max_results\": max_results or self.max_results,\n",
        "            \"include_domains\": include_domains or self.include_domains,\n",
        "            \"exclude_domains\": exclude_domains or self.exclude_domains,\n",
        "            \"include_answer\": (\n",
        "                include_answer if include_answer is not None else self.include_answer\n",
        "            ),\n",
        "            \"include_raw_content\": (\n",
        "                include_raw_content\n",
        "                if include_raw_content is not None\n",
        "                else self.include_raw_content\n",
        "            ),\n",
        "            \"include_images\": (\n",
        "                include_images if include_images is not None else self.include_images\n",
        "            ),\n",
        "            **kwargs,\n",
        "        }\n",
        "\n",
        "        # deal `days` parameter\n",
        "        if days is not None:\n",
        "            if params[\"topic\"] == \"general\":\n",
        "                print(\n",
        "                    \"Warning: days parameter is ignored for 'general' topic search. Set topic parameter to 'news' to use days.\"\n",
        "                )\n",
        "            else:\n",
        "                params[\"days\"] = days\n",
        "\n",
        "        # call API\n",
        "        response = self.client.search(**params)\n",
        "\n",
        "        # fomatting the results\n",
        "        format_output = (\n",
        "            format_output if format_output is not None else self.format_output\n",
        "        )\n",
        "        if format_output:\n",
        "            return [\n",
        "                format_search_result(r, params[\"include_raw_content\"])\n",
        "                for r in response[\"results\"]\n",
        "            ]\n",
        "        else:\n",
        "            return response[\"results\"]\n",
        "\n",
        "    def get_search_context(\n",
        "        self,\n",
        "        query: str,\n",
        "        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n",
        "        topic: Literal[\"general\", \"news\"] = \"general\",\n",
        "        days: int = 3,\n",
        "        max_results: int = 5,\n",
        "        include_domains: Sequence[str] = None,\n",
        "        exclude_domains: Sequence[str] = None,\n",
        "        max_tokens: int = 4000,\n",
        "        format_output: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves context for the search query. This is useful for extracting only relevant content from websites,\n",
        "        and you do not need to handle context extraction and limitation directly.\n",
        "\n",
        "        Args:\n",
        "            query (str): The search query.\n",
        "            search_depth (str): The depth of the search (\"basic\" or \"advanced\").\n",
        "            topic (str): The topic of the search (\"general\" or \"news\").\n",
        "            days (int): The range of dates to search.\n",
        "            max_results (int): The maximum number of search results.\n",
        "            include_domains (list): A list of domains to include in the search.\n",
        "            exclude_domains (list): A list of domains to exclude from the search.\n",
        "            max_tokens (int): The maximum number of tokens to return (based on OpenAI token calculation). The default is 4000.\n",
        "            format_output (bool): Whether to format the results.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            str: A JSON string containing the search context up to the context limit.\n",
        "        \"\"\"\n",
        "        response = self.client.search(\n",
        "            query,\n",
        "            search_depth=search_depth,\n",
        "            topic=topic,\n",
        "            days=days,\n",
        "            max_results=max_results,\n",
        "            include_domains=include_domains,\n",
        "            exclude_domains=exclude_domains,\n",
        "            include_answer=False,\n",
        "            include_raw_content=False,\n",
        "            include_images=False,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        sources = response.get(\"results\", [])\n",
        "        if not sources:\n",
        "            return json.dumps([], ensure_ascii=False)\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "        # Process and accumulate results while tracking tokens\n",
        "        processed_results = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for source in sources:\n",
        "            if format_output:\n",
        "                result = format_search_result(source, include_raw_content=False)\n",
        "            else:\n",
        "                result = {\n",
        "                    \"url\": source[\"url\"],\n",
        "                    \"content\": json.dumps(\n",
        "                        {\"title\": source[\"title\"], \"content\": source[\"content\"]},\n",
        "                        ensure_ascii=False,\n",
        "                    ),\n",
        "                }\n",
        "\n",
        "            # Convert to string to count tokens\n",
        "            result_str = json.dumps(result, ensure_ascii=False)\n",
        "            tokens_needed = len(encoding.encode(result_str))\n",
        "\n",
        "            # Check if adding this result would exceed the token limit\n",
        "            if current_tokens + tokens_needed > max_tokens:\n",
        "                break\n",
        "\n",
        "            processed_results.append(result)\n",
        "            current_tokens += tokens_needed\n",
        "\n",
        "        return json.dumps(processed_results, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786c076b",
      "metadata": {},
      "source": [
        "## Analysts Generation : Human in the Loop\n",
        "**Analyst Generation** : Create and review analysts using Human-In-The-Loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "188b28b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Get the latest model\n",
        "GPT4o = get_model_name(LLMs.GPT4o)\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatOpenAI(model=GPT4o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "13850b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Class defining analyst properties and metadata\n",
        "class Analyst(BaseModel):\n",
        "    # Primary affiliation information\n",
        "    affiliation: str = Field(\n",
        "        description=\"Primary affiliation of the analyst.\",\n",
        "    )\n",
        "    # Name\n",
        "    name: str = Field(description=\"Name of the analyst.\")\n",
        "\n",
        "    # Role\n",
        "    role: str = Field(\n",
        "        description=\"Role of the analyst in the context of the topic.\",\n",
        "    )\n",
        "    # Description of focus, concerns, and motives\n",
        "    description: str = Field(\n",
        "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
        "    )\n",
        "\n",
        "    # Property that returns analyst's personal information as a string\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "\n",
        "# Collection of analysts\n",
        "class Perspectives(BaseModel):\n",
        "    # List of analysts\n",
        "    analysts: List[Analyst] = Field(\n",
        "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713f52d5",
      "metadata": {},
      "source": [
        "The following defines the state that tracks the collection of analysts generated through the Analyst class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "15e0b45b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# State Definition\n",
        "class GenerateAnalystsState(TypedDict):\n",
        "    # Research topic\n",
        "    topic: str\n",
        "    # Maximum number of analysts to generate\n",
        "    max_analysts: int\n",
        "    # Human feedback\n",
        "    human_analyst_feedback: str\n",
        "    # List of analysts\n",
        "    analysts: List[Analyst]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0d517e",
      "metadata": {},
      "source": [
        "### Defining the Analyst Generation Node\n",
        "\n",
        "Next, we will define the analyst generation node.\n",
        "\n",
        "The code below implements the logic for generating various analysts based on the provided research topic. Each analyst has a unique role and affiliation, offering professional perspectives on the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1ec68f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Analyst generation prompt\n",
        "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
        "\n",
        "Follow these instructions carefully:\n",
        "1. First, review the research topic:\n",
        "\n",
        "{topic}\n",
        "        \n",
        "2. Examine any editorial feedback that has been optionally provided to guide the creation of the analysts: \n",
        "        \n",
        "{human_analyst_feedback}\n",
        "    \n",
        "3. Determine the most interesting themes based upon documents and/or feedback above.\n",
        "                    \n",
        "4. Pick the top {max_analysts} themes.\n",
        "\n",
        "5. Assign one analyst to each theme.\"\"\"\n",
        "\n",
        "\n",
        "# Analyst generation node\n",
        "def create_analysts(state: GenerateAnalystsState):\n",
        "    \"\"\"Function to create analyst personas\"\"\"\n",
        "\n",
        "    topic = state[\"topic\"]\n",
        "    max_analysts = state[\"max_analysts\"]\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
        "\n",
        "    # Apply structured output format to LLM\n",
        "    structured_llm = llm.with_structured_output(Perspectives)\n",
        "\n",
        "    # Construct system prompt for analyst creation\n",
        "    system_message = analyst_instructions.format(\n",
        "        topic=topic,\n",
        "        human_analyst_feedback=human_analyst_feedback,\n",
        "        max_analysts=max_analysts,\n",
        "    )\n",
        "\n",
        "    # Call LLM to generate analyst personas\n",
        "    analysts = structured_llm.invoke(\n",
        "        [SystemMessage(content=system_message)]\n",
        "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
        "    )\n",
        "\n",
        "    # Store generated list of analysts in state\n",
        "    return {\"analysts\": analysts.analysts}\n",
        "\n",
        "\n",
        "# User feedback node (can be left empty since it will update state)\n",
        "def human_feedback(state: GenerateAnalystsState):\n",
        "    \"\"\"A checkpoint node for receiving user feedback\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "# Function to decide the next step in the workflow based on human feedback\n",
        "def should_continue(state: GenerateAnalystsState):\n",
        "    \"\"\"Function to determine the next step in the workflow\"\"\"\n",
        "\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
        "    if human_analyst_feedback:\n",
        "        return \"create_analysts\"\n",
        "\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb884f7",
      "metadata": {},
      "source": [
        "**Explanation of Code Components**\n",
        "\n",
        "- **Analyst Instructions**: A prompt that guides the LLM in creating AI analyst personas based on a specified research topic and any provided feedback.\n",
        "\n",
        "- **create_analysts Function**: This function is responsible for generating a set of analysts based on the current state, which includes the research topic, maximum number of analysts, and any human feedback.\n",
        "\n",
        "- **human_feedback Function**: A placeholder function that can be expanded to handle user feedback.\n",
        "\n",
        "- **should_continue Function**: This function evaluates whether there is human feedback available and determines whether to proceed with creating analysts or end the process. \n",
        "\n",
        "This structure allows for a dynamic approach to generating tailored analyst personas that can provide diverse insights into a given research topic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77452ab5",
      "metadata": {},
      "source": [
        "### Building the Analyst Generation Graph\n",
        "\n",
        "Now we'll create the analyst generation graph that orchestrates the research workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a903259c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Create graph\n",
        "builder = StateGraph(GenerateAnalystsState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"create_analysts\", create_analysts)\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "\n",
        "# Connect edges\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
        "\n",
        "# Add conditional edge: return to analyst creation node if human feedback exists\n",
        "builder.add_conditional_edges(\n",
        "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
        ")\n",
        "\n",
        "# Create memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Compile graph (set breakpoints)\n",
        "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
        "\n",
        "# Visualize graph\n",
        "visualize_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2299a1d0",
      "metadata": {},
      "source": [
        "**Graph Components**\n",
        "\n",
        "- **Nodes**\n",
        "    - `create_analysts`: Generates analyst personas based on the research topic\n",
        "    - `human_feedback`: Checkpoint for receiving user input and feedback\n",
        "\n",
        "- **Edges**\n",
        "    - Initial flow from START to analyst creation\n",
        "    - Connection from analyst creation to human feedback\n",
        "    - Conditional path back to analyst creation based on feedback\n",
        "\n",
        "- **Features**\n",
        "    - Memory persistence using `MemorySaver`\n",
        "    - Breakpoints before human feedback collection\n",
        "    - Visual representation of workflow through `visualize_graph`\n",
        "\n",
        "This graph structure enables an iterative research process with human oversight and feedback integration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57868ee",
      "metadata": {},
      "source": [
        "### Running the Analyst Generation Graph\n",
        "Here's how to execute and manage the analyst generation workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "823b806f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='Tech Research Institute' name='Dr. Emma Hughes' role='RAG System Architect' description='Dr. Hughes specializes in designing and implementing RAG systems. Her focus is on understanding the architectural differences between Modular RAG and Naive RAG, particularly in terms of scalability and integration within existing infrastructures. She is motivated by optimizing RAG systems for better performance and reliability at the production level.'\n",
            "affiliation='Data Science Innovations Ltd.' name='Mr. Alex Chen' role='AI Performance Analyst' description='Mr. Chen focuses on evaluating the performance metrics of AI systems. His primary concern is comparing the efficiency and effectiveness of Modular RAG versus Naive RAG in real-world applications. He aims to identify how these differences impact computational resources and processing speed in production environments.'\n",
            "affiliation='Enterprise Solutions Group' name='Ms. Priya Raman' role='Industry Application Specialist' description='Ms. Raman explores the practical benefits and challenges of deploying RAG systems in commercial settings. Her interest lies in understanding the business advantages of choosing Modular RAG over Naive RAG, such as cost-effectiveness, adaptability, and maintenance in long-term usage.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "# Configure graph execution settings\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=10,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "# Set number of analysts\n",
        "max_analysts = 3\n",
        "\n",
        "# Define research topic\n",
        "topic = \"What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at the production level\"\n",
        "\n",
        "# Configure input parameters\n",
        "inputs = {\n",
        "    \"topic\": topic,\n",
        "    \"max_analysts\": max_analysts,\n",
        "}\n",
        "\n",
        "# Execute graph\n",
        "invoke_graph(graph, inputs, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd057565",
      "metadata": {},
      "source": [
        "When `__interrupt__` is displayed, the system is ready to receive human feedback. At this point, you can retrieve the current state and provide feedback to guide the analyst generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b6046767",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('human_feedback',)\n"
          ]
        }
      ],
      "source": [
        "# Get current graph state\n",
        "state = graph.get_state(config)\n",
        "\n",
        "# Check next node\n",
        "print(state.next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5981af",
      "metadata": {},
      "source": [
        "To inject human feedback into the graph, we use the `update_state` method with the following key components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bd46b1e2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '55d4904e-7a73-438a-ad4b-0196d893850b',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd8ce8-34da-64cc-8002-10289c286479'}}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Update graph state with human feedback\n",
        "graph.update_state(\n",
        "    config,\n",
        "    {\n",
        "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
        "    },\n",
        "    as_node=\"human_feedback\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346b11de",
      "metadata": {},
      "source": [
        "**Key Parameters**\n",
        "- `config` : Configuration object containing graph settings\n",
        "- `human_analyst_feedback` : Key for storing feedback content\n",
        "- `as_node` : Specifies the node that will process the feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12bf66a",
      "metadata": {},
      "source": [
        "**[Note]** : Assigning `None` as input triggers the graph to continue its execution from the last checkpoint. This is particularly useful when you want to resume processing after providing human feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12a3a25",
      "metadata": {},
      "source": [
        "(Continue) To resume the graph execution after the `__interrupt__` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f0cf45e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='Tech Startup' name='Teddy Lee' role='Entrepreneur' description='Focuses on the practical applications and business implications of adopting Modular RAG versus Naive RAG in startup environments. Concerned with scalability, cost-effectiveness, and innovation potential to maintain a competitive edge.'\n",
            "affiliation='University of Technology' name='Dr. Emily Foster' role='Academic Researcher' description='Explores the theoretical differences between Modular RAG and Naive RAG, emphasizing the technical nuances and potential for innovation. Interested in how these models can be leveraged for research and educational purposes.'\n",
            "affiliation='Global Tech Corporation' name='Michael Chen' role='Industry Expert' description='Analyzes the benefits and challenges of implementing Modular RAG at the production level in large enterprises. Focuses on operational efficiency, integration into existing systems, and long-term performance sustainability.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Continue execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52730608",
      "metadata": {},
      "source": [
        "When `__interrupt__` appears again, you have two options: \n",
        "\n",
        "- Option 1: Provide Additional Feedback\n",
        "    - You can provide more feedback to further refine the analyst personas using the same method as before\n",
        "- Option 2: Complete the Process\n",
        "\n",
        "To finish the analyst generation process without additional feedback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8a914e38",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '55d4904e-7a73-438a-ad4b-0196d893850b',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd8ce8-5fb0-66f1-8004-77359540a17e'}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set feedback input to None to indicate completion\n",
        "human_feedback_input = None\n",
        "\n",
        "# Update graph state with no feedback\n",
        "graph.update_state(\n",
        "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7b879519",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue final execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da31ff4",
      "metadata": {},
      "source": [
        "**Displaying Final Results**\n",
        "\n",
        "Get and display the final results from the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b3623113",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of analysts generated: 3\n",
            "================================\n",
            "Name: Teddy Lee\n",
            "Role: Entrepreneur\n",
            "Affiliation: Tech Startup\n",
            "Description: Focuses on the practical applications and business implications of adopting Modular RAG versus Naive RAG in startup environments. Concerned with scalability, cost-effectiveness, and innovation potential to maintain a competitive edge.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Name: Dr. Emily Foster\n",
            "Role: Academic Researcher\n",
            "Affiliation: University of Technology\n",
            "Description: Explores the theoretical differences between Modular RAG and Naive RAG, emphasizing the technical nuances and potential for innovation. Interested in how these models can be leveraged for research and educational purposes.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "Name: Michael Chen\n",
            "Role: Industry Expert\n",
            "Affiliation: Global Tech Corporation\n",
            "Description: Analyzes the benefits and challenges of implementing Modular RAG at the production level in large enterprises. Focuses on operational efficiency, integration into existing systems, and long-term performance sustainability.\n",
            "\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "()\n"
          ]
        }
      ],
      "source": [
        "# Get final state\n",
        "final_state = graph.get_state(config)\n",
        "\n",
        "# Get generated analysts\n",
        "analysts = final_state.values.get(\"analysts\")\n",
        "\n",
        "# Print analyst count\n",
        "print(\n",
        "    f\"Number of analysts generated: {len(analysts)}\",\n",
        "    end=\"\\n================================\\n\",\n",
        ")\n",
        "\n",
        "# Print each analyst's persona\n",
        "for analyst in analysts:\n",
        "    print(analyst.persona)\n",
        "    print(\"- \" * 30)\n",
        "\n",
        "# Get next node state (empty tuple indicates completion)\n",
        "print(final_state.next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b22fd5",
      "metadata": {},
      "source": [
        "**Key Components**\n",
        "- `final_state`: Contains the final state of the graph execution.\n",
        "- `analysts`: List of generated analyst personas.\n",
        "- `final_state.next`: Empty tuple indicating workflow completion.\n",
        "\n",
        "The output will display each analyst's complete persona information, including their name, role, affiliation, and description, followed by a separator line. The empty tuple printed at the end confirms that the graph execution has completed successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b277b6",
      "metadata": {},
      "source": [
        "## Interview Execution\n",
        "\n",
        "### Define Classes and `question_generation` Node\n",
        "Let's implement the interview execution components with proper state management and `question_generation` Node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4c00e51f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "class InterviewState(MessagesState):\n",
        "    \"\"\"State management for interview process\"\"\"\n",
        "\n",
        "    max_num_turns: int\n",
        "    context: Annotated[list, operator.add]  # Context list containing source documents\n",
        "    analyst: Analyst\n",
        "    interview: str  # String storing interview content\n",
        "    sections: list  # List of report sections\n",
        "\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    \"\"\"Data class for search queries\"\"\"\n",
        "\n",
        "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6ab79654",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_question(state: InterviewState):\n",
        "    \"\"\"Node for generating interview questions\"\"\"\n",
        "\n",
        "    # System prompt for question generation\n",
        "    question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
        "\n",
        "    Your goal is boil down to interesting and specific insights related to your topic.\n",
        "\n",
        "    1. Interesting: Insights that people will find surprising or non-obvious.\n",
        "            \n",
        "    2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
        "\n",
        "    Here is your topic of focus and set of goals: {goals}\n",
        "            \n",
        "    Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
        "\n",
        "    Continue to ask questions to drill down and refine your understanding of the topic.\n",
        "            \n",
        "    When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
        "\n",
        "    Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
        "\n",
        "    # Extract state components\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Generate question using LLM\n",
        "    system_message = question_instructions.format(goals=analyst.persona)\n",
        "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    # Return updated messages\n",
        "    return {\"messages\": [question]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6957dd0a",
      "metadata": {},
      "source": [
        "**State Management**\n",
        "- `InterviewState` tracks conversation turns, context, and interview content.\n",
        "- Annotated context list allows for document accumulation.\n",
        "- Maintains analyst persona and report sections.\n",
        "\n",
        "**Question Generation**\n",
        "- Structured system prompt for consistent interviewing style.\n",
        "- Persona-aware questioning based on analyst goals.\n",
        "- Progressive refinement of topic understanding.\n",
        "- Clear interview conclusion mechanism.\n",
        "\n",
        "The code provides a robust foundation for conducting structured interviews while maintaining conversation state and context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3a0fa5",
      "metadata": {},
      "source": [
        "### Defining Research Tools\n",
        "\n",
        "Experts collect information in parallel from multiple sources to answer questions.\n",
        "\n",
        "They can utilize various tools such as web document scraping, VectorDB, web search, and Wikipedia search.\n",
        "\n",
        "We'll focus on two main tools: **`Tavily`** for web search and **`ArxivRetriever`** for academic papers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac99427",
      "metadata": {},
      "source": [
        "`Tavily Search`\n",
        "- Real-time web search capabilities\n",
        "- Configurable result count and content depth\n",
        "- Structured output formatting\n",
        "- Raw content inclusion option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "79aea221",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TavilySearch with configuration\n",
        "tavily_search = TavilySearch(max_results=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "522b7832",
      "metadata": {},
      "source": [
        "`ArxivRetriever`\n",
        "- Access to academic papers and research\n",
        "- Full document retrieval\n",
        "- Comprehensive metadata access\n",
        "- Customizable document load limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "16b9cbca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '), Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'), Document(metadata={'Published': '2024-05-22', 'Title': 'FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research', 'Authors': 'Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou', 'Summary': 'With the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research\\nattention. Numerous novel algorithms and models have been introduced to enhance\\nvarious aspects of RAG systems. However, the absence of a standardized\\nframework for implementation, coupled with the inherently intricate RAG\\nprocess, makes it challenging and time-consuming for researchers to compare and\\nevaluate these approaches in a consistent environment. Existing RAG toolkits\\nlike LangChain and LlamaIndex, while available, are often heavy and unwieldy,\\nfailing to meet the personalized needs of researchers. In response to this\\nchallenge, we propose FlashRAG, an efficient and modular open-source toolkit\\ndesigned to assist researchers in reproducing existing RAG methods and in\\ndeveloping their own RAG algorithms within a unified framework. Our toolkit\\nimplements 12 advanced RAG methods and has gathered and organized 32 benchmark\\ndatasets. Our toolkit has various features, including customizable modular\\nframework, rich collection of pre-implemented RAG works, comprehensive\\ndatasets, efficient auxiliary pre-processing scripts, and extensive and\\nstandard evaluation metrics. Our toolkit and resources are available at\\nhttps://github.com/RUC-NLPIR/FlashRAG.', 'entry_id': 'http://arxiv.org/abs/2405.13576v1', 'published_first_time': '2024-05-22', 'comment': '8 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.IR'], 'links': ['http://arxiv.org/abs/2405.13576v1', 'http://arxiv.org/pdf/2405.13576v1']}, page_content='FlashRAG: A Modular Toolkit for Efficient\\nRetrieval-Augmented Generation Research\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\\nGaoling School of Artificial Intelligence\\nRenmin University of China\\n{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\\nAbstract\\nWith the advent of Large Language Models (LLMs), the potential of Retrieval\\nAugmented Generation (RAG) techniques have garnered considerable research\\nattention. Numerous novel algorithms and models have been introduced to enhance\\nvarious aspects of RAG systems. However, the absence of a standardized framework\\nfor implementation, coupled with the inherently intricate RAG process, makes it\\nchallenging and time-consuming for researchers to compare and evaluate these\\napproaches in a consistent environment. Existing RAG toolkits like LangChain\\nand LlamaIndex, while available, are often heavy and unwieldy, failing to\\nmeet the personalized needs of researchers. In response to this challenge, we\\npropose FlashRAG, an efficient and modular open-source toolkit designed to assist\\nresearchers in reproducing existing RAG methods and in developing their own\\nRAG algorithms within a unified framework. Our toolkit implements 12 advanced\\nRAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\\nhas various features, including customizable modular framework, rich collection\\nof pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\\nprocessing scripts, and extensive and standard evaluation metrics. Our toolkit and\\nresources are available at https://github.com/RUC-NLPIR/FlashRAG.\\n1\\nIntroduction\\nIn the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\\nemerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\\nknowledge bases [3]. The substantial applications and the potential of RAG technology have\\nattracted considerable research attention. With the introduction of a large number of new algorithms\\nand models to improve various facets of RAG systems in recent years, comparing and evaluating\\nthese methods under a consistent setting has become increasingly challenging.\\nMany works are not open-source or have fixed settings in their open-source code, making it difficult\\nto adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\\nvary, with resources being scattered, which can lead researchers to spend excessive time on pre-\\nprocessing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\\nof RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\\noften need to implement many parts of the system themselves. Although there are some existing RAG\\ntoolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\\nresearchers from implementing customized processes and failing to address the aforementioned issues.\\n∗Corresponding author\\nPreprint. Under review.\\narXiv:2405.13576v1  [cs.CL]  22 May 2024\\nThus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\\ndevelopment and comparative studies.\\nTo address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\\nenable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\\nThis library allows researchers to utilize built pipelines to replicate existing work, employ provided\\nRAG components to construct their own RAG processes, or simply use organized datasets and corpora\\nto accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\\nfor researchers. To summarize, the key features and capabilities of our FlashRAG library can be\\noutlined in the following four aspects:\\nExtensive and Customizable Modular RAG Framework.\\nTo facilitate an easily expandable\\nRAG process, we implemented modular RAG at two levels. At the component level, we offer\\ncomprehensive RAG compon')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "# Initialize ArxivRetriever with configuration\n",
        "arxiv_retriever = ArxivRetriever(\n",
        "    load_max_docs=3, load_all_available_meta=True, get_full_documents=True\n",
        ")\n",
        "\n",
        "# Execute arxiv search and print results\n",
        "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
        "print(arxiv_search_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "731bab0a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Published': '2024-07-26',\n",
              " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
              " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
              " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
              " 'entry_id': 'http://arxiv.org/abs/2407.21059v1',\n",
              " 'published_first_time': '2024-07-26',\n",
              " 'comment': None,\n",
              " 'journal_ref': None,\n",
              " 'doi': None,\n",
              " 'primary_category': 'cs.CL',\n",
              " 'categories': ['cs.CL', 'cs.AI', 'cs.IR'],\n",
              " 'links': ['http://arxiv.org/abs/2407.21059v1',\n",
              "  'http://arxiv.org/pdf/2407.21059v1']}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# metadata of Arxiv\n",
        "arxiv_search_results[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9c4563b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n"
          ]
        }
      ],
      "source": [
        "# content of Arxiv\n",
        "print(arxiv_search_results[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088a306d",
      "metadata": {},
      "source": [
        " format and display Arxiv search results in a structured XML-like format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4894dbc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문서 검색 결과를 포맷팅\n",
        "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "    [\n",
        "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
        "        for doc in arxiv_search_results\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "143d6b5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
            "challenges like hallucination, outdated knowledge, and non-transparent,\n",
            "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
            "emerged as a promising solution by incorporating knowledge from external\n",
            "databases. This enhances the accuracy and credibility of the generation,\n",
            "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
            "updates and integration of domain-specific information. RAG synergistically\n",
            "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
            "external databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
            "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
            "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
            "generation and the augmentation techniques. The paper highlights the\n",
            "state-of-the-art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG systems.\n",
            "Furthermore, this paper introduces up-to-date evaluation framework and\n",
            "benchmark. At the end, this article delineates the challenges currently faced\n",
            "and points out prospective avenues for research and development.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Retrieval-Augmented Generation for Large\n",
            "Language Models: A Survey\n",
            "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
            "Wangc, and Haofen Wang a,c\n",
            "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
            "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
            "cCollege of Design and Innovation, Tongji University\n",
            "Abstract—Large Language Models (LLMs) showcase impres-\n",
            "sive capabilities but encounter challenges like hallucination,\n",
            "outdated knowledge, and non-transparent, untraceable reasoning\n",
            "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
            "as a promising solution by incorporating knowledge from external\n",
            "databases. This enhances the accuracy and credibility of the\n",
            "generation, particularly for knowledge-intensive tasks, and allows\n",
            "for continuous knowledge updates and integration of domain-\n",
            "specific information. RAG synergistically merges LLMs’ intrin-\n",
            "sic knowledge with the vast, dynamic repositories of external\n",
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.\n",
            "Index Terms—Large language model, retrieval-augmented gen-\n",
            "eration, natural language processing, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE language models (LLMs) have achieved remark-\n",
            "able success, though they still face significant limitations,\n",
            "especially in domain-specific or knowledge-intensive tasks [1],\n",
            "notably producing “hallucinations” [2] when handling queries\n",
            "beyond their training data or requiring current information. To\n",
            "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
            "enhances LLMs by retrieving relevant document chunks from\n",
            "external knowledge base through semantic similarity calcu-\n",
            "lation. By referencing external knowledge, RAG effectively\n",
            "reduces the problem of generating factually incorrect content.\n",
            "Its integration into LLMs has resulted in widespread adoption,\n",
            "establishing RAG as a key technology in advancing chatbots\n",
            "and enhancing the suitability of LLMs for real-world applica-\n",
            "tions.\n",
            "RAG technology has rapidly developed in recent years, and\n",
            "the technology tree summarizing related research is shown\n",
            "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
            "1Resources\n",
            "are\n",
            "available\n",
            "at\n",
            "https://github.com/Tongji-KGLLM/\n",
            "RAG-Survey\n",
            "in Figure 1. The development trajectory of RAG in the era\n",
            "of large models exhibits several distinct stage characteristics.\n",
            "Initially, RAG’s inception coincided with the rise of the\n",
            "Transformer architecture, focusing on enhancing language\n",
            "models by incorporating additional knowledge through Pre-\n",
            "Training Models (PTM). This early stage was characterized\n",
            "by foundational work aimed at refining pre-training techniques\n",
            "[3]–[5].The subsequent arrival of ChatGPT [6] marked a\n",
            "pivotal moment, with LLM demonstrating powerful in context\n",
            "learning (ICL) capabilities. RAG research shifted towards\n",
            "providing better information for LLMs to answer more com-\n",
            "plex and knowledge-intensive tasks during the inference stage,\n",
            "leading to rapid development in RAG studies. As research\n",
            "progressed, the enhancement of RAG was no longer limited\n",
            "to the inference stage but began to incorporate more with LLM\n",
            "fine-tuning techniques.\n",
            "The burgeoning field of RAG has experienced swift growth,\n",
            "yet it has not been accompanied by a systematic synthesis that\n",
            "could clarify its broader trajectory. Thi\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2405.13576v1\" date=\"2024-05-22\" authors=\"Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou\"/>\n",
            "<Title>\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized\n",
            "framework for implementation, coupled with the inherently intricate RAG\n",
            "process, makes it challenging and time-consuming for researchers to compare and\n",
            "evaluate these approaches in a consistent environment. Existing RAG toolkits\n",
            "like LangChain and LlamaIndex, while available, are often heavy and unwieldy,\n",
            "failing to meet the personalized needs of researchers. In response to this\n",
            "challenge, we propose FlashRAG, an efficient and modular open-source toolkit\n",
            "designed to assist researchers in reproducing existing RAG methods and in\n",
            "developing their own RAG algorithms within a unified framework. Our toolkit\n",
            "implements 12 advanced RAG methods and has gathered and organized 32 benchmark\n",
            "datasets. Our toolkit has various features, including customizable modular\n",
            "framework, rich collection of pre-implemented RAG works, comprehensive\n",
            "datasets, efficient auxiliary pre-processing scripts, and extensive and\n",
            "standard evaluation metrics. Our toolkit and resources are available at\n",
            "https://github.com/RUC-NLPIR/FlashRAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "FlashRAG: A Modular Toolkit for Efficient\n",
            "Retrieval-Augmented Generation Research\n",
            "Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\n",
            "Gaoling School of Artificial Intelligence\n",
            "Renmin University of China\n",
            "{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\n",
            "Abstract\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized framework\n",
            "for implementation, coupled with the inherently intricate RAG process, makes it\n",
            "challenging and time-consuming for researchers to compare and evaluate these\n",
            "approaches in a consistent environment. Existing RAG toolkits like LangChain\n",
            "and LlamaIndex, while available, are often heavy and unwieldy, failing to\n",
            "meet the personalized needs of researchers. In response to this challenge, we\n",
            "propose FlashRAG, an efficient and modular open-source toolkit designed to assist\n",
            "researchers in reproducing existing RAG methods and in developing their own\n",
            "RAG algorithms within a unified framework. Our toolkit implements 12 advanced\n",
            "RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\n",
            "has various features, including customizable modular framework, rich collection\n",
            "of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\n",
            "processing scripts, and extensive and standard evaluation metrics. Our toolkit and\n",
            "resources are available at https://github.com/RUC-NLPIR/FlashRAG.\n",
            "1\n",
            "Introduction\n",
            "In the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\n",
            "emerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\n",
            "knowledge bases [3]. The substantial applications and the potential of RAG technology have\n",
            "attracted considerable research attention. With the introduction of a large number of new algorithms\n",
            "and models to improve various facets of RAG systems in recent years, comparing and evaluating\n",
            "these methods under a consistent setting has become increasingly challenging.\n",
            "Many works are not open-source or have fixed settings in their open-source code, making it difficult\n",
            "to adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\n",
            "vary, with resources being scattered, which can lead researchers to spend excessive time on pre-\n",
            "processing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\n",
            "of RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\n",
            "often need to implement many parts of the system themselves. Although there are some existing RAG\n",
            "toolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\n",
            "researchers from implementing customized processes and failing to address the aforementioned issues.\n",
            "∗Corresponding author\n",
            "Preprint. Under review.\n",
            "arXiv:2405.13576v1  [cs.CL]  22 May 2024\n",
            "Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\n",
            "development and comparative studies.\n",
            "To address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\n",
            "enable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\n",
            "This library allows researchers to utilize built pipelines to replicate existing work, employ provided\n",
            "RAG components to construct their own RAG processes, or simply use organized datasets and corpora\n",
            "to accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\n",
            "for researchers. To summarize, the key features and capabilities of our FlashRAG library can be\n",
            "outlined in the following four aspects:\n",
            "Extensive and Customizable Modular RAG Framework.\n",
            "To facilitate an easily expandable\n",
            "RAG process, we implemented modular RAG at two levels. At the component level, we offer\n",
            "comprehensive RAG compon\n",
            "</Content>\n",
            "</Document>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_search_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678568f9",
      "metadata": {},
      "source": [
        "### Defining Search Tool Nodes\n",
        "\n",
        "The code implements two main search tool nodes for gathering research information: web search via `Tavily` and academic paper search via `ArXiv`. Here's a breakdown of the key components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "621173b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Search query instruction\n",
        "search_instructions = SystemMessage(\n",
        "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
        "\n",
        "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
        "        \n",
        "First, analyze the full conversation.\n",
        "\n",
        "Pay particular attention to the final question posed by the analyst.\n",
        "\n",
        "Convert this final question into a well-structured web search query\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def search_web(state: InterviewState):\n",
        "    \"\"\"Performs web search using Tavily\"\"\"\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
        "\n",
        "    # Execute search\n",
        "    search_docs = tavily_search.invoke(search_query.search_query)\n",
        "\n",
        "    # Format results - handle both string and dict responses\n",
        "    if isinstance(search_docs, list):\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "            [\n",
        "                f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
        "                for doc in search_docs\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "\n",
        "def search_arxiv(state: InterviewState):\n",
        "    \"\"\"Performs academic paper search using ArXiv\"\"\"\n",
        "\n",
        "    # Generate search query\n",
        "    structured_llm = llm.with_structured_output(SearchQuery)\n",
        "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
        "\n",
        "    try:\n",
        "        # Execute search\n",
        "        arxiv_search_results = arxiv_retriever.invoke(\n",
        "            search_query.search_query,\n",
        "            load_max_docs=2,\n",
        "            load_all_available_meta=True,\n",
        "            get_full_documents=True,\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "            [\n",
        "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
        "                for doc in arxiv_search_results\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return {\"context\": [formatted_search_docs]}\n",
        "    except Exception as e:\n",
        "        print(f\"ArXiv search error: {str(e)}\")\n",
        "        return {\"context\": [\"<Error>Failed to retrieve ArXiv search results.</Error>\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86155692",
      "metadata": {},
      "source": [
        "**Key Features**\n",
        "- Query Generation: Uses LLM to create structured search queries from conversation context\n",
        "- Error Handling: Robust error management for ArXiv searches\n",
        "- Result Formatting: Consistent XML-style formatting for both web and academic results\n",
        "- Metadata Integration: Comprehensive metadata inclusion for academic papers\n",
        "- State Management: Maintains conversation context through InterviewState"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e666219e",
      "metadata": {},
      "source": [
        "### Define `generate_answer`, `save_interview`, `route_messages`, `write_section` Nodes\n",
        "\n",
        "- The `generate_answer` node is responsible for creating expert responses during the interview process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4509e48e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import get_buffer_string\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
        "\n",
        "Here is analyst area of focus: {goals}. \n",
        "        \n",
        "You goal is to answer a question posed by the interviewer.\n",
        "\n",
        "To answer question, use this context:\n",
        "        \n",
        "{context}\n",
        "\n",
        "When answering questions, follow these guidelines:\n",
        "        \n",
        "1. Use only the information provided in the context. \n",
        "        \n",
        "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
        "\n",
        "3. The context contain sources at the topic of each individual document.\n",
        "\n",
        "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
        "\n",
        "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
        "        \n",
        "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
        "        \n",
        "[1] assistant/docs/llama3_1.pdf, page 7 \n",
        "        \n",
        "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
        "\n",
        "\n",
        "def generate_answer(state: InterviewState):\n",
        "    \"\"\"Generates expert responses to analyst questions\"\"\"\n",
        "\n",
        "    # Get analyst and messages from state\n",
        "    analyst = state[\"analyst\"]\n",
        "    messages = state[\"messages\"]\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    # Generate answer for the question\n",
        "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
        "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
        "\n",
        "    # Name the message as expert response\n",
        "    answer.name = \"expert\"\n",
        "\n",
        "    # Add message to state\n",
        "    return {\"messages\": [answer]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d878d4",
      "metadata": {},
      "source": [
        "- `save_interview`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "42706ff4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_interview(state: InterviewState):\n",
        "    \"\"\"Saves the interview content\"\"\"\n",
        "\n",
        "    # Get messages from state\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Convert interview to string\n",
        "    interview = get_buffer_string(messages)\n",
        "\n",
        "    # Store under interview key\n",
        "    return {\"interview\": interview}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8aa4df",
      "metadata": {},
      "source": [
        "### Define `generate_answer`, `save_interview`, `route_messages`, `write_section` Nodes\n",
        "- `route_messages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "10bb1677",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
        "    \"\"\"Routes between questions and answers in the conversation\"\"\"\n",
        "\n",
        "    # Get messages from state\n",
        "    messages = state[\"messages\"]\n",
        "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
        "\n",
        "    # Count expert responses\n",
        "    num_responses = len(\n",
        "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
        "    )\n",
        "\n",
        "    # End interview if maximum turns reached\n",
        "    if num_responses >= max_num_turns:\n",
        "        return \"save_interview\"\n",
        "\n",
        "    # This router runs after each question-answer pair\n",
        "    # Get the last question to check for conversation end signal\n",
        "    last_question = messages[-2]\n",
        "\n",
        "    # Check for conversation end signal\n",
        "    if \"Thank you so much for your help\" in last_question.content:\n",
        "        return \"save_interview\"\n",
        "    return \"ask_question\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfaaef36",
      "metadata": {},
      "source": [
        "- The `write_section` function and its associated instructions implement a structured report generation system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ca2e1be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
        "\n",
        "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
        "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
        "\n",
        "Please follow these instructions:\n",
        "1. Analyze the content of the source documents: \n",
        "- The name of each source document is at the start of the document, with the <Document tag.\n",
        "        \n",
        "2. Create a report structure using markdown formatting:\n",
        "- Use ## for the section title\n",
        "- Use ### for sub-section headers\n",
        "        \n",
        "3. Write the report following this structure:\n",
        "a. Title (## header)\n",
        "b. Summary (### header)\n",
        "c. Comprehensive analysis (### header)\n",
        "d. Sources (### header)\n",
        "\n",
        "4. Make your title engaging based upon the focus area of the analyst: \n",
        "{focus}\n",
        "\n",
        "5. For the summary section:\n",
        "- Set up summary with general background / context related to the focus area of the analyst\n",
        "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
        "- Create a numbered list of source documents, as you use them\n",
        "- Do not mention the names of interviewers or experts\n",
        "- Aim for approximately 400 words maximum\n",
        "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
        "\n",
        "6. For the Comprehensive analysis section:\n",
        "- Provide a detailed examination of the information from the source documents.\n",
        "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
        "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
        "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
        "- Clearly explain the relevance of each point to the overall focus of the report.\n",
        "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
        "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
        "- Aim for at least 800 words to ensure the analysis is thorough.\n",
        "\n",
        "7. In the Sources section:\n",
        "- Include all sources used in your report\n",
        "- Provide full links to relevant websites or specific document paths\n",
        "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
        "- It will look like:\n",
        "\n",
        "### Sources\n",
        "[1] Link or Document name\n",
        "[2] Link or Document name\n",
        "\n",
        "8. Be sure to combine sources. For example this is not correct:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "\n",
        "There should be no redundant sources. It should simply be:\n",
        "\n",
        "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
        "        \n",
        "9. Final review:\n",
        "- Ensure the report follows the required structure\n",
        "- Include no preamble before the title of the report\n",
        "- Check that all guidelines have been followed\"\"\"\n",
        "\n",
        "\n",
        "def write_section(state: InterviewState):\n",
        "    \"\"\"Generates a structured report section based on interview content\"\"\"\n",
        "\n",
        "    # Get context and analyst from state\n",
        "    context = state[\"context\"]\n",
        "    analyst = state[\"analyst\"]\n",
        "\n",
        "    # Define system prompt for section writing\n",
        "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
        "    section = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_message),\n",
        "            HumanMessage(content=f\"Use this source to write your section: {context}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Add section to state\n",
        "    return {\"sections\": [section.content]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc292ede",
      "metadata": {},
      "source": [
        "### Building the Interview Graph\n",
        "Here's how to create and configure the interview execution graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "343f74be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Add nodes and edges\n",
        "interview_builder = StateGraph(InterviewState)\n",
        "\n",
        "# Define nodes\n",
        "interview_builder.add_node(\"ask_question\", generate_question)\n",
        "interview_builder.add_node(\"search_web\", search_web)\n",
        "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
        "interview_builder.add_node(\"answer_question\", generate_answer)\n",
        "interview_builder.add_node(\"save_interview\", save_interview)\n",
        "interview_builder.add_node(\"write_section\", write_section)\n",
        "\n",
        "# Configure flow\n",
        "interview_builder.add_edge(START, \"ask_question\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
        "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
        "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
        "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
        "interview_builder.add_conditional_edges(\n",
        "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
        ")\n",
        "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
        "interview_builder.add_edge(\"write_section\", END)\n",
        "\n",
        "# Create interview graph with memory\n",
        "memory = MemorySaver()\n",
        "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
        "    run_name=\"Conduct Interviews\"\n",
        ")\n",
        "\n",
        "# Visualize the graph\n",
        "visualize_graph(interview_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf40202",
      "metadata": {},
      "source": [
        "**Graph Structure**\n",
        "\n",
        "The interview process follows this flow:\n",
        "1. Question Generation\n",
        "2. Parallel Search (Web and ArXiv)\n",
        "3. Answer Generation\n",
        "4. Conditional Routing\n",
        "5. Interview Saving\n",
        "6. Section Writing\n",
        "\n",
        "**Key Components**\n",
        "- State Management: Uses InterviewState for tracking\n",
        "- Memory Persistence: Implements MemorySaver\n",
        "- Conditional Logic: Routes between questions and interview completion\n",
        "- Parallel Processing: Conducts simultaneous web and academic searches\n",
        "\n",
        "Note: Ensure the langgraph module is installed before running this code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c487bf",
      "metadata": {},
      "source": [
        "### Executing the Interview Graph\n",
        "Here's how to execute the graph and display the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cbced19b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Analyst(affiliation='Tech Startup', name='Teddy Lee', role='Entrepreneur', description='Focuses on the practical applications and business implications of adopting Modular RAG versus Naive RAG in startup environments. Concerned with scalability, cost-effectiveness, and innovation potential to maintain a competitive edge.')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select first analyst from the list\n",
        "analysts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6377d8b5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, I'm Alex Carter, a tech industry analyst. I'm keen to dive into the practical applications and business implications of adopting Modular RAG versus Naive RAG, particularly in a startup environment. Given your expertise, could you start by explaining the fundamental differences between Modular RAG and Naive RAG, especially in terms of scalability and cost-effectiveness?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "The consolidated table demonstrates that Modular RAG outperforms Naive RAG across all key metrics, making it a more effective and reliable solution for customer support chatbots. By adopting a modular approach, organizations can achieve better relevance, faster response times, greater scalability, and higher customer satisfaction.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\"/>\n",
            "Naive RAG🟩 Advanced RAG🟧 Modular RAG 🟥 | by Anix Lynch | Jan, 2025 | Medium · Step 3: Multi-Query Module · Step 4: Retrieve and Rank Data Naive RAG is the simplest RAG framework that combines document retrieval with LLM generation to produce context-aware answers. Upgrade to Advanced RAG — Incorporate query optimization and multi-query techniques. Pre-Retrieval Optimization — Add step-back prompting and HyDE (Hypothetical Embeddings) to improve query accuracy. Steps: Pre-Process ➡️ Multi-Query ➡️ Retrieve ➡️ Post-Process ➡️ Generate. response = llm.invoke(prompt.format(context=context, query=\"What is LangChain?\")) from langchain.retrievers.multi_query import MultiQueryRetriever   from langchain.chains import RetrievalQA  # Retrieval-Augmented Generation (RAG) 🟩 Flexible Modules: Customize components like query generation, retrieval, and memory management for specific tasks.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2409.11598v2\" date=\"2024-12-03\" authors=\"To Eun Kim, Fernando Diaz\"/>\n",
            "<Title>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG)\n",
            "systems. However, despite retrieval being a core component of RAG, much of the\n",
            "research in this area overlooks the extensive body of work on fair ranking,\n",
            "neglecting the importance of considering all stakeholders involved. This paper\n",
            "presents the first systematic evaluation of RAG systems integrated with fair\n",
            "rankings. We focus specifically on measuring the fair exposure of each relevant\n",
            "item across the rankings utilized by RAG systems (i.e., item-side fairness),\n",
            "aiming to promote equitable growth for relevant item providers. To gain a deep\n",
            "understanding of the relationship between item-fairness, ranking quality, and\n",
            "generation quality in the context of RAG, we analyze nine different RAG systems\n",
            "that incorporate fair rankings across seven distinct datasets. Our findings\n",
            "indicate that RAG systems with fair rankings can maintain a high level of\n",
            "generation quality and, in many cases, even outperform traditional RAG systems,\n",
            "despite the general trend of a tradeoff between ensuring fairness and\n",
            "maintaining system-effectiveness. We believe our insights lay the groundwork\n",
            "for responsible and equitable RAG systems and open new avenues for future\n",
            "research. We publicly release our codebase and dataset at\n",
            "https://github.com/kimdanny/Fair-RAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Towards Fair RAG: On the Impact of Fair Ranking\n",
            "in Retrieval-Augmented Generation\n",
            "To Eun Kim\n",
            "Carnegie Mellon University\n",
            "toeunk@cs.cmu.edu\n",
            "Fernando Diaz\n",
            "Carnegie Mellon University\n",
            "diazf@acm.org\n",
            "Abstract\n",
            "Many language models now enhance their responses with retrieval capabilities,\n",
            "leading to the widespread adoption of retrieval-augmented generation (RAG) systems.\n",
            "However, despite retrieval being a core component of RAG, much of the research\n",
            "in this area overlooks the extensive body of work on fair ranking, neglecting the\n",
            "importance of considering all stakeholders involved. This paper presents the first\n",
            "systematic evaluation of RAG systems integrated with fair rankings. We focus\n",
            "specifically on measuring the fair exposure of each relevant item across the rankings\n",
            "utilized by RAG systems (i.e., item-side fairness), aiming to promote equitable\n",
            "growth for relevant item providers. To gain a deep understanding of the relationship\n",
            "between item-fairness, ranking quality, and generation quality in the context of RAG,\n",
            "we analyze nine different RAG systems that incorporate fair rankings across seven\n",
            "distinct datasets. Our findings indicate that RAG systems with fair rankings can\n",
            "maintain a high level of generation quality and, in many cases, even outperform\n",
            "traditional RAG systems, despite the general trend of a tradeoff between ensuring\n",
            "fairness and maintaining system-effectiveness. We believe our insights lay the\n",
            "groundwork for responsible and equitable RAG systems and open new avenues for\n",
            "future research. We publicly release our codebase and dataset. 1\n",
            "1\n",
            "Introduction\n",
            "In recent years, the concept of fair ranking has emerged as a critical concern in modern information\n",
            "access systems [12]. However, despite its significance, fair ranking has yet to be thoroughly examined\n",
            "in the context of retrieval-augmented generation (RAG) [1, 29], a rapidly advancing trend in natural\n",
            "language processing (NLP) systems [27]. To understand why this is important, consider the RAG\n",
            "system in Figure 1, where a user asks a question about running shoes. A classic retrieval system\n",
            "might return several documents containing information from various running shoe companies. If the\n",
            "RAG system only selects the top two documents, then information from the remaining two relevant\n",
            "companies will not be relayed to the predictive model and will likely be omitted from its answer.\n",
            "The fair ranking literature refers to this situation as unfair because some relevant companies (i.e., in\n",
            "documents at position 3 and 4) receive less or no exposure compared to equally relevant company in\n",
            "the top position [12].\n",
            "Understanding the effect of fair ranking in RAG is fundamental to ensuring responsible and equitable\n",
            "NLP systems. Since retrieval results in RAG often underlie response attribution [15], unfair exposure\n",
            "of content to the RAG system can result in incomplete evidence in responses (thus compromising recall\n",
            "of potentially relevant information for users) or downstream representational harms (thus creating\n",
            "or reinforcing biases across the set of relevant entities). In situations where content providers are\n",
            "compensated for contributions to inference, there can be financial implications for the unfairness\n",
            "[4, 19, 31]. Indeed, the fair ranking literature indicates that these are precisely the harms that emerge\n",
            "1https://github.com/kimdanny/Fair-RAG\n",
            "arXiv:2409.11598v2  [cs.IR]  3 Dec 2024\n",
            "What are the best running shoes \n",
            "to buy for marathons?\n",
            "𝒅𝟏\n",
            "𝒅𝟐\n",
            "top-k \n",
            "truncation\n",
            "Here are some \n",
            "best options from \n",
            "company A and \n",
            "company B\n",
            "LM\n",
            "Corpus\n",
            "𝒅𝟏 (Company A)\n",
            "𝒅𝟐 (Company B)\n",
            "𝒅𝟑 (Company C)\n",
            "𝒅𝟒 (Company D)\n",
            "𝒅𝟓 (Company B)\n",
            "…\n",
            "Rel\n",
            "Non-Rel\n",
            "Rel\n",
            "Rel\n",
            "Company C and D\n",
            "We are also \n",
            "relevant!\n",
            "🤖\n",
            "🙁\n",
            "🧑💻\n",
            "Non-Rel\n",
            "Figure 1: Fairness concerns in RAG. A simplified example of how RAG models can ignore equally\n",
            "relevant items (d3 and d4) and always consume the fixed top-scoring items (d1 and d2) with the same\n",
            "order of ranking over the multiple user requests. This is due to the deterministic nature of the retrieval\n",
            "process and \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2410.13509v1\" date=\"2024-10-17\" authors=\"Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong\"/>\n",
            "<Title>\n",
            "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) has proven its effectiveness in\n",
            "mitigating hallucinations in Large Language Models (LLMs) by retrieving\n",
            "knowledge from external resources. To adapt LLMs for RAG pipelines, current\n",
            "approaches use instruction tuning to optimize LLMs, improving their ability to\n",
            "utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\n",
            "on equipping LLMs to handle diverse RAG tasks using different instructions.\n",
            "However, it trains RAG modules to overfit training signals and overlooks the\n",
            "varying data preferences among agents within the RAG system. In this paper, we\n",
            "propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\n",
            "systems by aligning data preferences between different RAG modules. DDR works\n",
            "by collecting the rewards to optimize each agent with a rollout method. This\n",
            "method prompts agents to sample some potential responses as perturbations,\n",
            "evaluates the impact of these perturbations on the whole RAG system, and\n",
            "subsequently optimizes the agent to produce outputs that improve the\n",
            "performance of the RAG system. Our experiments on various knowledge-intensive\n",
            "tasks demonstrate that DDR significantly outperforms the SFT method,\n",
            "particularly for LLMs with smaller-scale parameters that depend more on the\n",
            "retrieved knowledge. Additionally, DDR exhibits a stronger capability to align\n",
            "the data preference between RAG modules. The DDR method makes generation module\n",
            "more effective in extracting key information from documents and mitigating\n",
            "conflicts between parametric memory and external knowledge. All codes are\n",
            "available at https://github.com/OpenMatch/RAG-DDR.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Arxiv Preprint\n",
            "RAG-DDR:\n",
            "OPTIMIZING\n",
            "RETRIEVAL-AUGMENTED\n",
            "GENERATION\n",
            "USING\n",
            "DIFFERENTIABLE\n",
            "DATA\n",
            "RE-\n",
            "WARDS\n",
            "Xinze Li1∗Sen Mei1∗Zhenghao Liu1†\n",
            "Yukun Yan2†\n",
            "Shuo Wang2\n",
            "Shi Yu2\n",
            "Zheni Zeng2\n",
            "Hao Chen2\n",
            "Ge Yu1\n",
            "Zhiyuan Liu2\n",
            "Maosong Sun2\n",
            "Chenyan Xiong3\n",
            "1Department of Computer Science and Technology, Northeastern University, China\n",
            "2Department of Computer Science and Technology, Institute for AI, Tsinghua University, China\n",
            "Beijing National Research Center for Information Science and Technology, China\n",
            "3Language Technologies Institute, Carnegie Mellon University, United States\n",
            "ABSTRACT\n",
            "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigat-\n",
            "ing hallucinations in Large Language Models (LLMs) by retrieving knowledge\n",
            "from external resources. To adapt LLMs for RAG pipelines, current approaches\n",
            "use instruction tuning to optimize LLMs, improving their ability to utilize re-\n",
            "trieved knowledge. This supervised fine-tuning (SFT) approach focuses on equip-\n",
            "ping LLMs to handle diverse RAG tasks using different instructions. However,\n",
            "it trains RAG modules to overfit training signals and overlooks the varying data\n",
            "preferences among agents within the RAG system. In this paper, we propose a\n",
            "Differentiable Data Rewards (DDR) method, which end-to-end trains RAG sys-\n",
            "tems by aligning data preferences between different RAG modules. DDR works\n",
            "by collecting the rewards to optimize each agent with a rollout method. This\n",
            "method prompts agents to sample some potential responses as perturbations, eval-\n",
            "uates the impact of these perturbations on the whole RAG system, and subse-\n",
            "quently optimizes the agent to produce outputs that improve the performance of\n",
            "the RAG system. Our experiments on various knowledge-intensive tasks demon-\n",
            "strate that DDR significantly outperforms the SFT method, particularly for LLMs\n",
            "with smaller-scale parameters that depend more on the retrieved knowledge. Ad-\n",
            "ditionally, DDR exhibits a stronger capability to align the data preference be-\n",
            "tween RAG modules. The DDR method makes generation module more effec-\n",
            "tive in extracting key information from documents and mitigating conflicts be-\n",
            "tween parametric memory and external knowledge. All codes are available at\n",
            "https://github.com/OpenMatch/RAG-DDR.\n",
            "1\n",
            "INTRODUCTION\n",
            "Large Language Models (LLMs) have demonstrated impressive capabilities in language understand-\n",
            "ing, reasoning, and planning capabilities across a wide range of natural language processing (NLP)\n",
            "tasks (Achiam et al., 2023; Touvron et al., 2023; Hu et al., 2024). However, LLMs usually pro-\n",
            "duce incorrect responses due to hallucination (Ji et al., 2023; Xu et al., 2024c). To alleviate the\n",
            "problem, existing studies employ Retrieval Augmented Generation (RAG) (Lewis et al., 2020; Shi\n",
            "et al., 2023; Peng et al., 2023) to enhance the capability of LLMs and help LLMs access long-tailed\n",
            "knowledge and up-to-date knowledge from different data sources (Trivedi et al., 2023; He et al.,\n",
            "2021; Cai et al., 2019; Parvez et al., 2021). However, the conflict between retrieved knowledge and\n",
            "parametric memory usually misleads LLMs, challenging the effectiveness of RAG system (Li et al.,\n",
            "2022; Chen et al., 2023; Asai et al., 2024b).\n",
            "∗indicates equal contribution.\n",
            "†indicates corresponding author.\n",
            "1\n",
            "arXiv:2410.13509v1  [cs.CL]  17 Oct 2024\n",
            "Arxiv Preprint\n",
            "To ensure the effectiveness of RAG systems, existing research has focused on developing various\n",
            "agents to enhance the retrieval accuracy (Gao et al., 2024; Xu et al., 2023b; Jiang et al., 2023; Xu\n",
            "et al., 2024b). These approaches aim to refine retrieval results through query reformulation, rerank-\n",
            "ing candidate documents, summarizing the retrieved documents or performing additional retrieval\n",
            "steps to find more relevant information (Yan et al., 2024; Trivedi et al., 2023; Asai et al., 2023;\n",
            "Yu et al., 2023a). To optimize the RAG system, the methods independently optimize different RAG\n",
            "modules by using the EM method (Singh et al., 2021; Sachan et al., 2021) or build the instruct tuning\n",
            "dataset f\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, let's explore the differences and benefits of Modular RAG compared to Naive RAG, especially from a production standpoint.\n",
            "\n",
            "**Differences:**\n",
            "\n",
            "1. **Structure and Flexibility:**\n",
            "   - **Naive RAG:** This is the simplest form of Retrieval-Augmented Generation, which combines document retrieval with language model generation to produce context-aware answers. It typically involves a direct integration of retrieval and generation processes without much flexibility or customization [2].\n",
            "   - **Modular RAG:** In contrast, Modular RAG offers a more flexible and customizable approach. It allows for the configuration of individual components such as query generation, retrieval, and memory management tailored to specific tasks. This modularity supports a more robust and adaptable system [4].\n",
            "\n",
            "2. **Capabilities:**\n",
            "   - **Naive RAG:** While foundational, Naive RAG might lack advanced features such as multi-query techniques or pre-retrieval optimizations, which can enhance query accuracy and retrieval effectiveness [3].\n",
            "   - **Modular RAG:** It integrates these advanced techniques, enabling better relevance and faster response times through steps like pre-processing, multi-query, and post-processing. It also supports query optimization and ranking enhancements [4].\n",
            "\n",
            "**Benefits at Production Level:**\n",
            "\n",
            "1. **Scalability:**\n",
            "   - Modular RAG is inherently more scalable due to its flexible architecture. Startups can better manage increased loads and integrate new features or data sources without overhauling the entire system, which is crucial for scaling operations efficiently [4].\n",
            "\n",
            "2. **Cost-Effectiveness:**\n",
            "   - By optimizing retrieval and generation processes, Modular RAG can reduce computational costs. Efficient query handling and retrieval ranking decrease unnecessary data processing, which can lower operational costs [4].\n",
            "\n",
            "3. **Innovation Potential:**\n",
            "   - The modular nature allows startups to innovate by experimenting with different components independently without disrupting the entire system. This fosters a culture of continuous improvement and adaptation to new business needs or technologies [4].\n",
            "\n",
            "4. **Performance and Reliability:**\n",
            "   - With better relevance and faster response times, Modular RAG can significantly enhance user experience, leading to higher customer satisfaction. This is particularly beneficial in applications like customer support chatbots, where timely and accurate responses are critical [4].\n",
            "\n",
            "Overall, adopting Modular RAG over Naive RAG offers substantial advantages in adaptability, efficiency, and performance, which are vital for startups aiming to maintain a competitive edge.\n",
            "\n",
            "Sources:\n",
            "[1] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[2] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\n",
            "[3] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for the detailed explanation. Can you provide some specific examples of startups that have successfully implemented Modular RAG, and how it has impacted their scalability and cost-effectiveness?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2405.13576v1\" date=\"2024-05-22\" authors=\"Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou\"/>\n",
            "<Title>\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized\n",
            "framework for implementation, coupled with the inherently intricate RAG\n",
            "process, makes it challenging and time-consuming for researchers to compare and\n",
            "evaluate these approaches in a consistent environment. Existing RAG toolkits\n",
            "like LangChain and LlamaIndex, while available, are often heavy and unwieldy,\n",
            "failing to meet the personalized needs of researchers. In response to this\n",
            "challenge, we propose FlashRAG, an efficient and modular open-source toolkit\n",
            "designed to assist researchers in reproducing existing RAG methods and in\n",
            "developing their own RAG algorithms within a unified framework. Our toolkit\n",
            "implements 12 advanced RAG methods and has gathered and organized 32 benchmark\n",
            "datasets. Our toolkit has various features, including customizable modular\n",
            "framework, rich collection of pre-implemented RAG works, comprehensive\n",
            "datasets, efficient auxiliary pre-processing scripts, and extensive and\n",
            "standard evaluation metrics. Our toolkit and resources are available at\n",
            "https://github.com/RUC-NLPIR/FlashRAG.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "FlashRAG: A Modular Toolkit for Efficient\n",
            "Retrieval-Augmented Generation Research\n",
            "Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou∗\n",
            "Gaoling School of Artificial Intelligence\n",
            "Renmin University of China\n",
            "{jinjiajie, dou}@ruc.edu.cn, yutaozhu94@gmail.com\n",
            "Abstract\n",
            "With the advent of Large Language Models (LLMs), the potential of Retrieval\n",
            "Augmented Generation (RAG) techniques have garnered considerable research\n",
            "attention. Numerous novel algorithms and models have been introduced to enhance\n",
            "various aspects of RAG systems. However, the absence of a standardized framework\n",
            "for implementation, coupled with the inherently intricate RAG process, makes it\n",
            "challenging and time-consuming for researchers to compare and evaluate these\n",
            "approaches in a consistent environment. Existing RAG toolkits like LangChain\n",
            "and LlamaIndex, while available, are often heavy and unwieldy, failing to\n",
            "meet the personalized needs of researchers. In response to this challenge, we\n",
            "propose FlashRAG, an efficient and modular open-source toolkit designed to assist\n",
            "researchers in reproducing existing RAG methods and in developing their own\n",
            "RAG algorithms within a unified framework. Our toolkit implements 12 advanced\n",
            "RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit\n",
            "has various features, including customizable modular framework, rich collection\n",
            "of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-\n",
            "processing scripts, and extensive and standard evaluation metrics. Our toolkit and\n",
            "resources are available at https://github.com/RUC-NLPIR/FlashRAG.\n",
            "1\n",
            "Introduction\n",
            "In the era of large language models (LLMs), retrieval-augmented generation (RAG) [1, 2] has\n",
            "emerged as a robust solution to mitigate hallucination issues in LLMs by leveraging external\n",
            "knowledge bases [3]. The substantial applications and the potential of RAG technology have\n",
            "attracted considerable research attention. With the introduction of a large number of new algorithms\n",
            "and models to improve various facets of RAG systems in recent years, comparing and evaluating\n",
            "these methods under a consistent setting has become increasingly challenging.\n",
            "Many works are not open-source or have fixed settings in their open-source code, making it difficult\n",
            "to adapt to new data or innovative components. Besides, the datasets and retrieval corpus used often\n",
            "vary, with resources being scattered, which can lead researchers to spend excessive time on pre-\n",
            "processing steps instead of focusing on optimizing their methods. Furthermore, due to the complexity\n",
            "of RAG systems, involving multiple steps such as indexing, retrieval, and generation, researchers\n",
            "often need to implement many parts of the system themselves. Although there are some existing RAG\n",
            "toolkits like LangChain [4] and LlamaIndex [5], they are typically large and cumbersome, hindering\n",
            "researchers from implementing customized processes and failing to address the aforementioned issues.\n",
            "∗Corresponding author\n",
            "Preprint. Under review.\n",
            "arXiv:2405.13576v1  [cs.CL]  22 May 2024\n",
            "Thus, a unified, researcher-oriented RAG toolkit is urgently needed to streamline methodological\n",
            "development and comparative studies.\n",
            "To address the issue mentioned above, we introduce FlashRAG, an open-source library designed to\n",
            "enable researchers to easily reproduce existing RAG methods and develop their own RAG algorithms.\n",
            "This library allows researchers to utilize built pipelines to replicate existing work, employ provided\n",
            "RAG components to construct their own RAG processes, or simply use organized datasets and corpora\n",
            "to accelerate their own RAG workflow. Compared to existing RAG toolkits, FlashRAG is more suited\n",
            "for researchers. To summarize, the key features and capabilities of our FlashRAG library can be\n",
            "outlined in the following four aspects:\n",
            "Extensive and Customizable Modular RAG Framework.\n",
            "To facilitate an easily expandable\n",
            "RAG process, we implemented modular RAG at two levels. At the component level, we offer\n",
            "comprehensive RAG compon\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2402.09809v2\" date=\"2024-05-05\" authors=\"Owen Henkel, Hannah Horne-Robinson, Nessie Kozhakhmetova, Amanda Lee\"/>\n",
            "<Title>\n",
            "Effective and Scalable Math Support: Evidence on the Impact of an AI- Tutor on Math Achievement in Ghana\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This study evaluates the impact of Rori, an AI powered conversational math\n",
            "tutor accessible via WhatsApp, on the math performance of approximately 1,000\n",
            "students in grades 3-9 across 11 schools in Ghana. Each school was assigned to\n",
            "a treatment group or control group; the students in the control group continued\n",
            "their regular math instruction, while students in the treatment group engaged\n",
            "with Rori, for two 30-minute sessions per week over 8 months in addition to\n",
            "regular math instruction. We find that the math growth scores were\n",
            "substantially higher for the treatment group with an effect size of 0.37, and\n",
            "that the results were statistically significant (p < 0.001). The fact that Rori\n",
            "works with basic mobile devices on low-bandwidth data networks gives the\n",
            "intervention strong potential to support personalized learning on other\n",
            "low-and-middle-income countries (LMICs), where laptop ownership and high-speed\n",
            "internet - prerequisite for many video-centered learning platforms - remain\n",
            "extremely limited. While the results should be interpreted judiciously, as they\n",
            "only report on year 1 of the intervention, and future research is necessary to\n",
            "better understand which conditions are necessary for successful implementation,\n",
            "they do suggest that chat-based tutoring solutions leveraging artificial\n",
            "intelligence could offer a costeffective approach to enhancing learning\n",
            "outcomes for millions of students globally.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Effective and Scalable Math Support: Experimental          \n",
            "Evidence on the Impact of an AI- Math Tutor in Ghana \n",
            "Owen Henkel1, Hannah Horne-Robinson2, Nessie Kozhakhmetova, Amanda Lee3 \n",
            "1 University of Oxford \n",
            "2 Rising Academies \n",
            "3 J-PAL North America \n",
            "Abstract. This study is a preliminary evaluation of the impact of receiving \n",
            "extra math instruction provided by Rori, an AI-powered math tutor accessible via \n",
            "WhatsApp, on the math performance of approximately 500 students in Ghana. \n",
            "Students assigned to both the control and treatment groups continued their normal \n",
            "classes with identical curricula and classroom hours. Students in the treatment \n",
            "group were given access to a phone for one hour a week – during their study hall \n",
            "period – and were allowed to use Rori to independently study math.  All other \n",
            "aspects of the groups’ in-school experience were the same. We find that the math \n",
            "growth scores were substantially higher for the treatment group and statistically \n",
            "significant (p < 0.001). The effect size of 0.36 is considered large in the context \n",
            "of educational interventions: approximately equivalent to an extra year of learn-\n",
            "ing. Importantly, Rori works on basic mobile devices connected to low-band-\n",
            "width data networks, and the marginal cost of providing Rori is approximately \n",
            "$5 per student, making it a potentially scalable intervention in the context of \n",
            "LMICs’ education systems. While the results should be interpreted judiciously, \n",
            "as they only report on year 1 of the intervention, they do suggest that chat-based \n",
            "tutoring solutions leveraging AI could offer a cost-effective and operationally ef-\n",
            "ficient approach to enhancing learning outcomes for millions of students globally. \n",
            "Keywords: LLMs, conversational agents, mobile-learning \n",
            "1 \n",
            "Introduction \n",
            "Fewer than 15% of students in Africa achieve minimum proficiency in math by the end \n",
            "of middle school  [1]. Most students in the region are taught content that surpasses their \n",
            "ability level, have limited opportunities to practice new skills, and often do not receive \n",
            "the necessary feedback due to large class sizes and under-resourced teachers [1]. Re-\n",
            "search has long suggested that high-quality one-on-one instruction can significantly \n",
            "improve educational outcomes [2], [3], [4]. However, in many Low and Middle-Income \n",
            "Countries (LMICs), including West Africa, the low supply and high cost of quality \n",
            "tutors mean that many learners cannot access this type of support [5]. Thus, research is \n",
            "needed to identify affordable and feasible interventions appropriate for this space.  \n",
            "In recent years, interest has been increasing in technology-enabled approaches such \n",
            "as adaptive learning environments and virtual tutors to offer additional support to learn-\n",
            "ers where personal tutors are not available. These approaches have been used to varying \n",
            "degrees of success in supplementing traditional instruction, but, when they work, they \n",
            "2  Experimental Evidence on the Impact of an AI- Math Tutor in Ghana \n",
            " \n",
            "are extremely cost-effective, a consideration that is particularly important in LMIC con-\n",
            "texts. However, a major hurdle to extending the use of these types of tools to LMICs, \n",
            "has been the lack of widespread access to personal computers and reliable, high-speed \n",
            "internet that these learning environments typically require. In West Africa, for instance, \n",
            "less than 20% of the population has access to home computers and robust internet con-\n",
            "nections [6]. However, mobile phone usage is remarkably high in the region, with data \n",
            "indicating that around 90% of West Africans own a mobile phone and reside in areas \n",
            "with 3G coverage [6], [7]. This trend suggests the possibility of implementing interac-\n",
            "tive educational experiences on mobile platforms, circumventing the need for expen-\n",
            "sive personal computers and high-speed internet.  \n",
            "Rising Academies, an educational network based in Ghana, has created Rori, a free \n",
            "AI-powered math tutor available on WhatsApp. Inspired by successful elements of \n",
            "Teac\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
            "A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems | by Gaurav Nigam | aingineer | Dec, 2024 | Medium A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. This guide aims to provide an in-depth exploration of Modular RAG, from its foundational principles to practical implementation strategies, tailored for professionals with a keen interest in scaling enterprise AI systems.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Modular Retrieval-Augmented Generation (RAG) represents an evolution in the design and implementation of RAG systems. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.rest/@gauravnigam/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
            "This guide aims to provide an in-depth exploration of Modular RAG, from its foundational principles to practical implementation strategies, tailored for professionals with a keen interest in scaling enterprise AI systems.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "The context provided does not contain specific examples of startups that have successfully implemented Modular RAG. However, I can explain how the typical implementation of Modular RAG can impact scalability and cost-effectiveness in a startup environment.\n",
            "\n",
            "**Scalability:**\n",
            "Startups that adopt Modular RAG can handle increased data loads and user demands more efficiently. The modular architecture allows for easy expansion and integration of new data sources or functionalities without major system overhauls. This flexibility ensures that as a startup grows and its data requirements increase, the system can scale accordingly without significant disruptions or costs.\n",
            "\n",
            "**Cost-Effectiveness:**\n",
            "Modular RAG reduces costs by optimizing the retrieval and generation processes. By improving query accuracy and relevance, the system processes only the necessary data, thus lowering computational expenses. Furthermore, the modular approach means that startups can implement changes or enhancements to specific system components without needing to invest in a complete system redesign, which can be both time-consuming and costly.\n",
            "\n",
            "Overall, while I can't provide specific startup examples from the context, it's clear that Modular RAG's flexibility and efficiency make it a valuable tool for startups looking to scale effectively and manage costs efficiently. If you need more detailed examples, I recommend looking at case studies or industry reports that focus on AI and machine learning implementations in startups.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you so much for your help!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/1911.08973v1\" date=\"2019-11-20\" authors=\"Henry Edison, Xiaofeng Wang, Pekka Abrahamsson\"/>\n",
            "<Title>\n",
            "Product Innovation through Internal Startup in Large Software Companies: a Case Study\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Product innovation is a risky activity, but when successful, it enables large\n",
            "software companies accrue high profits and leapfrog the competition. Internal\n",
            "startups have been promoted as one way to foster product innovation in large\n",
            "companies, which allows them to innovate as startups do. However, internal\n",
            "startups in large companies are challenging endeavours despite of the promised\n",
            "benefits. How large software companies can leverage internal startups in\n",
            "software product innovation is not fully understood due to the scarcity of the\n",
            "relevant studies. Based on a conceptual framework that combines the elements\n",
            "from the Lean startup approach and an internal corporate venturing model, we\n",
            "conducted a case study of a large software company to examine how a new product\n",
            "was developed through the internal startup effort and struggled to achieve the\n",
            "desired outcomes set by the management. As a result, the conceptual framework\n",
            "was further developed into a Lean startup-enabled new product development model\n",
            "for large software companies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Product Innovation through Internal Startup in\n",
            "Large Software Companies: a Case Study\n",
            "Henry Edison∗‡, Xiaofeng Wang∗‡ and Pekka Abrahamsson†‡\n",
            "∗Free University of Bozen-Bolzano, Bolzano, 39100, Italy\n",
            "†Norwegian University of Science Technology, Trondheim, NO-7491, Norway\n",
            "‡Software Startups Research Network, http://softwarestartups.org\n",
            "Email: (henry.edison, xiaofeng.wang)@unibz.it, pekka.abrahamsson@id.ntnu.no\n",
            "!\n",
            "Abstract—Product innovation is a risky activity, but when successful,\n",
            "it enables large software companies accrue high proﬁts and leapfrog\n",
            "the competition. Internal startups have been promoted as one way\n",
            "to foster product innovation in large companies, which allows them to\n",
            "innovate as startups do. However, internal startups in large companies\n",
            "are challenging endeavours despite of the promised beneﬁts. How large\n",
            "software companies can leverage internal startups in software product\n",
            "innovation is not fully understood due to the scarcity of the relevant\n",
            "studies. Based on a conceptual framework that combines the elements\n",
            "from the Lean startup approach and an internal corporate venturing\n",
            "model, we conducted a case study of a large software company to\n",
            "examine how a new product was developed through the internal startup\n",
            "effort and struggled to achieve the desired outcomes set by the man-\n",
            "agement. As a result, the conceptual framework was further developed\n",
            "into a Lean startup-enabled new product development model for large\n",
            "software companies.\n",
            "Index Terms—software product innovation, internal startup, Lean\n",
            "startup, large software companies, case study\n",
            "1\n",
            "INTRODUCTION\n",
            "It is widely accepted that product innovation is vital to\n",
            "companies to sustain their competitive advantages (e.g.\n",
            "[1], [2], [3]). Product innovation refers to the creation and\n",
            "introduction of new (technologically new or signiﬁcantly\n",
            "improved) products which are different from existing prod-\n",
            "ucts [4], [5], [6]. Through product innovation, companies\n",
            "are able to create new market and entry barriers, challenge\n",
            "market leaders and leapfrog competition [7]. Companies\n",
            "are able to accrue high proﬁt because at the time a new\n",
            "product is released, there is no competition in the market\n",
            "until imitators produce similar products [8].\n",
            "* This is the authors version of the manuscript accepted for publication in the\n",
            "Proceedings of 42th Euromicro Conference on Software Engineering and Ad-\n",
            "vanced Application (SEAA), 2016. Copyright owner’s version can be accessed\n",
            "at https://doi.org/10.1109/SEAA.2016.36. This manuscript version is made\n",
            "available under the CC-BY-NC-ND 4.0 license. http://creativecommons.org/\n",
            "licenses/by-nc-nd/4.0\n",
            "** Please cite as: Henry Edison, Xiaofeng Wang, and Pekka Abrahamsson\n",
            "(2016). Product Innovation through Internal Startup in Large Software\n",
            "Companies: a Case Study. Proceedings of 42th Euromicro Conference on\n",
            "Software Engineering and Advanced Application (SEAA), 2016, 128–135.\n",
            "Developing product innovation is a risky activity [9],\n",
            "[10]. Many companies are too risk-averse to engage in any\n",
            "innovation initiatives [11]. As in automated factories, people\n",
            "in large companies are trained to do prescribed and speciﬁc\n",
            "tasks reliably. Hence, any endeavour to change the status-\n",
            "quo will emerge resistance. The implementation of an inno-\n",
            "vative idea must compete with other product development\n",
            "activities [12], [13]. This is the case for software industry,\n",
            "too. Little attention is given to product innovativeness [14].\n",
            "The studies on corporate entrepreneurship suggest that\n",
            "internal startup is an ideal environment to nurture inno-\n",
            "vation and entrepreneurship in large companies [15]. Al-\n",
            "though the internal startup is still operating within the\n",
            "corporation, the way of working is different with respect\n",
            "to the traditional research and development (R&D) system.\n",
            "An internal startup takes the responsibility from end to end;\n",
            "from ﬁnding a business idea to developing a new product\n",
            "and introducing it to market [16]. Therefore, internal startup\n",
            "is also seen as a learning process to create new \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2406.04369v1\" date=\"2024-05-31\" authors=\"Tilmann Bruckhaus\"/>\n",
            "<Title>\n",
            "RAG Does Not Work for Enterprises\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of\n",
            "large language model outputs by incorporating knowledge retrieval. However,\n",
            "implementing RAG in enterprises poses challenges around data security,\n",
            "accuracy, scalability, and integration. This paper explores the unique\n",
            "requirements for enterprise RAG, surveys current approaches and limitations,\n",
            "and discusses potential advances in semantic search, hybrid queries, and\n",
            "optimized retrieval. It proposes an evaluation framework to validate enterprise\n",
            "RAG solutions, including quantitative testing, qualitative analysis, ablation\n",
            "studies, and industry case studies. This framework aims to help demonstrate the\n",
            "ability of purpose-built RAG architectures to deliver accuracy and relevance\n",
            "improvements with enterprise-grade security, compliance and integration. The\n",
            "paper concludes with implications for enterprise deployments, limitations, and\n",
            "future research directions. Close collaboration between researchers and\n",
            "industry partners may accelerate progress in developing and deploying\n",
            "retrieval-augmented generation technology.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "‭RAG Does Not Work for Enterprises‬\n",
            "‭Tilmann Bruckhaus, Ph.D., Strative‬\n",
            "‭tilmann@strative.ai‬\n",
            "‭Abstract‬\n",
            "‭Retrieval-Augmented‬‭Generation‬‭(RAG)‬‭improves‬‭the‬‭accuracy‬‭and‬‭relevance‬‭of‬\n",
            "‭large‬ ‭language‬ ‭model‬ ‭outputs‬ ‭by‬ ‭incorporating‬ ‭knowledge‬ ‭retrieval.‬ ‭However,‬\n",
            "‭implementing‬ ‭RAG‬ ‭in‬ ‭enterprises‬ ‭poses‬ ‭challenges‬ ‭around‬ ‭data‬ ‭security,‬ ‭accuracy,‬\n",
            "‭scalability, and integration.‬\n",
            "‭This‬‭paper‬‭explores‬‭the‬‭unique‬‭requirements‬‭for‬‭enterprise‬‭RAG,‬‭surveys‬‭current‬\n",
            "‭approaches‬‭and‬‭limitations,‬‭and‬‭discusses‬‭potential‬‭advances‬‭in‬‭semantic‬‭search,‬‭hybrid‬\n",
            "‭queries,‬ ‭and‬ ‭optimized‬ ‭retrieval.‬ ‭It‬ ‭proposes‬ ‭an‬ ‭evaluation‬ ‭framework‬ ‭to‬ ‭validate‬\n",
            "‭enterprise‬‭RAG‬‭solutions,‬‭including‬‭quantitative‬‭testing,‬‭qualitative‬‭analysis,‬‭ablation‬\n",
            "‭studies,‬‭and‬‭industry‬‭case‬‭studies.‬‭This‬‭framework‬‭aims‬‭to‬‭help‬‭demonstrate‬‭the‬‭ability‬\n",
            "‭of‬ ‭purpose-built‬ ‭RAG‬ ‭architectures‬ ‭to‬‭deliver‬‭accuracy‬‭and‬‭relevance‬‭improvements‬\n",
            "‭with enterprise-grade security, compliance and integration.‬\n",
            "‭The‬‭paper‬‭concludes‬‭with‬‭implications‬‭for‬‭enterprise‬‭deployments,‬‭limitations,‬‭and‬\n",
            "‭future‬ ‭research‬ ‭directions.‬ ‭Close‬ ‭collaboration‬ ‭between‬ ‭researchers‬ ‭and‬ ‭industry‬\n",
            "‭partners‬ ‭may‬ ‭accelerate‬ ‭progress‬ ‭in‬ ‭developing‬ ‭and‬ ‭deploying‬ ‭retrieval-augmented‬\n",
            "‭generation technology.‬\n",
            "‭Keywords‬‭:‬‭Retrieval-Augmented‬‭Generation‬‭(RAG),‬‭Enterprise‬‭AI,‬‭Compliance-regulated‬‭industries,‬\n",
            "‭Semantic‬ ‭search,‬ ‭Hybrid‬ ‭query‬ ‭strategies,‬ ‭Enterprise‬ ‭integration,‬ ‭Accuracy‬ ‭and‬ ‭relevance,‬‭Evaluation‬\n",
            "‭frameworks,‬ ‭Healthcare‬ ‭applications,‬ ‭Financial‬ ‭services‬ ‭applications,‬ ‭Legal‬ ‭domain‬ ‭applications,‬\n",
            "‭Generative‬ ‭AI,‬ ‭Language‬ ‭models,‬ ‭Information‬ ‭retrieval,‬ ‭Enterprise‬ ‭knowledge‬ ‭bases,‬ ‭Regulatory‬\n",
            "‭compliance‬\n",
            "‭1. Introduction‬\n",
            "‭This‬‭section‬‭provides‬‭an‬‭overview‬‭of‬‭Retrieval-Augmented‬‭Generation‬‭(RAG)‬‭and‬‭its‬‭importance‬‭for‬\n",
            "‭enterprises.‬ ‭It‬ ‭discusses‬ ‭the‬ ‭key‬ ‭challenges‬ ‭in‬ ‭implementing‬ ‭RAG‬ ‭effectively‬ ‭in‬ ‭enterprise‬ ‭settings,‬\n",
            "‭particularly‬ ‭in‬‭compliance-regulated‬‭industries.‬‭The‬‭unique‬‭requirements‬‭and‬‭constraints‬‭for‬‭enterprise‬\n",
            "‭RAG‬‭are‬‭examined,‬‭highlighting‬‭the‬‭need‬‭for‬‭purpose-built‬‭solutions‬‭that‬‭address‬‭issues‬‭around‬‭accuracy,‬\n",
            "‭security,‬ ‭scalability,‬ ‭and‬ ‭integration.‬ ‭The‬ ‭introduction‬ ‭also‬ ‭sets‬ ‭the‬ ‭stage‬ ‭for‬ ‭exploring‬ ‭recent‬\n",
            "‭technological‬ ‭advances‬ ‭and‬ ‭innovations‬ ‭that‬ ‭can‬ ‭enable‬ ‭enterprises‬‭to‬‭overcome‬‭these‬‭challenges‬‭and‬\n",
            "‭unlock the transformative potential of RAG.‬\n",
            "‭1‬\n",
            "‭1.1 Brief overview of Retrieval-Augmented Generation (RAG) and its‬\n",
            "‭importance‬\n",
            "‭Retrieval-Augmented‬‭Generation‬‭(RAG)‬‭is‬‭an‬‭emerging‬‭paradigm‬‭in‬‭natural‬‭language‬‭processing‬‭and‬\n",
            "‭generative‬ ‭AI‬ ‭that‬ ‭combines‬ ‭the‬ ‭strengths‬ ‭of‬ ‭pre-trained‬ ‭language‬ ‭models‬ ‭with‬ ‭external‬ ‭knowledge‬\n",
            "‭retrieved‬‭from‬‭databases‬‭or‬‭document‬‭collections‬‭[‬‭Lewis‬‭et‬‭al.,‬‭2020‬‭].‬‭In‬‭a‬‭typical‬‭RAG‬‭architecture,‬‭a‬\n",
            "‭retriever‬‭component‬‭first‬‭selects‬‭the‬‭most‬‭relevant‬‭documents‬‭or‬‭passages‬‭based‬‭on‬‭the‬‭input‬‭query,‬‭and‬\n",
            "‭then‬ ‭a‬‭generator‬‭component‬‭conditions‬‭on‬‭both‬‭the‬‭query‬‭and‬‭the‬‭retrieved‬‭content‬‭to‬‭produce‬‭a‬‭final‬\n",
            "‭output [‬‭Izacard and Grave, 2021‬‭].‬\n",
            "‭RAG‬‭has‬‭shown‬‭significant‬‭promise‬‭in‬‭enhancing‬‭the‬‭factual‬‭accuracy,‬‭consistency,‬‭and‬‭contextual‬\n",
            "‭awareness‬‭of‬‭generative‬‭models‬‭across‬‭a‬‭wide‬‭range‬‭of‬‭applications,‬‭such‬‭as‬‭question‬‭answering,‬‭dialogue‬\n",
            "‭systems,‬‭and‬‭content‬‭creation‬‭[‬‭Zhao‬‭et‬‭al.,‬‭2024‬‭].‬‭By‬‭allowing‬‭the‬‭model‬‭to‬‭access‬‭and‬‭incorporate‬‭vast‬\n",
            "‭amounts‬ ‭of‬ ‭external‬ ‭knowledge‬ ‭on-demand,‬ ‭RAG‬ ‭can‬ ‭help‬ ‭mitigate‬ ‭issues‬ ‭like‬ ‭hallucination,‬\n",
            "‭inconsistency, and lack of grounding that often plague purely generative approaches [‬‭Shuster et al., 2021‬‭].‬\n",
            "‭However,‬‭implementing‬‭RAG‬‭effectively‬‭\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/1802.09393v1\" date=\"2018-02-23\" authors=\"Henry Edison, Nina M. Smørsgård, Xiaofeng Wang, Pekka Abrahamsson\"/>\n",
            "<Title>\n",
            "Lean Internal Startups for Software Product Innovation in Large Companies: Enablers and Inhibitors\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "To compete in this age of disruption, large companies cannot rely on cost\n",
            "efficiency, lead time reduction and quality improvement. They are now looking\n",
            "for ways to innovate like startups. Meanwhile, the awareness and use of the\n",
            "Lean startup approach have grown rapidly amongst the software startup community\n",
            "in recent years. This study investigates how Lean internal startup facilitates\n",
            "software product innovation in large companies and identifies its enablers and\n",
            "inhibitors. A multiple case study approach is followed in the investigation.\n",
            "Two software product innovation projects from two large companies are examined,\n",
            "using a conceptual framework that is based on the method-in-action framework\n",
            "and extended with the previously developed Lean-Internal Corporate Venture\n",
            "model. Seven face-to-face in-depth interviews of the employees with different\n",
            "roles are conducted. Within-case analysis and cross-case comparison are applied\n",
            "to draw the findings from the cases. A generic process flow summarises the\n",
            "common key processes of Lean internal startups. The findings suggest that an\n",
            "internal startup that is initiated management or employees faces different\n",
            "challenges. A list of enablers of applying Lean startup in large companies are\n",
            "identified, including top management support and cross-functional team. Both\n",
            "cases face different inhibitors due to the different process of inception,\n",
            "objective of the team and type of the product. Our contributions are threefold.\n",
            "First, this study is one of the first attempt to investigate the use of Lean\n",
            "startup approach in large companies empirically. Second, the study shows the\n",
            "potential of the method-in-action framework to investigate the Lean startup\n",
            "approach in non-startup context. The third is a general process of Lean\n",
            "internal startup and the evidence of the enablers and inhibitors of\n",
            "implementing it, which are both theory-informed and empirically grounded.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Lean Internal Startups for Software Product Innovation\n",
            "in Large Companies: Enablers and Inhibitors$,$$\n",
            "Henry Edisona,d,∗, Nina M. Smørsg˚ardb, Xiaofeng Wanga,d, Pekka\n",
            "Abrahamssonc,d\n",
            "aFree University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano 39100, Italy\n",
            "bNorwegian University of Science and Technology, Høgskoleringen 1, 7491 Trondheim,\n",
            "Norway\n",
            "cFaculty of Information Technology, P.O. Box 35, FI-40014, University of Jyv¨askyl¨a,\n",
            "Finland\n",
            "dSoftware Startup Research Network https: // softwarestartups. org/\n",
            "Abstract\n",
            "Context:\n",
            "Startups are disrupting traditional markets and replacing well-\n",
            "established actors with their innovative products.To compete in this age of\n",
            "disruption, large and established companies cannot rely on traditional ways\n",
            "of advancement, which focus on cost eﬃciency, lead time reduction and qual-\n",
            "ity improvement. Corporate management is now looking for possibilities to\n",
            "innovate like startups. Along with it, the awareness and the use of the Lean\n",
            "startup approach have grown rapidly amongst the software startup commu-\n",
            "nity and large companies in recent years.\n",
            "Objective:\n",
            "The aim of this study is to investigate how Lean internal\n",
            "startup facilitates software product innovation in large companies.\n",
            "This\n",
            "study also identiﬁes the enablers and inhibitors for Lean internal startups.\n",
            "$This is the authors’ version of the manuscript accepted for publication in the Journal\n",
            "of Systems and Software. Copyright owner’s version can be accessed at https://doi.\n",
            "org/10.1016/j.jss.2017.09.034.\n",
            "c⃝2018. This manuscript version is made available\n",
            "under the CC-BY-NC-ND 4.0 license.\n",
            "$$Please cite as: Edison. H, Smørsg˚ard, N. M., Wang, X. and Abrahamsson, P. (2018).\n",
            "Lean Internal Startups for Software Product Innovation in Large Companies: Enablers\n",
            "and Inhibitors. Journal of Systems and Software, 135:69–87.\n",
            "∗Corresponding author\n",
            "Email addresses: henry.edison@inf.unibz.it (Henry Edison),\n",
            "nina.m.smorsgard@gmail.com (Nina M. Smørsg˚ard), xiaofeng.wang@unibz.it\n",
            "(Xiaofeng Wang), pekka.abrahamsson@jyu.fi (Pekka Abrahamsson)\n",
            "Preprint submitted to Journal of Systems and Software\n",
            "February 27, 2018\n",
            "arXiv:1802.09393v1  [cs.CY]  23 Feb 2018\n",
            "Method: A multiple case study approach is followed in the investigation.\n",
            "Two software product innovation projects from two diﬀerent large companies\n",
            "are examined, using a conceptual framework that is based on the method-in-\n",
            "action framework and extended with the previously developed Lean-Internal\n",
            "Corporate Venture model. Seven face-to-face in-depth interviews of the em-\n",
            "ployees with diﬀerent roles and responsibilities are conducted. The collected\n",
            "data is analysed through a careful coding process. Within-case analysis and\n",
            "cross-case comparison are applied to draw the ﬁndings from the two cases.\n",
            "Results: A generic process ﬂow summarises the common key processes\n",
            "of Lean internal startups in the context of large companies. The ﬁndings\n",
            "suggest that an internal startup can be initiated top-down by management, or\n",
            "bottom-up by employees, which faces diﬀerent challenges. A list of enablers\n",
            "and inhibitors of applying Lean startup in large companies are identiﬁed,\n",
            "including top management support and cross-functional team as key enablers.\n",
            "Both cases face diﬀerent inhibitors due to the diﬀerent process of inception,\n",
            "objective of the team and type of the product.\n",
            "Conclusions: The contribution of this study for research is threefold.\n",
            "First, this study is one of the ﬁrst attempt to investigate the use of Lean\n",
            "startup approach in the context of large companies empirically. Second, the\n",
            "study shows the potential of the method-in-action framework to investigate\n",
            "the Lean startup approach in non-startup context. The third contribution is\n",
            "a general process of Lean internal startup and the evidence of the enablers\n",
            "and inhibitors of implementing it, which are both theory-informed and em-\n",
            "pirically grounded. Future studies could extend our study by addressing the\n",
            "limitations of the research approach undertaken in this study.\n",
            "Keywords:\n",
            "Lean start\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
            "A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems | by Gaurav Nigam | aingineer | Dec, 2024 | Medium A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. This guide aims to provide an in-depth exploration of Modular RAG, from its foundational principles to practical implementation strategies, tailored for professionals with a keen interest in scaling enterprise AI systems.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://ragaboutit.com/scaling-rag-for-big-data-techniques-and-strategies-for-handling-large-datasets/\"/>\n",
            "To ensure cost-effective scaling, organizations should adopt a modular and decoupled architecture for their RAG systems. By separating components such as data ingestion, preprocessing, retrieval, and generation into independent and scalable modules, organizations can selectively scale specific components based on demand, optimizing resource\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "The consolidated table demonstrates that Modular RAG outperforms Naive RAG across all key metrics, making it a more effective and reliable solution for customer support chatbots. By adopting a modular approach, organizations can achieve better relevance, faster response times, greater scalability, and higher customer satisfaction.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "You're welcome! If you have any more questions or need further assistance, feel free to ask. Good luck with your article!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at production level?\n",
            "AI: Hello, I'm Alex Carter, a tech industry analyst. I'm keen to dive into the practical applications and business implications of adopting Modular RAG versus Naive RAG, particularly in a startup environment. Given your expertise, could you start by explaining the fundamental differences between Modular RAG and Naive RAG, especially in terms of scalability and cost-effectiveness?\n",
            "AI: Certainly, let's explore the differences and benefits of Modular RAG compared to Naive RAG, especially from a production standpoint.\n",
            "\n",
            "**Differences:**\n",
            "\n",
            "1. **Structure and Flexibility:**\n",
            "   - **Naive RAG:** This is the simplest form of Retrieval-Augmented Generation, which combines document retrieval with language model generation to produce context-aware answers. It typically involves a direct integration of retrieval and generation processes without much flexibility or customization [2].\n",
            "   - **Modular RAG:** In contrast, Modular RAG offers a more flexible and customizable approach. It allows for the configuration of individual components such as query generation, retrieval, and memory management tailored to specific tasks. This modularity supports a more robust and adaptable system [4].\n",
            "\n",
            "2. **Capabilities:**\n",
            "   - **Naive RAG:** While foundational, Naive RAG might lack advanced features such as multi-query techniques or pre-retrieval optimizations, which can enhance query accuracy and retrieval effectiveness [3].\n",
            "   - **Modular RAG:** It integrates these advanced techniques, enabling better relevance and faster response times through steps like pre-processing, multi-query, and post-processing. It also supports query optimization and ranking enhancements [4].\n",
            "\n",
            "**Benefits at Production Level:**\n",
            "\n",
            "1. **Scalability:**\n",
            "   - Modular RAG is inherently more scalable due to its flexible architecture. Startups can better manage increased loads and integrate new features or data sources without overhauling the entire system, which is crucial for scaling operations efficiently [4].\n",
            "\n",
            "2. **Cost-Effectiveness:**\n",
            "   - By optimizing retrieval and generation processes, Modular RAG can reduce computational costs. Efficient query handling and retrieval ranking decrease unnecessary data processing, which can lower operational costs [4].\n",
            "\n",
            "3. **Innovation Potential:**\n",
            "   - The modular nature allows startups to innovate by experimenting with different components independently without disrupting the entire system. This fosters a culture of continuous improvement and adaptation to new business needs or technologies [4].\n",
            "\n",
            "4. **Performance and Reliability:**\n",
            "   - With better relevance and faster response times, Modular RAG can significantly enhance user experience, leading to higher customer satisfaction. This is particularly beneficial in applications like customer support chatbots, where timely and accurate responses are critical [4].\n",
            "\n",
            "Overall, adopting Modular RAG over Naive RAG offers substantial advantages in adaptability, efficiency, and performance, which are vital for startups aiming to maintain a competitive edge.\n",
            "\n",
            "Sources:\n",
            "[1] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[2] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\n",
            "[3] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "AI: Thank you for the detailed explanation. Can you provide some specific examples of startups that have successfully implemented Modular RAG, and how it has impacted their scalability and cost-effectiveness?\n",
            "AI: The context provided does not contain specific examples of startups that have successfully implemented Modular RAG. However, I can explain how the typical implementation of Modular RAG can impact scalability and cost-effectiveness in a startup environment.\n",
            "\n",
            "**Scalability:**\n",
            "Startups that adopt Modular RAG can handle increased data loads and user demands more efficiently. The modular architecture allows for easy expansion and integration of new data sources or functionalities without major system overhauls. This flexibility ensures that as a startup grows and its data requirements increase, the system can scale accordingly without significant disruptions or costs.\n",
            "\n",
            "**Cost-Effectiveness:**\n",
            "Modular RAG reduces costs by optimizing the retrieval and generation processes. By improving query accuracy and relevance, the system processes only the necessary data, thus lowering computational expenses. Furthermore, the modular approach means that startups can implement changes or enhancements to specific system components without needing to invest in a complete system redesign, which can be both time-consuming and costly.\n",
            "\n",
            "Overall, while I can't provide specific startup examples from the context, it's clear that Modular RAG's flexibility and efficiency make it a valuable tool for startups looking to scale effectively and manage costs efficiently. If you need more detailed examples, I recommend looking at case studies or industry reports that focus on AI and machine learning implementations in startups.\n",
            "AI: Thank you so much for your help!\n",
            "AI: You're welcome! If you have any more questions or need further assistance, feel free to ask. Good luck with your article!\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Modular RAG vs. Naive RAG: Business Implications for Startups\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly advancing field of artificial intelligence, the adoption of Retrieval-Augmented Generation (RAG) systems has become a focal point for startups aiming to leverage large language models (LLMs) for their business operations. Specifically, the transition from Naive RAG to Modular RAG presents significant opportunities for scalability, cost-effectiveness, and innovation, essential for maintaining a competitive edge. The analysis of recent studies and theoretical frameworks reveals intriguing insights into the practical applications and business implications of these systems, particularly in startup environments. This report synthesizes the findings from several key documents, highlighting novel methodologies and the transformative potential of Modular RAG.\n",
            "\n",
            "1. **A Theory for Token-Level Harmonization in Retrieval-Augmented Generation** [1]: This document introduces a theoretical framework for understanding the benefit and detriment dynamics in RAG systems, proposing a novel Tok-RAG method that enhances token-level generation.\n",
            "\n",
            "2. **Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation** [2]: This paper explores the integration of fair ranking within RAG systems, emphasizing the balance between fairness and quality in content generation.\n",
            "\n",
            "3. **RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards** [3]: Presents a Differentiable Data Rewards method to optimize RAG systems, showcasing improved alignment of data preferences across RAG modules.\n",
            "\n",
            "4. **Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks** [4]: Introduces the modular RAG framework, highlighting its ability to decompose RAG systems into independent, reconfigurable modules.\n",
            "\n",
            "5. **How Does Modular RAG Improve Upon Naive RAG?** [5]: Discusses the advantages of Modular RAG in terms of relevance, response time, scalability, and customer satisfaction, particularly in customer support applications.\n",
            "\n",
            "6. **A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems** [6]: Provides a detailed exploration of Modular RAG's foundational principles and practical implementation strategies, tailored for enterprise scalability.\n",
            "\n",
            "These documents collectively contribute to a deeper understanding of how Modular RAG can fundamentally alter the landscape for startups by offering more flexible, efficient, and scalable AI solutions.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Theoretical Foundations and Practical Applications\n",
            "\n",
            "1. **Theoretical Advancements in RAG** [1]:\n",
            "   - The introduction of a theoretical framework for RAG, focusing on the balance between benefit and detriment, addresses the core challenges faced by startups using RAG systems. The Tok-RAG method enhances token-level generation by predicting the net effect of RAG without additional training, offering a cost-effective solution for startups.\n",
            "\n",
            "2. **Fairness in RAG Systems** [2]:\n",
            "   - The integration of fair ranking mechanisms into RAG systems highlights the importance of equitable content exposure. For startups, especially those in content generation and customer service, ensuring fairness can lead to higher satisfaction and trust among users. The findings suggest that fair RAG systems can outperform traditional ones by maintaining high generation quality.\n",
            "\n",
            "#### Optimization and Performance Enhancement\n",
            "\n",
            "3. **Optimization through Differentiable Data Rewards (DDR)** [3]:\n",
            "   - The DDR method significantly improves RAG systems by aligning data preferences and enhancing retrieval accuracy. This method is particularly beneficial for startups with limited computational resources, as it optimizes the performance of smaller-scale LLMs that rely heavily on retrieved knowledge.\n",
            "\n",
            "4. **Modular RAG: A Leap Forward** [4], [5]:\n",
            "   - Modular RAG introduces a reconfigurable framework, enabling startups to decompose and reassemble RAG systems as needed. This flexibility allows for targeted improvements, such as enhancing retrieval mechanisms or optimizing generation modules. The modular approach not only improves system efficiency but also facilitates innovation by allowing startups to experiment with different configurations.\n",
            "\n",
            "5. **Scalability and Implementation Strategies** [6]:\n",
            "   - By decoupling retrieval, reasoning, and generation, Modular RAG systems enable startups to scale AI solutions efficiently. The guide provides practical strategies for implementing these systems, emphasizing the importance of a modular architecture in managing large datasets and adapting to evolving business needs.\n",
            "\n",
            "#### Business Implications and Innovation Potential\n",
            "\n",
            "- **Scalability and Cost-Effectiveness**:\n",
            "  - Modular RAG offers startups the ability to scale specific components based on demand, optimizing resource allocation and reducing costs. This selective scaling is crucial for startups that must balance growth with financial constraints.\n",
            "\n",
            "- **Competitive Edge through Innovation**:\n",
            "  - The flexibility of Modular RAG allows startups to innovate rapidly, adapting to market changes and developing new products or services. This adaptability is essential for maintaining a competitive edge in the fast-paced tech industry.\n",
            "\n",
            "- **Customer Satisfaction and Market Adaptation**:\n",
            "  - Enhanced relevance and response times, as demonstrated by Modular RAG, lead to higher customer satisfaction. Startups can leverage this advantage to improve customer support and engagement, ultimately driving business success.\n",
            "\n",
            "Overall, the transition from Naive to Modular RAG systems represents a significant advancement for startups, providing a framework for scalable, cost-effective, and innovative AI solutions that can drive business growth and competitive advantage.\n",
            "\n",
            "### Sources\n",
            "[1] http://arxiv.org/abs/2406.00944v2  \n",
            "[2] http://arxiv.org/abs/2409.11598v2  \n",
            "[3] http://arxiv.org/abs/2410.13509v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# Set research topic\n",
        "topic = \"What are the differences between Modular RAG and Naive RAG, and what are the benefits of using it at production level\"\n",
        "\n",
        "# Create initial interview message\n",
        "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
        "\n",
        "# Configure thread ID\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=100,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "# Execute graph\n",
        "invoke_graph(\n",
        "    interview_graph,\n",
        "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 5},\n",
        "    config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4090d7",
      "metadata": {},
      "source": [
        "Display completed interview section in markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "2eef4a1f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Modular RAG vs. Naive RAG: Business Implications for Startups\n",
              "\n",
              "### Summary\n",
              "\n",
              "In the rapidly advancing field of artificial intelligence, the adoption of Retrieval-Augmented Generation (RAG) systems has become a focal point for startups aiming to leverage large language models (LLMs) for their business operations. Specifically, the transition from Naive RAG to Modular RAG presents significant opportunities for scalability, cost-effectiveness, and innovation, essential for maintaining a competitive edge. The analysis of recent studies and theoretical frameworks reveals intriguing insights into the practical applications and business implications of these systems, particularly in startup environments. This report synthesizes the findings from several key documents, highlighting novel methodologies and the transformative potential of Modular RAG.\n",
              "\n",
              "1. **A Theory for Token-Level Harmonization in Retrieval-Augmented Generation** [1]: This document introduces a theoretical framework for understanding the benefit and detriment dynamics in RAG systems, proposing a novel Tok-RAG method that enhances token-level generation.\n",
              "\n",
              "2. **Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation** [2]: This paper explores the integration of fair ranking within RAG systems, emphasizing the balance between fairness and quality in content generation.\n",
              "\n",
              "3. **RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards** [3]: Presents a Differentiable Data Rewards method to optimize RAG systems, showcasing improved alignment of data preferences across RAG modules.\n",
              "\n",
              "4. **Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks** [4]: Introduces the modular RAG framework, highlighting its ability to decompose RAG systems into independent, reconfigurable modules.\n",
              "\n",
              "5. **How Does Modular RAG Improve Upon Naive RAG?** [5]: Discusses the advantages of Modular RAG in terms of relevance, response time, scalability, and customer satisfaction, particularly in customer support applications.\n",
              "\n",
              "6. **A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems** [6]: Provides a detailed exploration of Modular RAG's foundational principles and practical implementation strategies, tailored for enterprise scalability.\n",
              "\n",
              "These documents collectively contribute to a deeper understanding of how Modular RAG can fundamentally alter the landscape for startups by offering more flexible, efficient, and scalable AI solutions.\n",
              "\n",
              "### Comprehensive Analysis\n",
              "\n",
              "#### Theoretical Foundations and Practical Applications\n",
              "\n",
              "1. **Theoretical Advancements in RAG** [1]:\n",
              "   - The introduction of a theoretical framework for RAG, focusing on the balance between benefit and detriment, addresses the core challenges faced by startups using RAG systems. The Tok-RAG method enhances token-level generation by predicting the net effect of RAG without additional training, offering a cost-effective solution for startups.\n",
              "\n",
              "2. **Fairness in RAG Systems** [2]:\n",
              "   - The integration of fair ranking mechanisms into RAG systems highlights the importance of equitable content exposure. For startups, especially those in content generation and customer service, ensuring fairness can lead to higher satisfaction and trust among users. The findings suggest that fair RAG systems can outperform traditional ones by maintaining high generation quality.\n",
              "\n",
              "#### Optimization and Performance Enhancement\n",
              "\n",
              "3. **Optimization through Differentiable Data Rewards (DDR)** [3]:\n",
              "   - The DDR method significantly improves RAG systems by aligning data preferences and enhancing retrieval accuracy. This method is particularly beneficial for startups with limited computational resources, as it optimizes the performance of smaller-scale LLMs that rely heavily on retrieved knowledge.\n",
              "\n",
              "4. **Modular RAG: A Leap Forward** [4], [5]:\n",
              "   - Modular RAG introduces a reconfigurable framework, enabling startups to decompose and reassemble RAG systems as needed. This flexibility allows for targeted improvements, such as enhancing retrieval mechanisms or optimizing generation modules. The modular approach not only improves system efficiency but also facilitates innovation by allowing startups to experiment with different configurations.\n",
              "\n",
              "5. **Scalability and Implementation Strategies** [6]:\n",
              "   - By decoupling retrieval, reasoning, and generation, Modular RAG systems enable startups to scale AI solutions efficiently. The guide provides practical strategies for implementing these systems, emphasizing the importance of a modular architecture in managing large datasets and adapting to evolving business needs.\n",
              "\n",
              "#### Business Implications and Innovation Potential\n",
              "\n",
              "- **Scalability and Cost-Effectiveness**:\n",
              "  - Modular RAG offers startups the ability to scale specific components based on demand, optimizing resource allocation and reducing costs. This selective scaling is crucial for startups that must balance growth with financial constraints.\n",
              "\n",
              "- **Competitive Edge through Innovation**:\n",
              "  - The flexibility of Modular RAG allows startups to innovate rapidly, adapting to market changes and developing new products or services. This adaptability is essential for maintaining a competitive edge in the fast-paced tech industry.\n",
              "\n",
              "- **Customer Satisfaction and Market Adaptation**:\n",
              "  - Enhanced relevance and response times, as demonstrated by Modular RAG, lead to higher customer satisfaction. Startups can leverage this advantage to improve customer support and engagement, ultimately driving business success.\n",
              "\n",
              "Overall, the transition from Naive to Modular RAG systems represents a significant advancement for startups, providing a framework for scalable, cost-effective, and innovative AI solutions that can drive business growth and competitive advantage.\n",
              "\n",
              "### Sources\n",
              "[1] http://arxiv.org/abs/2406.00944v2  \n",
              "[2] http://arxiv.org/abs/2409.11598v2  \n",
              "[3] http://arxiv.org/abs/2410.13509v1  \n",
              "[4] http://arxiv.org/abs/2407.21059v1  \n",
              "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
              "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(interview_graph.get_state(config).values[\"sections\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "eec5b106",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Modular RAG vs. Naive RAG: Business Implications for Startups\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly advancing field of artificial intelligence, the adoption of Retrieval-Augmented Generation (RAG) systems has become a focal point for startups aiming to leverage large language models (LLMs) for their business operations. Specifically, the transition from Naive RAG to Modular RAG presents significant opportunities for scalability, cost-effectiveness, and innovation, essential for maintaining a competitive edge. The analysis of recent studies and theoretical frameworks reveals intriguing insights into the practical applications and business implications of these systems, particularly in startup environments. This report synthesizes the findings from several key documents, highlighting novel methodologies and the transformative potential of Modular RAG.\n",
            "\n",
            "1. **A Theory for Token-Level Harmonization in Retrieval-Augmented Generation** [1]: This document introduces a theoretical framework for understanding the benefit and detriment dynamics in RAG systems, proposing a novel Tok-RAG method that enhances token-level generation.\n",
            "\n",
            "2. **Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation** [2]: This paper explores the integration of fair ranking within RAG systems, emphasizing the balance between fairness and quality in content generation.\n",
            "\n",
            "3. **RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards** [3]: Presents a Differentiable Data Rewards method to optimize RAG systems, showcasing improved alignment of data preferences across RAG modules.\n",
            "\n",
            "4. **Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks** [4]: Introduces the modular RAG framework, highlighting its ability to decompose RAG systems into independent, reconfigurable modules.\n",
            "\n",
            "5. **How Does Modular RAG Improve Upon Naive RAG?** [5]: Discusses the advantages of Modular RAG in terms of relevance, response time, scalability, and customer satisfaction, particularly in customer support applications.\n",
            "\n",
            "6. **A Comprehensive Guide to Implementing Modular RAG for Scalable AI Systems** [6]: Provides a detailed exploration of Modular RAG's foundational principles and practical implementation strategies, tailored for enterprise scalability.\n",
            "\n",
            "These documents collectively contribute to a deeper understanding of how Modular RAG can fundamentally alter the landscape for startups by offering more flexible, efficient, and scalable AI solutions.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Theoretical Foundations and Practical Applications\n",
            "\n",
            "1. **Theoretical Advancements in RAG** [1]:\n",
            "   - The introduction of a theoretical framework for RAG, focusing on the balance between benefit and detriment, addresses the core challenges faced by startups using RAG systems. The Tok-RAG method enhances token-level generation by predicting the net effect of RAG without additional training, offering a cost-effective solution for startups.\n",
            "\n",
            "2. **Fairness in RAG Systems** [2]:\n",
            "   - The integration of fair ranking mechanisms into RAG systems highlights the importance of equitable content exposure. For startups, especially those in content generation and customer service, ensuring fairness can lead to higher satisfaction and trust among users. The findings suggest that fair RAG systems can outperform traditional ones by maintaining high generation quality.\n",
            "\n",
            "#### Optimization and Performance Enhancement\n",
            "\n",
            "3. **Optimization through Differentiable Data Rewards (DDR)** [3]:\n",
            "   - The DDR method significantly improves RAG systems by aligning data preferences and enhancing retrieval accuracy. This method is particularly beneficial for startups with limited computational resources, as it optimizes the performance of smaller-scale LLMs that rely heavily on retrieved knowledge.\n",
            "\n",
            "4. **Modular RAG: A Leap Forward** [4], [5]:\n",
            "   - Modular RAG introduces a reconfigurable framework, enabling startups to decompose and reassemble RAG systems as needed. This flexibility allows for targeted improvements, such as enhancing retrieval mechanisms or optimizing generation modules. The modular approach not only improves system efficiency but also facilitates innovation by allowing startups to experiment with different configurations.\n",
            "\n",
            "5. **Scalability and Implementation Strategies** [6]:\n",
            "   - By decoupling retrieval, reasoning, and generation, Modular RAG systems enable startups to scale AI solutions efficiently. The guide provides practical strategies for implementing these systems, emphasizing the importance of a modular architecture in managing large datasets and adapting to evolving business needs.\n",
            "\n",
            "#### Business Implications and Innovation Potential\n",
            "\n",
            "- **Scalability and Cost-Effectiveness**:\n",
            "  - Modular RAG offers startups the ability to scale specific components based on demand, optimizing resource allocation and reducing costs. This selective scaling is crucial for startups that must balance growth with financial constraints.\n",
            "\n",
            "- **Competitive Edge through Innovation**:\n",
            "  - The flexibility of Modular RAG allows startups to innovate rapidly, adapting to market changes and developing new products or services. This adaptability is essential for maintaining a competitive edge in the fast-paced tech industry.\n",
            "\n",
            "- **Customer Satisfaction and Market Adaptation**:\n",
            "  - Enhanced relevance and response times, as demonstrated by Modular RAG, lead to higher customer satisfaction. Startups can leverage this advantage to improve customer support and engagement, ultimately driving business success.\n",
            "\n",
            "Overall, the transition from Naive to Modular RAG systems represents a significant advancement for startups, providing a framework for scalable, cost-effective, and innovative AI solutions that can drive business growth and competitive advantage.\n",
            "\n",
            "### Sources\n",
            "[1] http://arxiv.org/abs/2406.00944v2  \n",
            "[2] http://arxiv.org/abs/2409.11598v2  \n",
            "[3] http://arxiv.org/abs/2410.13509v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[6] https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e  \n"
          ]
        }
      ],
      "source": [
        "print(interview_graph.get_state(config).values[\"sections\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f982d5",
      "metadata": {},
      "source": [
        "### Parallel Interviewing by `map-reduce`\n",
        "Here's how to implement parallel interviews using map-reduce in LangGraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ede721fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class ResearchGraphState(TypedDict):\n",
        "    \"\"\"State definition for research graph\"\"\"\n",
        "\n",
        "    # Research topic\n",
        "    topic: str\n",
        "    # Maximum number of analysts\n",
        "    max_analysts: int\n",
        "    # Human analyst feedback\n",
        "    human_analyst_feedback: str\n",
        "    # List of questioning analysts\n",
        "    analysts: List[Analyst]\n",
        "    # List of sections containing Send() API keys\n",
        "    sections: Annotated[list, operator.add]\n",
        "    # Report components\n",
        "    introduction: str\n",
        "    content: str\n",
        "    conclusion: str\n",
        "    final_report: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3827de29",
      "metadata": {},
      "source": [
        "Let me explain how the `Send()` function is used in LangGraph for parallel interview execution:\n",
        "\n",
        "**Reference**\n",
        "- [LangGraph `Send()`](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "d4389d5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.constants import Send\n",
        "\n",
        "\n",
        "def initiate_all_interviews(state: ResearchGraphState):\n",
        "    \"\"\"Initiates parallel interviews for all analysts\"\"\"\n",
        "\n",
        "    # Check for human feedback\n",
        "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
        "\n",
        "    # Return to analyst creation if human feedback exists\n",
        "    if human_analyst_feedback:\n",
        "        return \"create_analysts\"\n",
        "\n",
        "    # Otherwise, initiate parallel interviews using Send()\n",
        "    else:\n",
        "        topic = state[\"topic\"]\n",
        "        return [\n",
        "            Send(\n",
        "                \"conduct_interview\",\n",
        "                {\n",
        "                    \"analyst\": analyst,\n",
        "                    \"messages\": [\n",
        "                        HumanMessage(\n",
        "                            content=f\"So you said you were writing an article on {topic}?\"\n",
        "                        )\n",
        "                    ],\n",
        "                },\n",
        "            )\n",
        "            for analyst in state[\"analysts\"]\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb746ea",
      "metadata": {},
      "source": [
        "## Report Writing\n",
        "\n",
        "Next, we will define the guidelines for writing a report based on the interview content and define a function for report writing.\n",
        "\n",
        "### Define Nodes\n",
        "- Main Report Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8c0d3b46",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
        "\n",
        "{topic}\n",
        "\n",
        "You have a team of analysts. Each analyst has done two things:\n",
        "\n",
        "1. They conducted an interview with an expert on a specific sub-topic.\n",
        "2. They write up their finding into a memo.\n",
        "\n",
        "Your task:\n",
        "\n",
        "1. You will be given a collection of memos from your analysts.  \n",
        "2. Carefully review and analyze the insights from each memo.  \n",
        "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
        "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
        "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
        "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
        "\n",
        "**Sections to consider (including optional ones for greater depth):**\n",
        "\n",
        "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
        "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
        "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
        "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
        "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
        "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
        "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
        "\n",
        "To format your report:\n",
        "\n",
        "1. Use markdown formatting.\n",
        "2. Include no pre-amble for the report.\n",
        "3. Use no sub-heading.\n",
        "4. Start your report with a single title header: ## Insights\n",
        "5. Do not mention any analyst names in your report.\n",
        "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
        "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
        "8. List your sources in order and do not repeat.\n",
        "\n",
        "[1] Source 1\n",
        "[2] Source 2\n",
        "\n",
        "Here are the memos from your analysts to build your report from:\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "\n",
        "def write_report(state: ResearchGraphState):\n",
        "    \"\"\"Generates main report content from interview sections\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Combine all sections\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    # Generate report from sections\n",
        "    system_message = report_writer_instructions.format(\n",
        "        topic=topic, context=formatted_str_sections\n",
        "    )\n",
        "    report = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=system_message),\n",
        "            HumanMessage(content=\"Write a report based upon these memos.\"),\n",
        "        ]\n",
        "    )\n",
        "    return {\"content\": report.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978dbdf4",
      "metadata": {},
      "source": [
        "- Introduction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "be9672e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
        "\n",
        "You will be given all of the sections of the report.\n",
        "\n",
        "You job is to write a crisp and compelling introduction or conclusion section.\n",
        "\n",
        "The user will instruct you whether to write the introduction or conclusion.\n",
        "\n",
        "Include no pre-amble for either section.\n",
        "\n",
        "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
        "\n",
        "Use markdown formatting.\n",
        "\n",
        "For your introduction, create a compelling title and use the # header for the title.\n",
        "\n",
        "For your introduction, use ## Introduction as the section header.\n",
        "\n",
        "For your conclusion, use ## Conclusion as the section header.\n",
        "\n",
        "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
        "\n",
        "\n",
        "def write_introduction(state: ResearchGraphState):\n",
        "    \"\"\"Creates report introduction\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(\n",
        "        topic=topic, formatted_str_sections=formatted_str_sections\n",
        "    )\n",
        "    intro = llm.invoke(\n",
        "        [instructions, HumanMessage(content=\"Write the report introduction\")]\n",
        "    )\n",
        "    return {\"introduction\": intro.content}\n",
        "\n",
        "\n",
        "def write_conclusion(state: ResearchGraphState):\n",
        "    \"\"\"Creates report conclusion\"\"\"\n",
        "    sections = state[\"sections\"]\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
        "\n",
        "    instructions = intro_conclusion_instructions.format(\n",
        "        topic=topic, formatted_str_sections=formatted_str_sections\n",
        "    )\n",
        "    conclusion = llm.invoke(\n",
        "        [instructions, HumanMessage(content=\"Write the report conclusion\")]\n",
        "    )\n",
        "    return {\"conclusion\": conclusion.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf3d5e8",
      "metadata": {},
      "source": [
        "- Final Report Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "29bbf7e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def finalize_report(state: ResearchGraphState):\n",
        "    \"\"\"Assembles final report with all components\"\"\"\n",
        "    content = state[\"content\"]\n",
        "\n",
        "    # Clean up content formatting\n",
        "    if content.startswith(\"## Insights\"):\n",
        "        content = content.strip(\"## Insights\")\n",
        "\n",
        "    # Handle sources section\n",
        "    if \"## Sources\" in content:\n",
        "        try:\n",
        "            content, sources = content.split(\"\\n## Sources\\n\")\n",
        "        except:\n",
        "            sources = None\n",
        "    else:\n",
        "        sources = None\n",
        "\n",
        "    # Assemble final report\n",
        "    final_report = (\n",
        "        state[\"introduction\"]\n",
        "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
        "        + content\n",
        "        + \"\\n\\n---\\n\\n\"\n",
        "        + state[\"conclusion\"]\n",
        "    )\n",
        "\n",
        "    # Add sources if available\n",
        "    if sources is not None:\n",
        "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
        "\n",
        "    return {\"final_report\": final_report}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83bbe53",
      "metadata": {},
      "source": [
        "Each function handles a specific aspect of report generation:\n",
        "- Content synthesis from interview sections\n",
        "- Introduction creation\n",
        "- Conclusion development\n",
        "- Final assembly with proper formatting and structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef82f56",
      "metadata": {},
      "source": [
        "### Building the Report Writing Graph\n",
        "Here's the implementation of the research graph that orchestrates the entire workflow: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "c59ca60f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.constants import START, END\n",
        "\n",
        "# Create graph\n",
        "builder = StateGraph(ResearchGraphState)\n",
        "\n",
        "# Define nodes\n",
        "builder.add_node(\"create_analysts\", create_analysts)\n",
        "builder.add_node(\"human_feedback\", human_feedback)\n",
        "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
        "builder.add_node(\"write_report\", write_report)\n",
        "builder.add_node(\"write_introduction\", write_introduction)\n",
        "builder.add_node(\"write_conclusion\", write_conclusion)\n",
        "builder.add_node(\"finalize_report\", finalize_report)\n",
        "\n",
        "# Define edges\n",
        "builder.add_edge(START, \"create_analysts\")\n",
        "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
        "builder.add_conditional_edges(\n",
        "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
        ")\n",
        "\n",
        "# Report generation from interviews\n",
        "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
        "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
        "\n",
        "# Final report assembly\n",
        "builder.add_edge(\n",
        "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
        ")\n",
        "builder.add_edge(\"finalize_report\", END)\n",
        "\n",
        "# Compile graph\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
        "\n",
        "# Visualize the workflow\n",
        "visualize_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4444392d",
      "metadata": {},
      "source": [
        "The graph structure implements:\n",
        "\n",
        "**Core Workflow Stages**\n",
        "- Analyst Creation\n",
        "- Human Feedback Integration\n",
        "- Parallel Interview Execution\n",
        "- Report Generation\n",
        "- Final Assembly\n",
        "\n",
        "**Key Components**\n",
        "- State Management using ResearchGraphState\n",
        "- Memory persistence with MemorySaver\n",
        "- Conditional routing based on human feedback\n",
        "- Parallel processing of interviews\n",
        "- Synchronized report assembly\n",
        "\n",
        "**Flow Control**\n",
        "- Starts with analyst creation\n",
        "- Allows for human feedback and iteration\n",
        "- Conducts parallel interviews\n",
        "- Generates report components simultaneously\n",
        "- Assembles final report with all components\n",
        "\n",
        "This implementation creates a robust workflow for automated research with human oversight and parallel processing capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd98ba7",
      "metadata": {},
      "source": [
        "### Executing the Report Writing  Graph\n",
        "\n",
        " Here's how to run the graph with the specified parameters: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "32489294",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='Tech Research Institute' name='Dr. Emily Wong' role='AI System Architect' description='Focuses on the architectural differences between Modular RAG and traditional Naive RAG, analyzing how modularity impacts system design and efficiency.'\n",
            "affiliation='Data Science Consultancy' name='Mr. James Patel' role='Data Integration Specialist' description='Examines the benefits of Modular RAG in production environments, particularly in terms of integration flexibility and real-time data adaptability.'\n",
            "affiliation='AI Ethics Foundation' name='Ms. Clara Johnson' role='Ethical AI Advocate' description='Concerned with the ethical implications of deploying Modular RAG in real-world applications, ensuring that modularity does not compromise ethical standards and data privacy.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Set input parameters\n",
        "max_analysts = 3\n",
        "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
        "\n",
        "# Configure execution settings\n",
        "config = RunnableConfig(\n",
        "    recursion_limit=30,\n",
        "    configurable={\"thread_id\": random_uuid()},\n",
        ")\n",
        "\n",
        "# Prepare input data\n",
        "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
        "\n",
        "# Execute graph until first breakpoint\n",
        "invoke_graph(graph, inputs, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2794ed52",
      "metadata": {},
      "source": [
        "Let's add `human_feedback` to customize the analyst team and continue the graph execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "43f717a3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '58fde4a4-a06a-433e-9dad-bdad40897347',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd8ceb-cfa8-67a4-8002-053a4bfb86cd'}}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add new analyst with human feedback\n",
        "graph.update_state(\n",
        "    config,\n",
        "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
        "    as_node=\"human_feedback\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "af4526f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "affiliation='University of Toronto' name='Prof. Jeffrey Hinton' role='Head of AI Analyst' description='Prof. Hinton focuses on the foundational principles and future directions of AI models. He is particularly interested in how Modular RAG can enhance efficiency and scalability in AI systems compared to traditional Naive RAG, with an emphasis on production-level applications.'\n",
            "affiliation='OpenAI' name='Dr. Emma Thompson' role='Scalability Expert' description='Dr. Thompson concentrates on the scalability aspects of AI models, examining how Modular RAG can be more efficiently scaled in production environments than traditional Naive RAG. She is motivated by improving model performance and reducing computational overhead.'\n",
            "affiliation='DeepMind' name='Dr. Raj Patel' role='Efficiency Analyst' description='Dr. Patel is dedicated to analyzing the efficiency and optimization of AI architectures. His focus is on how Modular RAG can offer better efficiency and resource management at the production level, surpassing the limitations of Naive RAG.'\n",
            "affiliation='MIT AI Lab' name='Dr. Lisa Nguyen' role='Production Systems Specialist' description='Dr. Nguyen specializes in the deployment and integration of AI systems in real-world applications. She explores the benefits of using Modular RAG at the production level, ensuring robustness and reliability compared to traditional Naive RAG approaches.'\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Continue graph execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f53e4c9",
      "metadata": {},
      "source": [
        "Let's complete the human feedback phase and resume the graph execution: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "05725a6d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '58fde4a4-a06a-433e-9dad-bdad40897347',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1efd8cec-034d-6c60-8004-b1efc8a67975'}}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# End human feedback phase\n",
        "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1e2f4ee1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, my name is Alex Carter, and I'm an analyst eager to delve deeper into the world of AI models. Thank you for taking the time to speak with me, Prof. Hinton. Let's start with the basics: Could you explain what Modular RAG is and how it fundamentally differs from traditional Naive RAG?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, my name is Alex Reed, and I'm an analyst exploring the intricacies of AI scalability. I'm very interested in understanding how Modular RAG differs from traditional Naive RAG and the benefits it offers, particularly in production environments. Dr. Thompson, could you elaborate on the key differences between Modular RAG and Naive RAG? Specifically, how does the modular approach enhance scalability?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, I'm Alex Thompson, an analyst exploring the intricacies of AI deployment in real-world scenarios. Dr. Nguyen, could you start by explaining how Modular RAG specifically differs from the traditional Naive RAG in the context of AI systems?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Dr. Patel, my name is Alex Green, and I'm a technology writer keen on delving deeper into the nuances of AI architectures. I understand that you have a focus on efficiency and optimization, particularly regarding Modular RAG and Naive RAG. Could you explain how Modular RAG differs from traditional Naive RAG in terms of efficiency and resource management? Are there any specific examples or case studies you could share to illustrate these differences?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Retrieval-augmented generation (RAG) has emerged as a powerful technique that combines the strengths of information retrieval and natural language generation. However, not all RAG implementations are created equal. The traditional or \"Naive\" RAG, while groundbreaking, often struggles with limitations such as inflexibility and inefficiencies in handling diverse and dynamic datasets.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\"/>\n",
            "Naive RAG🟩 Advanced RAG🟧 Modular RAG 🟥 | by Anix Lynch | Jan, 2025 | Medium · Step 3: Multi-Query Module · Step 4: Retrieve and Rank Data Naive RAG is the simplest RAG framework that combines document retrieval with LLM generation to produce context-aware answers. Upgrade to Advanced RAG — Incorporate query optimization and multi-query techniques. Pre-Retrieval Optimization — Add step-back prompting and HyDE (Hypothetical Embeddings) to improve query accuracy. Steps: Pre-Process ➡️ Multi-Query ➡️ Retrieve ➡️ Post-Process ➡️ Generate. response = llm.invoke(prompt.format(context=context, query=\"What is LangChain?\")) from langchain.retrievers.multi_query import MultiQueryRetriever   from langchain.chains import RetrievalQA  # Retrieval-Augmented Generation (RAG) 🟩 Flexible Modules: Customize components like query generation, retrieval, and memory management for specific tasks.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\"/>\n",
            "6 Progression of RAG Systems: Naïve to Advanced, and Modular RAG · A Simple Guide to Retrieval Augmented Generation 6 Progression of RAG Systems: Naïve to Advanced, and Modular RAG Limitations of Naïve RAG approach Advanced RAG strategies and techniques Modular patterns in RAG The basic, or the Naïve RAG approach that we have discussed is, generally, inadequate when it comes to production-grade systems. In this chapter, we will begin by revisiting the limitations and the points of failure of the Naïve RAG approach. Advanced strategies and techniques to address these points of failure will be understood in distinct phases of the RAG pipeline. 6.1 Limitations of Naïve RAG 6.2 Advanced RAG techniques 6.3 Modular RAG\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Learn about the key differences between Modular and Naive RAG, case study, and the significant advantages of Modular RAG.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "==================================================\n",
            "ArXiv search error: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf'\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Error>Failed to retrieve ArXiv search results.</Error>\n",
            "==================================================\n",
            "ArXiv search error: [WinError 32] 다른 프로세스가 파일을 사용 중이기 때문에 프로세스가 액세스 할 수 없습니다: './2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf'\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Error>Failed to retrieve ArXiv search results.</Error>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2401.08406v3\" date=\"2024-01-30\" authors=\"Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, Ranveer Chandra\"/>\n",
            "<Title>\n",
            "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "There are two common ways in which developers are incorporating proprietary\n",
            "and domain-specific data when building applications of Large Language Models\n",
            "(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the\n",
            "prompt with the external data, while fine-Tuning incorporates the additional\n",
            "knowledge into the model itself. However, the pros and cons of both approaches\n",
            "are not well understood. In this paper, we propose a pipeline for fine-tuning\n",
            "and RAG, and present the tradeoffs of both for multiple popular LLMs, including\n",
            "Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,\n",
            "including extracting information from PDFs, generating questions and answers,\n",
            "using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We\n",
            "propose metrics to assess the performance of different stages of the RAG and\n",
            "fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.\n",
            "Agriculture as an industry has not seen much penetration of AI, and we study a\n",
            "potentially disruptive application - what if we could provide location-specific\n",
            "insights to a farmer? Our results show the effectiveness of our dataset\n",
            "generation pipeline in capturing geographic-specific knowledge, and the\n",
            "quantitative and qualitative benefits of RAG and fine-tuning. We see an\n",
            "accuracy increase of over 6 p.p. when fine-tuning the model and this is\n",
            "cumulative with RAG, which increases accuracy by 5 p.p. further. In one\n",
            "particular experiment, we also demonstrate that the fine-tuned model leverages\n",
            "information from across geographies to answer specific questions, increasing\n",
            "answer similarity from 47% to 72%. Overall, the results point to how systems\n",
            "built using LLMs can be adapted to respond and incorporate knowledge across a\n",
            "dimension that is critical for a specific industry, paving the way for further\n",
            "applications of LLMs in other industrial domains.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "RAG VS FINE-TUNING: PIPELINES, TRADEOFFS, AND A CASE\n",
            "STUDY ON AGRICULTURE\n",
            "Microsoft\n",
            "Angels Balaguer, Vinamra Benara, Renato Cunha, Roberto Estevão, Todd Hendry, Daniel Holstein,\n",
            "Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp,\n",
            "Bruno Silva, Swati Sharma, Vijay Aski, Ranveer Chandra\n",
            "ABSTRACT\n",
            "There are two common ways in which developers are incorporating proprietary and domain-specific\n",
            "data when building applications of Large Language Models (LLMs): Retrieval-Augmented Genera-\n",
            "tion (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning\n",
            "incorporates the additional knowledge into the model itself. However, the pros and cons of both\n",
            "approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and\n",
            "present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4.\n",
            "Our pipeline consists of multiple stages, including extracting information from PDFs, generating\n",
            "questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results.\n",
            "We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline.\n",
            "We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen\n",
            "much penetration of AI, and we study a potentially disruptive application - what if we could provide\n",
            "location-specific insights to a farmer? Our results show the effectiveness of our dataset generation\n",
            "pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of\n",
            "RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this\n",
            "is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment,\n",
            "we also demonstrate that the fine-tuned model leverages information from across geographies to\n",
            "answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point\n",
            "to how systems built using LLMs can be adapted to respond and incorporate knowledge across a\n",
            "dimension that is critical for a specific industry, paving the way for further applications of LLMs in\n",
            "other industrial domains.\n",
            "Keywords GPT-4 · Agriculture · Retrieval Augmented Generation · Fine-tuning\n",
            "1\n",
            "Introduction\n",
            "Over the past few years, artificial intelligence and natural language processing have seen significant advancements,\n",
            "leading to the development of powerful large language models (LLMs) such as the Generative Pre-trained Transformer\n",
            "(GPT). The technology driving LLMs, including advanced deep learning techniques, large-scale transformers, and\n",
            "vast amounts of data, have propelled their rapid evolution. Models like GPT-4 (OpenAI, 2023) and Llama 2 (Touvron\n",
            "et al., 2023b) have demonstrated exceptional performance across numerous tasks and domains, often without specific\n",
            "prompts. These models surpass their predecessors and hold immense potential in various fields like coding, medicine,\n",
            "law, agriculture, and psychology, closely approaching human-level expertise (Bubeck et al., 2023; Nori et al., 2023;\n",
            "Demszky et al., 2023). As LLM research continues, it is critical to identify their limitations and address the challenges\n",
            "of developing more comprehensive artificial general intelligence (AGI) systems. Moreover, the machine learning\n",
            "community must move beyond traditional benchmarking datasets and evaluate LLMs in ways that closely resemble\n",
            "human cognitive ability assessments.\n",
            "The adoption of Artificial Intelligence (AI) copilots across various industries is revolutionizing the way businesses\n",
            "operate and interact with their environment. These AI copilots, powered by LLMs, provide invaluable assistance in\n",
            "arXiv:2401.08406v3  [cs.CL]  30 Jan 2024\n",
            "data processing and decision-making processes. In healthcare, for example, AI copilots are being leveraged to predict\n",
            "patient risks and improve diagnostic accuracy (Kim et al., 2023; Thirunavukarasu et al., 2023; Alo\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2406.00944v2\" date=\"2024-10-17\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
            "<Title>\n",
            "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
            "large language models (LLMs). Studies show that while RAG provides valuable\n",
            "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
            "or incorrect retrieved texts. Although many existing methods attempt to\n",
            "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
            "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
            "black box that cannot be quantified or compared in an explainable manner, so\n",
            "existing methods are data-driven, need additional utility evaluators or\n",
            "post-hoc. This paper takes the first step towards providing a theory to explain\n",
            "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
            "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
            "texts. Then, we formalize the trade-off between the value of external knowledge\n",
            "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
            "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
            "that the actual effect of RAG on the token, which is the comparison between\n",
            "benefit and detriment, can be predicted without any training or accessing the\n",
            "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
            "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
            "at token level to preserve benefit and avoid detriment. Experiments in\n",
            "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
            "RETRIEVAL-AUGMENTED GENERATION\n",
            "Shicheng Xu\n",
            "Liang Pang∗Huawei Shen\n",
            "Xueqi Cheng\n",
            "CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS\n",
            "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
            "ABSTRACT\n",
            "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
            "language models (LLMs). Studies show that while RAG provides valuable external\n",
            "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
            "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
            "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
            "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
            "be quantified or compared in an explainable manner, so existing methods are data-\n",
            "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
            "towards providing a theory to explain and trade off the benefit and detriment in\n",
            "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
            "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
            "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
            "(detriment) in next token prediction of RAG by distribution difference in this\n",
            "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
            "comparison between benefit and detriment, can be predicted without any training or\n",
            "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
            "method, Tok-RAG, which achieves collaborative generation between the pure\n",
            "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
            "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
            "effectiveness of our method and support our theoretical findings.\n",
            "1\n",
            "INTRODUCTION\n",
            "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
            "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
            "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
            "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
            "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024a; Xie et al., 2023; Jin et al.,\n",
            "2024b). Existing methods attempt to preserve benefit and avoid detriment by adding utility evaluators\n",
            "for retrieval, prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al.,\n",
            "2024b; Yoran et al., 2024; Ren et al., 2023; Feng et al., 2023; Mallen et al., 2022; Jiang et al., 2023).\n",
            "However, existing methods are data-driven, need evaluator for utility of retrieved texts or post-hoc. A\n",
            "theory-based method, focusing on core principles of RAG is urgently needed, which is crucial for\n",
            "consistent and reliable improvements without relying on additional training or utility evaluators and\n",
            "improving our understanding for RAG.\n",
            "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
            "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
            "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
            "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
            "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
            "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
            "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
            "the distribution completion measures how much out-of-distribution knowledge that retrieved texts\n",
            "∗Corresponding author\n",
            "1\n",
            "arXiv:2406.00944v2  [cs.CL]  17 Oct 2024\n",
            "Query\n",
            "Wole\n",
            "Query\n",
            "Ernst\n",
            "Soyinka\n",
            "…\n",
            "LLM’s \n",
            "Distribution\n",
            "Retrieved \n",
            "Distribution \n",
            "Fusion\n",
            "Distribution\n",
            "Difference\n",
            "Olanipekun\n",
            "LLM’s \n",
            "Dis\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.19994v3\" date=\"2024-09-13\" authors=\"Cheonsu Jeong\"/>\n",
            "<Title>\n",
            "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This study aims to improve knowledge-based question-answering (QA) systems by\n",
            "overcoming the limitations of existing Retrieval-Augmented Generation (RAG)\n",
            "models and implementing an advanced RAG system based on Graph technology to\n",
            "develop high-quality generative AI services. While existing RAG models\n",
            "demonstrate high accuracy and fluency by utilizing retrieved information, they\n",
            "may suffer from accuracy degradation as they generate responses using\n",
            "pre-loaded knowledge without reprocessing. Additionally, they cannot\n",
            "incorporate real-time data after the RAG configuration stage, leading to issues\n",
            "with contextual understanding and biased information. To address these\n",
            "limitations, this study implemented an enhanced RAG system utilizing Graph\n",
            "technology. This system is designed to efficiently search and utilize\n",
            "information. Specifically, it employs LangGraph to evaluate the reliability of\n",
            "retrieved information and synthesizes diverse data to generate more accurate\n",
            "and enhanced responses. Furthermore, the study provides a detailed explanation\n",
            "of the system's operation, key implementation steps, and examples through\n",
            "implementation code and validation results, thereby enhancing the understanding\n",
            "of advanced RAG technology. This approach offers practical guidelines for\n",
            "implementing advanced RAG systems in corporate services, making it a valuable\n",
            "resource for practical application.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            " \n",
            "* Corresponding Author: Cheonsu Jeong; paripal@korea.ac.kr \n",
            " \n",
            " \n",
            "1  \n",
            " \n",
            "A Study on the Implementation Method \n",
            "of an Agent-Based Advanced RAG  \n",
            "System Using Graph \n",
            "Cheonsu Jeong1 \n",
            " \n",
            " \n",
            " \n",
            "1 Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS; \n",
            " \n",
            "Abstract  \n",
            "This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of \n",
            "existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on \n",
            "Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high ac-\n",
            "curacy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate \n",
            "responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data \n",
            "after the RAG configuration stage, leading to issues with contextual understanding and biased information. \n",
            "To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system \n",
            "is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability \n",
            "of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, \n",
            "the study provides a detailed explanation of the system's operation, key implementation steps, and examples through \n",
            "implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This \n",
            "approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valu-\n",
            "able resource for practical application. \n",
            " \n",
            "Keywords \n",
            "Advance RAG; Agent RAG; LLM; Generative AI; LangGraph \n",
            " \n",
            " \n",
            "I. Introduction \n",
            "Recent advancements in AI technology have brought sig-\n",
            "nificant attention to Generative AI. Generative AI, a form \n",
            "of artificial intelligence that can create new content such \n",
            "as text, images, audio, and video based on vast amounts \n",
            "of trained data models (Jeong, 2023d), is being applied in \n",
            "various fields, including daily conversations, finance, \n",
            "healthcare, education, and entertainment (Ahn & Park, \n",
            "2023). As generative AI services become more accessible \n",
            "to the general public, the role of generative AI-based chat-\n",
            "bots is becoming increasingly important (Adam et al., \n",
            "2021; Przegalinska et al., 2019; Park, 2024). A chatbot is \n",
            "an intelligent agent that allows users to have conversa-\n",
            "tions typically through text or voice (Sánchez-Díaz et al., \n",
            "2018; Jeong & Jeong, 2020). Recently, generative AI \n",
            "chatbots have advanced to the level of analyzing human \n",
            "emotions and intentions to provide responses (Jeong, \n",
            "2023a). With the advent of large language models \n",
            "(LLMs), these chatbots can now be utilized for automatic \n",
            " \n",
            "Cheonsu Jeong \n",
            " \n",
            "2  \n",
            " \n",
            "dialogue generation and translation (Jeong, 2023b). How-\n",
            "ever, they may generate responses that conflict with the \n",
            "latest information and have a low understanding of new \n",
            "problems or domains as they rely on previously trained \n",
            "data (Jeong, 2023c). While 2023 was marked by the re-\n",
            "lease of foundational large language models (LLMs) like \n",
            "ChatGPT and Llama-2, experts predict that 2024 will be \n",
            "the year of Retrieval Augmented Generation (RAG) and \n",
            "AI Agents (Skelter Labs, 2024). \n",
            "However, there are several considerations for companies \n",
            "looking to adopt generative AI services. Companies must \n",
            "address concerns such as whether the AI can provide ac-\n",
            "curate responses based on internal data, the potential risk \n",
            "of internal data leakage, and how to integrate generative \n",
            "AI with corporate systems. Solutions include using do-\n",
            "main-specific fine-tuned LLMs and enhancing reliability \n",
            "with RAG that utilizes internal information (Jung, 2024). \n",
            "When domain-specific information is fine-tuned on GPT-\n",
            "4 LLM, accuracy improves from 75% to 81%, and adding \n",
            "RAG can further increase accuracy to 86% (Angels et al., \n",
            "2024). RAG models are known for effectively combinin\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.10670v1\" date=\"2024-07-15\" authors=\"Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu\"/>\n",
            "<Title>\n",
            "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) techniques leverage the in-context\n",
            "learning capabilities of large language models (LLMs) to produce more accurate\n",
            "and relevant responses. Originating from the simple 'retrieve-then-read'\n",
            "approach, the RAG framework has evolved into a highly flexible and modular\n",
            "paradigm. A critical component, the Query Rewriter module, enhances knowledge\n",
            "retrieval by generating a search-friendly query. This method aligns input\n",
            "questions more closely with the knowledge base. Our research identifies\n",
            "opportunities to enhance the Query Rewriter module to Query Rewriter+ by\n",
            "generating multiple queries to overcome the Information Plateaus associated\n",
            "with a single query and by rewriting questions to eliminate Ambiguity, thereby\n",
            "clarifying the underlying intent. We also find that current RAG systems exhibit\n",
            "issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\n",
            "Filter. These two modules are both based on the instruction-tuned Gemma-2B\n",
            "model, which together enhance response quality. The final identified issue is\n",
            "Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the\n",
            "Retriever Trigger to solve this. The former supports the dynamic expansion of\n",
            "the RAG system's knowledge base in a parameter-free manner, while the latter\n",
            "optimizes the cost for accessing external knowledge, thereby improving resource\n",
            "utilization and response efficiency. These four RAG modules synergistically\n",
            "improve the response quality and efficiency of the RAG system. The\n",
            "effectiveness of these modules has been validated through experiments and\n",
            "ablation studies across six common QA datasets. The source code can be accessed\n",
            "at https://github.com/Ancientshi/ERM4.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Enhancing Retrieval and Managing Retrieval:\n",
            "A Four-Module Synergy for Improved Quality and\n",
            "Efficiency in RAG Systems\n",
            "Yunxiao Shia, Xing Zia, Zijing Shia, Haimin Zhanga, Qiang Wua and Min Xua,*\n",
            "aUniversity of Technology Sydney, Broadway, Sydney, 2007, NSW, Australia.\n",
            "Abstract. Retrieval-augmented generation (RAG) techniques lever-\n",
            "age the in-context learning capabilities of large language models\n",
            "(LLMs) to produce more accurate and relevant responses. Originat-\n",
            "ing from the simple ’retrieve-then-read’ approach, the RAG frame-\n",
            "work has evolved into a highly flexible and modular paradigm. A\n",
            "critical component, the Query Rewriter module, enhances knowl-\n",
            "edge retrieval by generating a search-friendly query. This method\n",
            "aligns input questions more closely with the knowledge base. Our\n",
            "research identifies opportunities to enhance the Query Rewriter mod-\n",
            "ule to Query Rewriter+ by generating multiple queries to over-\n",
            "come the Information Plateaus associated with a single query and\n",
            "by rewriting questions to eliminate Ambiguity, thereby clarifying\n",
            "the underlying intent. We also find that current RAG systems ex-\n",
            "hibit issues with Irrelevant Knowledge; to overcome this, we pro-\n",
            "pose the Knowledge Filter. These two modules are both based on\n",
            "the instructional-tuned Gemma-2B model, which together enhance\n",
            "response quality. The final identified issue is Redundant Retrieval;\n",
            "we introduce the Memory Knowledge Reservoir and the Retriever\n",
            "Trigger to solve this. The former supports the dynamic expansion\n",
            "of the RAG system’s knowledge base in a parameter-free manner,\n",
            "while the latter optimizes the cost for accessing external knowl-\n",
            "edge, thereby improving resource utilization and response efficiency.\n",
            "These four RAG modules synergistically improve the response qual-\n",
            "ity and efficiency of the RAG system. The effectiveness of these\n",
            "modules has been validated through experiments and ablation studies\n",
            "across six common QA datasets. The source code can be accessed at\n",
            "https://github.com/Ancientshi/ERM4.\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) represent a significant leap in ar-\n",
            "tificial intelligence, with breakthroughs in generalization and adapt-\n",
            "ability across diverse tasks [4, 6]. However, challenges such as hal-\n",
            "lucinations [32], temporal misalignments [27], context processing\n",
            "issues [1], and fine-tuning inefficiencies [8] have raised significant\n",
            "concerns about their reliability. In response, recent research has fo-\n",
            "cused on enhancing LLMs’ capabilities by integrating them with ex-\n",
            "ternal knowledge sources through Retrieval-Augmented Generation\n",
            "(RAG) [2, 20, 13, 15]. This approach significantly improves LLMs’\n",
            "ability to answer questions more accurately and contextually.\n",
            "The basic RAG system comprises a knowledge retrieval mod-\n",
            "ule and a read module, forming the retrieve-then-read pipeline\n",
            "∗Corresponding author with email: Min.Xu@uts.edu.au.\n",
            "[20, 15, 13]. However, this vanilla pipeline has low retrieval qual-\n",
            "ity and produces unreliable answers. To transcend this, more ad-\n",
            "vanced RAG modules have been developed and integrated into the\n",
            "basic pipeline. For example, the Query Rewriter module acts as a\n",
            "bridge between the input question and the retrieval module. Instead\n",
            "of directly using the original question as the query text, it gener-\n",
            "ates a new query that better facilitates the retrieval of relevant infor-\n",
            "mation. This enhancement forms the Rewrite-Retrieve-Read pipeline\n",
            "[23, 22]. Furthermore, models like RETA-LLM [22] and RARR [10]\n",
            "integrate a post-reading and fact-checking component to further so-\n",
            "lidify the reliability of responses. Additional auxiliary modules such\n",
            "as the query router [21] and the resource ranker 1 [14] have also been\n",
            "proposed to be integrated into the RAG’s framework to improve the\n",
            "practicality in complex application scenario. This integration of var-\n",
            "ious modules into the RAG pipeline leading to the emergence of a\n",
            "modular RAG paradigm [11], transforming the RAG framework into\n",
            "a highly flexible system.\n",
            "Despite significant advancement\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2410.20299v1\" date=\"2024-10-27\" authors=\"Jiaxing Li, Chi Xu, Lianchen Jia, Feng Wang, Cong Zhang, Jiangchuan Liu\"/>\n",
            "<Title>\n",
            "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Large Language Models are revolutionizing Web, mobile, and Web of Things\n",
            "systems, driving intelligent and scalable solutions. However, as\n",
            "Retrieval-Augmented Generation (RAG) systems expand, they encounter significant\n",
            "challenges related to scalability, including increased delay and communication\n",
            "overhead. To address these issues, we propose EACO-RAG, an edge-assisted\n",
            "distributed RAG system that leverages adaptive knowledge updates and inter-node\n",
            "collaboration. By distributing vector datasets across edge nodes and optimizing\n",
            "retrieval processes, EACO-RAG significantly reduces delay and resource\n",
            "consumption while enhancing response accuracy. The system employs a multi-armed\n",
            "bandit framework with safe online Bayesian methods to balance performance and\n",
            "cost. Extensive experimental evaluation demonstrates that EACO-RAG outperforms\n",
            "traditional centralized RAG systems in both response time and resource\n",
            "efficiency. EACO-RAG effectively reduces delay and resource expenditure to\n",
            "levels comparable to, or even lower than, those of local RAG systems, while\n",
            "significantly improving accuracy. This study presents the first systematic\n",
            "exploration of edge-assisted distributed RAG architectures, providing a\n",
            "scalable and cost-effective solution for large-scale distributed environments.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "EACO-RAG: Edge-Assisted and Collaborative RAG with\n",
            "Adaptive Knowledge Update\n",
            "Jiaxing Li\n",
            "Simon Fraser University\n",
            "Burnaby, BC, Canada\n",
            "jla641@sfu.ca\n",
            "Chi Xu\n",
            "Simon Fraser University\n",
            "Burnaby, BC, Canada\n",
            "chix@sfu.ca\n",
            "Lianchen Jia\n",
            "Tsinghua University\n",
            "Beijing, China\n",
            "jlc21@mails.tsinghua.edu.cn\n",
            "Feng Wang\n",
            "University of Mississippi\n",
            "University, MS, USA\n",
            "fwang@cs.olemiss.edu\n",
            "Cong Zhang\n",
            "Jiangxing Intelligence Inc.\n",
            "Shenzhen, China\n",
            "vcongzc@gmail.com\n",
            "Jiangchuan Liu\n",
            "Simon Fraser University\n",
            "Burnaby, BC, Canada\n",
            "jcliu@sfu.ca\n",
            "ABSTRACT\n",
            "Large Language Models are revolutionizing Web, mobile, and Web\n",
            "of Things systems, driving intelligent and scalable solutions. How-\n",
            "ever, as Retrieval-Augmented Generation (RAG) systems expand,\n",
            "they encounter significant challenges related to scalability, includ-\n",
            "ing increased delay and communication overhead. To address these\n",
            "issues, we propose EACO-RAG, an edge-assisted distributed RAG\n",
            "system that leverages adaptive knowledge updates and inter-node\n",
            "collaboration. By distributing vector datasets across edge nodes and\n",
            "optimizing retrieval processes, EACO-RAG significantly reduces\n",
            "delay and resource consumption while enhancing response accu-\n",
            "racy. The system employs a multi-armed bandit framework with\n",
            "safe online Bayesian methods to balance performance and cost.\n",
            "Extensive experimental evaluation demonstrates that EACO-RAG\n",
            "outperforms traditional centralized RAG systems in both response\n",
            "time and resource efficiency. EACO-RAG effectively reduces delay\n",
            "and resource expenditure to levels comparable to, or even lower\n",
            "than, those of local RAG systems, while significantly improving\n",
            "accuracy. This study presents the first systematic exploration of\n",
            "edge-assisted distributed RAG architectures, providing a scalable\n",
            "and cost-effective solution for large-scale distributed environments.\n",
            "1\n",
            "INTRODUCTION\n",
            "Large Language Models (LLMs) are reshaping Web, mobile, and Web\n",
            "of Things (WoT) systems by enabling more intelligent, adaptive,\n",
            "and scalable solutions. Their integration enhances user experience\n",
            "with accurate, real-time, and context-aware responses, improving\n",
            "system efficiency and unlocking new capabilities for autonomous\n",
            "processing [1], recommendations [2, 3], and decision-making [4, 5].\n",
            "Retrieval-Augmented Generation (RAG) extends LLMs’ capa-\n",
            "bilities by integrating retrieval mechanisms that provide relevant\n",
            "context from a knowledge base, further improving the accuracy of\n",
            "generated responses. Its expanding presence in healthcare, educa-\n",
            "tion, and legal services is driving rapid adoption, with the market\n",
            "projected to grow at a compound annual growth rate of 44.7% be-\n",
            "tween 2024 and 2030 [6].\n",
            "As RAG services continue to expand rapidly, scalability chal-\n",
            "lenges can result in Quality of Service (QoS) degradation, with typ-\n",
            "ical concerns including reduced answer quality [7] and increased\n",
            "response delay [8, 9]. Commonly, RAG systems convert text into\n",
            "vector representations stored in databases and are hosted alongside\n",
            "language models in centralized data centers. However, as service\n",
            "Figure 1: Overview of Edge-assisted and Collaborative RAG\n",
            "(EACO-RAG), together with a comparison to other solutions,\n",
            "showing its improvements in answer quality, response time,\n",
            "and cost efficiency.\n",
            "demand grows, these centralized architectures encounter problems\n",
            "such as increased delay, communication overhead with end users,\n",
            "and inefficient global database searches. A notable trend is the\n",
            "deployment of LLMs at the edge [10, 11], which shows promis-\n",
            "ing potential to reduce generation delay. In light of this, we ar-\n",
            "gue that shifting RAG services toward edge-assisted distributed\n",
            "architectures could be a highly effective approach to address these\n",
            "scalability and performance challenges.\n",
            "More specifically, edge integration could help by strategically\n",
            "planning retrieval and generation based on end-user behavioral\n",
            "patterns in proximity, reducing delay, and improving performance.\n",
            "Edge nodes can analyze behaviors locally to preemptively optimize\n",
            "retrievals. In a cascaded fashi\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Prof. Hinton. Modular RAG, or Modular Retrieval-Augmented Generation, is an advanced approach that builds upon the foundational principles of Naive RAG. Naive RAG combines information retrieval with natural language generation to produce responses to queries. It typically uses retrieval models to rank indexed data based on relevance to the input query, generating text that aims to be coherent and contextually relevant [1].\n",
            "\n",
            "However, Naive RAG has certain limitations, such as inflexibility and inefficiencies when handling diverse and dynamic datasets [3]. This is where Modular RAG comes into play. Modular RAG enhances the traditional approach by incorporating dynamic and flexible components that allow for better adaptation to varying datasets and contexts. For instance, Modular RAG can fine-tune embeddings to capture task-specific semantics or domain knowledge, thus improving the quality of both the retrieved information and the generated responses [1].\n",
            "\n",
            "One of the key benefits of using Modular RAG at the production level is its enhanced efficiency and scalability. By using dynamic embedding techniques, Modular RAG can adaptively adjust embeddings during inference based on the context of the query or retrieved information. This adaptability makes it more suitable for applications that require current or specialized knowledge, as it can deliver more accurate and context-aware outputs [2].\n",
            "\n",
            "Overall, the modularity and flexibility of Modular RAG make it a more robust and scalable solution for production-level applications compared to traditional Naive RAG.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for breaking that down. I'm curious about the adaptability you mentioned. Can you provide a specific example of how Modular RAG's adaptability can enhance an AI system's performance in a real-world application?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, let's explore the differences between Modular RAG and traditional Naive RAG, along with the benefits of using Modular RAG in production environments.\n",
            "\n",
            "Traditional Naive RAG combines document retrieval with language model generation to produce context-aware responses. It typically involves a basic framework where retrieval models rank indexed data based on relevance to an input query, thereby aiding the generation of text that is coherent and contextually relevant [1]. However, Naive RAG lacks advanced optimization features, making it less efficient in handling complex queries or domain-specific tasks.\n",
            "\n",
            "On the other hand, Modular RAG introduces a more flexible and scalable approach. It allows customization of components such as query generation, retrieval, and memory management to suit specific tasks. This modularity enables the integration of advanced techniques like multi-query modules and pre-retrieval optimization strategies, such as step-back prompting and Hypothetical Embeddings (HyDE), which enhance query accuracy and result quality [3]. \n",
            "\n",
            "The scalability benefits of Modular RAG at the production level are significant. By customizing and optimizing each component, it reduces computational overhead and improves performance efficiency. This approach is particularly beneficial for applications that require current or specialized knowledge, as it allows for precise similarity searches and more accurate, context-aware outputs [2]. The ability to fine-tune embeddings for task-specific semantics or domain knowledge further enhances the quality of information retrieval and generated responses, making Modular RAG a more robust choice for scalable AI applications.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for that detailed explanation, Dr. Thompson. You've mentioned the computational overhead and performance efficiency improvements with Modular RAG. Can you provide a specific example of a production scenario where Modular RAG has significantly outperformed Naive RAG?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Nguyen. Modular RAG and traditional Naive RAG differ significantly in their frameworks and capabilities:\n",
            "\n",
            "1. **Framework Structure**:\n",
            "   - **Naive RAG**: This approach combines document retrieval with language model generation in a straightforward manner. It relies heavily on the similarity of data chunks to retrieve information, which can lead to performance issues with complex queries or data that has substantial variability [1].\n",
            "   - **Modular RAG**: In contrast, Modular RAG decomposes complex RAG systems into independent modules and specialized operators, offering a highly reconfigurable framework. This modular approach goes beyond the linear \"retrieve-then-generate\" process of Naive RAG by incorporating routing, scheduling, and fusion mechanisms, allowing for a more sophisticated and adaptable system [2].\n",
            "\n",
            "2. **Handling of Information**:\n",
            "   - **Naive RAG**: It tends to have a shallow understanding of queries, often resulting in retrieval redundancy and noise. This can lead to the generation of erroneous or hallucinated responses due to the direct use of all retrieved chunks [1].\n",
            "   - **Modular RAG**: By using a modular design, it allows for more precise information processing and reduces noise. This results in more accurate and context-aware outputs, crucial for tasks requiring current or specialized knowledge [2].\n",
            "\n",
            "3. **Reconfigurability and Adaptability**:\n",
            "   - **Naive RAG**: Generally lacks flexibility and struggles with the integration of advanced retrieval and generation methods [3].\n",
            "   - **Modular RAG**: Its LEGO-like reconfigurable framework enables easy adaptation to different application scenarios, making it suitable for production-level deployment where robustness and reliability are critical [2].\n",
            "\n",
            "In terms of benefits at the production level, Modular RAG offers enhanced reliability and robustness compared to Naive RAG. Its ability to adapt to specific application needs and reduce noise in retrieved information makes it particularly valuable in environments where accuracy and contextual relevance are paramount.\n",
            "\n",
            "Sources:\n",
            "[1] livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\n",
            "[2] arxiv.org/abs/2407.21059v1\n",
            "[3] zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\"/>\n",
            "These sub-modules allow for more granular control over the RAG process, enabling the system to fine-tune its operations based on the specific requirements of the task​. This pattern closely resembles the traditional RAG process but benefits from the modular approach by allowing individual modules to be optimized or replaced without altering the overall flow. For example, a linear RAG flow might begin with a query expansion module to refine the user’s input, followed by the retrieval module, which fetches the most relevant data chunks. The Modular RAG framework embraces this need through various tuning patterns that enhance the system’s ability to adapt to specific tasks and datasets. Managing these different data types and ensuring seamless integration within the RAG system can be difficult, requiring sophisticated data processing and retrieval strategies​(modular rag paper).\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.aporia.com/learn/introduction-to-rags-examples-from-the-real-world/\"/>\n",
            "Ensure reliable, on-target Gen-AI responses\n",
            "Protect intellectual property and ensure compliance\n",
            "Safely navigate GenAI: Detect and avoid off-topic conversations\n",
            "Keep interactions tasteful, filter NSFW content\n",
            "Secure company data: Detect and anonymize sensitive info\n",
            "Shield data from smart LLM SQL queries\n",
            "Detect and filter out malicious input for prompt integrity\n",
            "Safeguard LLM: Keep model instructions confidential\n",
            "Explore LLM interactions for user engagement insights\n",
            "Track costs, queries, and tokens for budget control\n",
            "Tailored production ML dashboards to monitor key metrics\n",
            "Real-time ML monitoring to detect drifts and monitor predictions\n",
            "Direct Data Connectors: Monitor and observe billions of predictions\n",
            "Root Cause Analysis to gain actionable insights and explore model predictions\n",
            "LLM Observability for your ML: Monitor, troubleshoot and enhance efficiency\n",
            "Explainable AI to understand, ensure trust, and communicate predictions\n",
            "Tailored Aporia Observe for your models: Integrate any model in minutes\n",
            "Integrate Aporia to every LLM and tool in the market\n",
            "Empower tabular models with Aporia\n",
            "Streamline AI Act compliance with Aporia Guardrails and Observe\n",
            "Unlock potential in CV & NLP models\n",
            "A team of Cybersecurity, Compliance, and AI Experts that ensures Aporia users top-tier protection\n",
            "Optimize LLM & GenAI apps for peak performance\n",
            "Your go-to resource for Aporia insights and guides\n",
            "Integrate Aporia to your LLM as a Proxy with Guardrail Policies\n",
            "Integrate Aporia with Your Firewall for AI Tool Security\n",
            "Easily Integrate and Monitor ML Models in Production\n",
            "Define ML Observability Resources as Code with SDK\n",
            "Learn about AI control from our experts\n",
            "Your dictionary for AI terminology.\n",
            " Step-by-step guides to master AI\n",
            "Dive into our GitHub projects and examples\n",
            "Unlock AI secrets with our eBooks\n",
            "Elevate your GenAI and LLM knwoledge\n",
            "Metrics, feature importance and more\n",
            "Introduction >\n",
            "Introduction to RAGs: Real-world applications and examples\n",
            "Alon is the CTO of Aporia.\n",
            " As we explore language models, the knowledge retrieval and creative generation embodied in RAG promises a future where machines comprehend and adeptly contribute to human-like conversations, setting the stage for a new era of sophisticated and context-aware artificial intelligence.\n",
            " In the world of natural language processing (NLP) and large language models (LLMs), Retrieval-Augmented Generation (RAG) stands as a transformative approach, seamlessly blending the strengths of retrieval and generation models.\n",
            " This specialized application ensures the generation of accurate and relevant code snippets and summaries by leveraging both retrieval and generation processes, catering to the specific requirements of developers and programmers.\n",
            "\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.restack.io/p/multi-agent-systems-answer-modular-rag-cat-ai\"/>\n",
            "Explore the role of modular rag in enhancing the efficiency of multi-agent systems through structured interactions. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities: By utilizing the Modular RAG framework, developers can create more robust and adaptable systems that meet the evolving demands of data retrieval and processing. Multimodal Retrieval-Augmented Generation (MM-RAG) represents a significant advancement in the capabilities of language models by integrating both text and image retrieval into the generation process. This approach enhances the traditional Retrieval-Augmented Generation (RAG) by allowing models to utilize a broader range of data types, thereby improving the quality and relevance of generated outputs. The Modular RAG framework is designed to allow for module substitution or reconfiguration, addressing specific challenges in retrieval and processing.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for that detailed explanation. Could you provide a specific example of a real-world application where Modular RAG has demonstrated a clear advantage over Naive RAG, particularly in terms of robustness and reliability?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://drjulija.github.io/posts/rag/\"/>\n",
            "3️⃣ Modular RAG# Modular RAG integrates various modules and techniques from Adanced RAG to improve the overall RAG system. For example, incorporating a search module for similarity retrieval and applying a fine-tuning approach in the retriever. Modular RAG became a standard paradigm when building RAG applications. A few example of modules:\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@abhilashkrish/understanding-rag-architectures-naive-advanced-multi-modal-and-graph-rag-for-real-world-1c98da879f76\"/>\n",
            "Understanding RAG Architectures: Naive, Advanced, Multi-modal, and Graph RAG for Real-World Applications | by Abhilash Krishnan | Nov, 2024 | Medium These additions improve retrieval accuracy and relevance, making Advanced RAG suitable for use cases that require nuanced understanding of query intent and better context matching. Cross-modal Understanding: By aligning embeddings from different modalities, Multi-modal RAG can understand and retrieve information even when the query and the data are in different formats (e.g., text query to retrieve image-based results). Multi-modal RAG opens possibilities for cross-modal understanding, and Graph RAG enables retrieval from complex, relational data structures, making it highly valuable in fields requiring structured knowledge and reasoning.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
            "3️⃣ Modular RAG. Modular RAG integrates various modules and techniques from Adanced RAG to improve the overall RAG system. For example, incorporating a search module for similarity retrieval\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Patel. Modular RAG significantly differs from traditional Naive RAG in its approach to efficiency and resource management, particularly at the production level.\n",
            "\n",
            "1. **Modular Architecture**: Modular RAG is characterized by its flexible and compartmentalized design, which allows for the integration of various specialized modules. This modularity means that different components, such as query rewriting, knowledge filtering, and memory management, can be optimized independently. This contrasts with Naive RAG, which typically follows a more linear 'retrieve-then-read' pipeline without such flexibility [2].\n",
            "\n",
            "2. **Enhanced Retrieval Efficiency**: In Modular RAG, components like the Query Rewriter+ and Knowledge Filter are used to enhance retrieval efficiency. The Query Rewriter+ generates multiple queries to overcome information plateaus and eliminates ambiguities, while the Knowledge Filter removes irrelevant information. These enhancements lead to more precise and efficient retrieval processes compared to the more straightforward retrieval methods in Naive RAG [2].\n",
            "\n",
            "3. **Resource Optimization**: Modular RAG systems include modules like the Memory Knowledge Reservoir and the Retriever Trigger, which optimize resource usage by dynamically expanding the knowledge base and managing access to external knowledge sources. This reduces redundant retrievals and improves the overall efficiency of the system, which is a notable improvement over the resource management in Naive RAG [2].\n",
            "\n",
            "4. **Scalability and Cost-Effectiveness**: Modular RAG frameworks, such as EACO-RAG, leverage edge-assisted architectures to distribute workloads and optimize retrieval processes. This results in reduced delays and lower resource consumption, making them more scalable and cost-effective compared to traditional centralized systems used in Naive RAG [3].\n",
            "\n",
            "These differences in architecture and process mean that Modular RAG systems are better suited for production environments where efficiency, scalability, and cost-effectiveness are critical. They offer improved accuracy and context-aware outputs, essential for applications requiring current or specialized knowledge [4].\n",
            "\n",
            "Sources:\n",
            "[1] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[2] arxiv.org/abs/2407.10670v1\n",
            "[3] arxiv.org/abs/2410.20299v1\n",
            "[4] zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Thank you for that detailed explanation, Dr. Patel. Could you provide any specific examples or case studies where Modular RAG has been implemented successfully, demonstrating its advantages over Naive RAG in a real-world application?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2407.19994v3\" date=\"2024-09-13\" authors=\"Cheonsu Jeong\"/>\n",
            "<Title>\n",
            "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "This study aims to improve knowledge-based question-answering (QA) systems by\n",
            "overcoming the limitations of existing Retrieval-Augmented Generation (RAG)\n",
            "models and implementing an advanced RAG system based on Graph technology to\n",
            "develop high-quality generative AI services. While existing RAG models\n",
            "demonstrate high accuracy and fluency by utilizing retrieved information, they\n",
            "may suffer from accuracy degradation as they generate responses using\n",
            "pre-loaded knowledge without reprocessing. Additionally, they cannot\n",
            "incorporate real-time data after the RAG configuration stage, leading to issues\n",
            "with contextual understanding and biased information. To address these\n",
            "limitations, this study implemented an enhanced RAG system utilizing Graph\n",
            "technology. This system is designed to efficiently search and utilize\n",
            "information. Specifically, it employs LangGraph to evaluate the reliability of\n",
            "retrieved information and synthesizes diverse data to generate more accurate\n",
            "and enhanced responses. Furthermore, the study provides a detailed explanation\n",
            "of the system's operation, key implementation steps, and examples through\n",
            "implementation code and validation results, thereby enhancing the understanding\n",
            "of advanced RAG technology. This approach offers practical guidelines for\n",
            "implementing advanced RAG systems in corporate services, making it a valuable\n",
            "resource for practical application.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            " \n",
            "* Corresponding Author: Cheonsu Jeong; paripal@korea.ac.kr \n",
            " \n",
            " \n",
            "1  \n",
            " \n",
            "A Study on the Implementation Method \n",
            "of an Agent-Based Advanced RAG  \n",
            "System Using Graph \n",
            "Cheonsu Jeong1 \n",
            " \n",
            " \n",
            " \n",
            "1 Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS; \n",
            " \n",
            "Abstract  \n",
            "This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of \n",
            "existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on \n",
            "Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high ac-\n",
            "curacy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate \n",
            "responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data \n",
            "after the RAG configuration stage, leading to issues with contextual understanding and biased information. \n",
            "To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system \n",
            "is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability \n",
            "of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, \n",
            "the study provides a detailed explanation of the system's operation, key implementation steps, and examples through \n",
            "implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This \n",
            "approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valu-\n",
            "able resource for practical application. \n",
            " \n",
            "Keywords \n",
            "Advance RAG; Agent RAG; LLM; Generative AI; LangGraph \n",
            " \n",
            " \n",
            "I. Introduction \n",
            "Recent advancements in AI technology have brought sig-\n",
            "nificant attention to Generative AI. Generative AI, a form \n",
            "of artificial intelligence that can create new content such \n",
            "as text, images, audio, and video based on vast amounts \n",
            "of trained data models (Jeong, 2023d), is being applied in \n",
            "various fields, including daily conversations, finance, \n",
            "healthcare, education, and entertainment (Ahn & Park, \n",
            "2023). As generative AI services become more accessible \n",
            "to the general public, the role of generative AI-based chat-\n",
            "bots is becoming increasingly important (Adam et al., \n",
            "2021; Przegalinska et al., 2019; Park, 2024). A chatbot is \n",
            "an intelligent agent that allows users to have conversa-\n",
            "tions typically through text or voice (Sánchez-Díaz et al., \n",
            "2018; Jeong & Jeong, 2020). Recently, generative AI \n",
            "chatbots have advanced to the level of analyzing human \n",
            "emotions and intentions to provide responses (Jeong, \n",
            "2023a). With the advent of large language models \n",
            "(LLMs), these chatbots can now be utilized for automatic \n",
            " \n",
            "Cheonsu Jeong \n",
            " \n",
            "2  \n",
            " \n",
            "dialogue generation and translation (Jeong, 2023b). How-\n",
            "ever, they may generate responses that conflict with the \n",
            "latest information and have a low understanding of new \n",
            "problems or domains as they rely on previously trained \n",
            "data (Jeong, 2023c). While 2023 was marked by the re-\n",
            "lease of foundational large language models (LLMs) like \n",
            "ChatGPT and Llama-2, experts predict that 2024 will be \n",
            "the year of Retrieval Augmented Generation (RAG) and \n",
            "AI Agents (Skelter Labs, 2024). \n",
            "However, there are several considerations for companies \n",
            "looking to adopt generative AI services. Companies must \n",
            "address concerns such as whether the AI can provide ac-\n",
            "curate responses based on internal data, the potential risk \n",
            "of internal data leakage, and how to integrate generative \n",
            "AI with corporate systems. Solutions include using do-\n",
            "main-specific fine-tuned LLMs and enhancing reliability \n",
            "with RAG that utilizes internal information (Jung, 2024). \n",
            "When domain-specific information is fine-tuned on GPT-\n",
            "4 LLM, accuracy improves from 75% to 81%, and adding \n",
            "RAG can further increase accuracy to 86% (Angels et al., \n",
            "2024). RAG models are known for effectively combinin\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2412.14751v1\" date=\"2024-12-19\" authors=\"Maolin He, Rena Gao, Mike Conway, Brian E. Chapman\"/>\n",
            "<Title>\n",
            "Query pipeline optimization for cancer patient question answering systems\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented generation (RAG) mitigates hallucination in Large\n",
            "Language Models (LLMs) by using query pipelines to retrieve relevant external\n",
            "information and grounding responses in retrieved knowledge. However, query\n",
            "pipeline optimization for cancer patient question-answering (CPQA) systems\n",
            "requires separately optimizing multiple components with domain-specific\n",
            "considerations. We propose a novel three-aspect optimization approach for the\n",
            "RAG query pipeline in CPQA systems, utilizing public biomedical databases like\n",
            "PubMed and PubMed Central. Our optimization includes: (1) document retrieval,\n",
            "utilizing a comparative analysis of NCBI resources and introducing Hybrid\n",
            "Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,\n",
            "identifying optimal pairings of dense retrievers and rerankers; and (3)\n",
            "semantic representation, introducing Semantic Enhanced Overlap Segmentation\n",
            "(SEOS) for improved contextual understanding. On a custom-developed dataset\n",
            "tailored for cancer-related inquiries, our optimized RAG approach improved the\n",
            "answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and\n",
            "about 3% over a naive RAG setup. This study highlights the importance of\n",
            "domain-specific query optimization in realizing the full potential of RAG and\n",
            "provides a robust framework for building more accurate and reliable CPQA\n",
            "systems, advancing the development of RAG-based biomedical systems.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "IEEE TRANSACTIONS AND JOURNALS TEMPLATE\n",
            "1\n",
            "Query pipeline optimization for cancer patient\n",
            "question answering systems\n",
            "Maolin He, Rena Gao, Mike Conway, and Brian E. Chapman\n",
            "Abstract— Retrieval-augmented generation (RAG) miti-\n",
            "gates hallucination in Large Language Models (LLMs) by\n",
            "using query pipelines to retrieve relevant external infor-\n",
            "mation and grounding responses in retrieved knowledge.\n",
            "However, query pipeline optimization for cancer patient\n",
            "question-answering (CPQA) systems requires separately\n",
            "optimizing multiple components with domain-specific con-\n",
            "siderations. We propose a novel three-aspect optimization\n",
            "approach for the RAG query pipeline in CPQA systems,\n",
            "utilizing public biomedical databases like PubMed and\n",
            "PubMed Central. Our optimization includes: (1) document\n",
            "retrieval, utilizing a comparative analysis of NCBI resources\n",
            "and introducing Hybrid Semantic Real-time Document Re-\n",
            "trieval (HSRDR); (2) passage retrieval, identifying optimal\n",
            "pairings of dense retrievers and rerankers; and (3) semantic\n",
            "representation, introducing Semantic Enhanced Overlap\n",
            "Segmentation (SEOS) for improved contextual understand-\n",
            "ing. On a custom-developed dataset tailored for cancer-\n",
            "related inquiries, our optimized RAG approach improved\n",
            "the answer accuracy of Claude-3-haiku by 5.24% over\n",
            "chain-of-thought prompting and about 3% over a naive RAG\n",
            "setup. This study highlights the importance of domain-\n",
            "specific query optimization in realizing the full potential of\n",
            "RAG and provides a robust framework for building more\n",
            "accurate and reliable CPQA systems, advancing the devel-\n",
            "opment of RAG-based biomedical systems.\n",
            "Index Terms— Biomedical computing, Oncology, Large\n",
            "language models\n",
            "I. INTRODUCTION\n",
            "Question-answering (QA) tasks are crucial in the biomedical\n",
            "domain, where timely and accurate responses can impact\n",
            "human lives. With more than a million new citations added\n",
            "to PubMed annually [1], healthcare professionals and patients\n",
            "face an overwhelming influx of information, highlighting the\n",
            "need to quickly process, analyze and summarize the vast\n",
            "biomedical literature. Large Language Models (LLMs) have\n",
            "revolutionized QA tasks in diverse domains [2]. Unlike tradi-\n",
            "tional search engines that rely on keyword matching, LLMs\n",
            "Maolin He is with the School of Computing and Information Sys-\n",
            "tems, University of Melbourne, Parkville, VIC 3052 Australia (e-mail:\n",
            "maolinh@student.unimelb.edu.au)\n",
            "Rena Gao is with the School of Computing and Information Sys-\n",
            "tems, University of Melbourne, Parkville, VIC 3052, Australia (e-mail:\n",
            "wegao@student.unimelb.edu.au).\n",
            "Mike Conway is with the School of Computing and Information Sys-\n",
            "tems, University of Melbourne, Parkville, VIC 3052 Australia (e-mail:\n",
            "mike.conway@unimelb.edu.au).\n",
            "Brian E. Chapman is with the School of Computing and Information\n",
            "Systems, University of Melbourne, Parville, VIC 3052, Australia (e-mail:\n",
            "chapmanbe@gmail.com).\n",
            "leverage transformer architectures to capture semantic relation-\n",
            "ships and nuances, enabling them to find semantically relevant\n",
            "information and process it into precise, coherent answers.\n",
            "Thus, LLM-based QA systems reduce users’ need to syn-\n",
            "thesize the data manually. However, LLMs face a significant\n",
            "challenge: hallucination—producing fluent but unfaithful or\n",
            "nonsensical responses [3]. Further, LLMs rely on pre-trained\n",
            "data, which may lack domain-specific or real-time knowledge\n",
            "[2]. These issues are particularly acute in healthcare [4], where\n",
            "inaccuracy can have severe consequences [5], requiring the QA\n",
            "system to demonstrate accuracy, reliability, and currency.\n",
            "Retrieval-Augmented Generation (RAG) [6] is a solution\n",
            "to these challenges by guiding LLMs in generating accurate\n",
            "responses by retrieving relevant external information, rather\n",
            "than relying solely on the model’s neural weights. This\n",
            "approach can enhance performance in knowledge-intensive\n",
            "tasks [7] and open-domain QA. Especially in medical QA\n",
            "systems where questions are knowledge-intensive, LLMs excel\n",
            "as generators rather than knowledge databases\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
            "Learn about the key differences between Modular and Naive RAG, case study, and the significant advantages of Modular RAG. Modular RAG enhances flexibility, scalability, and accuracy compared to Naive RAG. ... Modular Retrieval-Augmented Generation (RAG) represents an evolution in the design and implementation of RAG systems. By adopting a\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
            "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
            "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2501.00353v1\" date=\"2024-12-31\" authors=\"Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, Benyou Wang\"/>\n",
            "<Title>\n",
            "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for\n",
            "enhancing large language models (LLMs) by incorporating external knowledge.\n",
            "However, current RAG methods face two limitations: (1) they only cover limited\n",
            "RAG scenarios. (2) They suffer from limited task diversity due to the lack of a\n",
            "general RAG dataset. To address these limitations, we propose RAG-Instruct, a\n",
            "general method for synthesizing diverse and high-quality RAG instruction data\n",
            "based on any source corpus. Our approach leverages (1) five RAG paradigms,\n",
            "which encompass diverse query-document relationships, and (2) instruction\n",
            "simulation, which enhances instruction diversity and quality by utilizing the\n",
            "strengths of existing instruction datasets. Using this method, we construct a\n",
            "40K instruction dataset from Wikipedia, comprehensively covering diverse RAG\n",
            "scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively\n",
            "enhances LLMs' RAG capabilities, achieving strong zero-shot performance and\n",
            "significantly outperforming various RAG baselines across a diverse set of\n",
            "tasks. RAG-Instruct is publicly available at\n",
            "https://github.com/FreedomIntelligence/RAG-Instruct.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n",
            "Instructions\n",
            "Wanlong Liu2†, Junying Chen1†, Ke Ji1, Li Zhou1, Wenyu Chen2, Benyou Wang1*\n",
            "1 The Chinese University of Hong Kong, Shenzhen,\n",
            "2 University of Electronic Science and Technology of China\n",
            "wangbenyou@cuhk.edu.cn\n",
            "Abstract\n",
            "Retrieval-Augmented Generation (RAG) has\n",
            "emerged as a key paradigm for enhancing large\n",
            "language models (LLMs) by incorporating\n",
            "external knowledge. However, current RAG\n",
            "methods face two limitations: (1) they only\n",
            "cover limited RAG scenarios. (2) They suffer\n",
            "from limited task diversity due to the lack\n",
            "of a general RAG dataset. To address these\n",
            "limitations, we propose RAG-Instruct, a\n",
            "general method for synthesizing diverse and\n",
            "high-quality RAG instruction data based on\n",
            "any source corpus. Our approach leverages\n",
            "(1) five RAG paradigms, which encompass\n",
            "diverse query-document relationships, and\n",
            "(2) instruction simulation, which enhances\n",
            "instruction diversity and quality by utilizing\n",
            "the strengths of existing instruction datasets.\n",
            "Using this method, we construct a 40K\n",
            "instruction dataset from Wikipedia, compre-\n",
            "hensively covering diverse RAG scenarios\n",
            "and tasks.\n",
            "Experiments demonstrate that\n",
            "RAG-Instruct effectively enhances LLMs’\n",
            "RAG capabilities, achieving strong zero-shot\n",
            "performance and significantly outperforming\n",
            "various RAG baselines across a diverse set of\n",
            "tasks. RAG-Instruct is publicly available at\n",
            "https://github.com/FreedomIntelligence/RAG-\n",
            "Instruct.\n",
            "1\n",
            "Introduction\n",
            "Retrieval-Augmented Generation (RAG) (Guu\n",
            "et al., 2020; Asai et al., 2024b) enhances large\n",
            "language models (LLMs) by integrating exter-\n",
            "nal knowledge through document retrieval, effec-\n",
            "tively reducing hallucinations and improving per-\n",
            "formance across diverse tasks (Asai et al., 2023;\n",
            "Jin et al., 2024; Lu et al., 2022; Liu et al., 2024a).\n",
            "Since retrievers are not perfect, and consider-\n",
            "able research has shown that noisy retrieval can\n",
            "adversely impact LLM performance (Petroni et al.,\n",
            "*Corresponding author. †Equal Contribution.\n",
            "2020; Shi et al., 2023; Maekawa et al., 2024), nu-\n",
            "merous studies have focused on enhancing the ro-\n",
            "bustness of RAG in handling noisy retrieval con-\n",
            "texts (Wei et al., 2024; Chan et al., 2024). On the\n",
            "one hand, some studies involve adaptive retrieval\n",
            "based on query analysis (Asai et al., 2024a; Jeong\n",
            "et al., 2024), or query reformulation (Chan et al.,\n",
            "2024; Ma et al., 2023) to enhance the robustness\n",
            "of LLM-based RAG systems. On the other hand,\n",
            "(Zhang et al., 2024; Liu et al., 2024b; Yoran et al.,\n",
            "2024) enhance the robustness of models’ naive\n",
            "RAG capabilities by training them to adapt to irrel-\n",
            "evant and noisy documents.\n",
            "However, existing RAG methods have two limi-\n",
            "tations: (1) Limited RAG scenarios. Real-world\n",
            "RAG scenarios are complex: Given the query, the\n",
            "retrieved information may directly contain the an-\n",
            "swer, offer partial help, or be helpless. Some an-\n",
            "swers can be obtained from a single document,\n",
            "while others require multi-hop reasoning across\n",
            "multiple documents. Our preliminary study demon-\n",
            "strates existing RAG methods cannot adequately\n",
            "handle all such scenarios (Chan et al., 2024; Asai\n",
            "et al., 2024a; Liu et al., 2024b).\n",
            "(2) Limited\n",
            "task diversity. Due to the lack of a general RAG\n",
            "dataset, most current RAG methods (Wei et al.,\n",
            "2024; Zhang et al., 2024) are fine-tuned on task-\n",
            "specific datasets (e.g., NQ (Kwiatkowski et al.,\n",
            "2019), TrivialQA (Joshi et al., 2017)), which suffer\n",
            "from limited question diversity and data volume.\n",
            "To address these limitations, we propose RAG-\n",
            "Instruct, a general method for synthesizing diverse\n",
            "and high-quality RAG instruction data based on any\n",
            "source corpus. Using this method, we construct a\n",
            "40K synthetic instruction dataset from Wikipedia\n",
            "tailored for RAG. Our method emphasizes the di-\n",
            "versity in two aspects:\n",
            "1. Defining diverse RAG paradigms: we define\n",
            "five RAG query paradigms that encompass\n",
            "various query-document relationships to adapt\n",
            "arXiv:2501.00353v1  [cs.CL]  31 Dec 2024\n",
            "to different RAG scenarios, considering both\n",
            "doc\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2404.01037v1\" date=\"2024-04-01\" authors=\"Matouš Eibich, Shivay Nagpal, Alexander Fred-Ojala\"/>\n",
            "<Title>\n",
            "ARAGOG: Advanced RAG Output Grading\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) is essential for integrating external\n",
            "knowledge into Large Language Model (LLM) outputs. While the literature on RAG\n",
            "is growing, it primarily focuses on systematic reviews and comparisons of new\n",
            "state-of-the-art (SoTA) techniques against their predecessors, with a gap in\n",
            "extensive experimental comparisons. This study begins to address this gap by\n",
            "assessing various RAG methods' impacts on retrieval precision and answer\n",
            "similarity. We found that Hypothetical Document Embedding (HyDE) and LLM\n",
            "reranking significantly enhance retrieval precision. However, Maximal Marginal\n",
            "Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a\n",
            "baseline Naive RAG system, and Multi-query approaches underperformed. Sentence\n",
            "Window Retrieval emerged as the most effective for retrieval precision, despite\n",
            "its variable performance on answer similarity. The study confirms the potential\n",
            "of the Document Summary Index as a competent retrieval approach. All resources\n",
            "related to this research are publicly accessible for further investigation\n",
            "through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We\n",
            "welcome the community to further this exploratory study in RAG systems.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "ARAGOG: Advanced RAG Output Grading\n",
            "Matouˇs Eibich\n",
            "Predli\n",
            "matous@predli.com\n",
            "Shivay Nagpal\n",
            "Predli\n",
            "shivay@predli.com\n",
            "Alexander Fred-Ojala\n",
            "Predli & UC Berkeley\n",
            "afo@berkeley.edu\n",
            "April 2, 2024\n",
            "Abstract\n",
            "Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into\n",
            "Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily\n",
            "focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against\n",
            "their predecessors, with a gap in extensive experimental comparisons. This study begins to address\n",
            "this gap by assessing various RAG methods’ impacts on retrieval precision and answer similarity.\n",
            "We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly en-\n",
            "hance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did\n",
            "not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches\n",
            "underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision,\n",
            "despite its variable performance on answer similarity. The study confirms the potential of the\n",
            "Document Summary Index as a competent retrieval approach. All resources related to this re-\n",
            "search are publicly accessible for further investigation through our GitHub repository ARAGOG.\n",
            "We welcome the community to further this exploratory study in RAG systems.\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) have significantly advanced the field of natural language processing,\n",
            "enabling a wide range of applications from text generation to question answering. However, inte-\n",
            "grating dynamic, external information remains a challenge for these models. Retrieval Augmented\n",
            "Generation (RAG) techniques address this limitation by incorporating external knowledge sources into\n",
            "the generation process, thus enhancing the models’ ability to produce contextually relevant and in-\n",
            "formed outputs. This integration of retrieval mechanisms with generative models is a key development\n",
            "in improving the performance and versatility of LLMs, facilitating more accurate and context-aware\n",
            "responses. See Figure 1 for an overview of the standard RAG workflow.\n",
            "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of\n",
            "literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between\n",
            "successive state-of-the-art (SoTA) models (Gao et al., 2022; Jiang et al., 2023). This pattern reveals\n",
            "a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG\n",
            "techniques is missing.\n",
            "Such a comparison is crucial for understanding the relative strengths and\n",
            "weaknesses of these techniques in enhancing LLMs’ performance across various tasks. This study seeks\n",
            "to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques\n",
            "and their combinations, thereby offering insights into their efficacy and applicability in real-world\n",
            "scenarios.\n",
            "The focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the\n",
            "retrieval process. These techniques can be categorized into several areas:\n",
            "1\n",
            "arXiv:2404.01037v1  [cs.CL]  1 Apr 2024\n",
            "RAG Technique\n",
            "Type\n",
            "Sentence-window retrieval\n",
            "Decoupling of Retrieval and Generation\n",
            "Document summary index\n",
            "HyDE\n",
            "Query Expansion\n",
            "Multi-query\n",
            "Maximal Marginal Relevance (MMR)\n",
            "Enhancement Mechanism\n",
            "Cohere Re-ranker\n",
            "Re-rankers\n",
            "LLM-based Re-ranker\n",
            "To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer\n",
            "Similarity (Tonic AI, 2023). Retrieval Precision measures the relevance of the retrieved context to the\n",
            "question asked, while Answer Similarity assesses how closely the system’s answers align with reference\n",
            "responses, on a scale from 0 to 5.\n",
            "Figure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system. This\n",
            "process diagram shows how a user query is processed by the system to retrieve relevant documents from a database\n",
            "and how\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2412.05838v1\" date=\"2024-12-08\" authors=\"Aniruddha Salve, Saba Attar, Mahesh Deshmukh, Sayali Shivpuje, Arnab Mitra Utsab\"/>\n",
            "<Title>\n",
            "A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\n",
            "incorporating external, domain-specific data into the generative process. While\n",
            "LLMs are highly capable, they often rely on static, pre-trained datasets,\n",
            "limiting their ability to integrate dynamic or private data. Traditional RAG\n",
            "systems typically use a single-agent architecture to handle query generation,\n",
            "data retrieval, and response synthesis. However, this approach becomes\n",
            "inefficient when dealing with diverse data sources, such as relational\n",
            "databases, document stores, and graph databases, often leading to performance\n",
            "bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system\n",
            "to address these limitations. Specialized agents, each optimized for a specific\n",
            "data source, handle query generation for relational, NoSQL, and document-based\n",
            "systems. These agents collaborate within a modular framework, with query\n",
            "execution delegated to an environment designed for compatibility across various\n",
            "database types. This distributed approach enhances query efficiency, reduces\n",
            "token overhead, and improves response accuracy by ensuring that each agent\n",
            "focuses on its specialized task. The proposed system is scalable and adaptable,\n",
            "making it ideal for generative AI workflows that require integration with\n",
            "diverse, dynamic, or private data sources. By leveraging specialized agents and\n",
            "a modular execution environment, the system provides an efficient and robust\n",
            "solution for handling complex, heterogeneous data environments in generative AI\n",
            "applications.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "A COLLABORATIVE MULTI-AGENT APPROACH TO\n",
            "RETRIEVAL-AUGMENTED GENERATION ACROSS DIVERSE DATA\n",
            "SOURCES\n",
            "Aniruddha Salve\n",
            "iASYS Technology Solutions Pvt. Ltd.\n",
            "Pune, Maharashtra, India\n",
            "aniruddha.salve@iasys.co.in\n",
            "Mahesh Deshmukh\n",
            "iASYS Technology Solutions Pvt. Ltd.\n",
            "Pune, Maharashtra, India\n",
            "mahesh.deshmukh@iasys.co.in\n",
            "Saba Attar\n",
            "SVPM’s College of Engineering\n",
            "Baramati, Pune, Maharashtra, India\n",
            "sabaattar1702@gmail.com\n",
            "Sayali Shivpuje\n",
            "SVPM’s College of Engineering\n",
            "Baramati, Pune, Maharashtra, India\n",
            "shivpujesayali.2243@gmail.com\n",
            "Arnab Mitra Utsab\n",
            "School of Data and Sciences\n",
            "Brac University\n",
            "Dhaka, Bangladesh\n",
            "arnab.mitra.utsab@g.bracu.ac.bd\n",
            "ABSTRACT\n",
            "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating\n",
            "external, domain-specific data into the generative process. While LLMs are highly capable, they\n",
            "often rely on static, pre-trained datasets, limiting their ability to integrate dynamic or private data.\n",
            "Traditional RAG systems typically use a single-agent architecture to handle query generation, data\n",
            "retrieval, and response synthesis. However, this approach becomes inefficient when dealing with\n",
            "diverse data sources, such as relational databases, document stores, and graph databases, often leading\n",
            "to performance bottlenecks and reduced accuracy.\n",
            "This paper proposes a Multi-Agent RAG system to address these limitations. Specialized agents, each\n",
            "optimized for a specific data source, handle query generation for relational, NoSQL, and document-\n",
            "based systems. These agents collaborate within a modular framework, with query execution delegated\n",
            "to an environment designed for compatibility across various database types. This distributed approach\n",
            "enhances query efficiency, reduces token overhead, and improves response accuracy by ensuring that\n",
            "each agent focuses on its specialized task.\n",
            "The proposed system is scalable and adaptable, making it ideal for generative AI workflows that\n",
            "require integration with diverse, dynamic, or private data sources. By leveraging specialized agents\n",
            "and a modular execution environment, the system provides an efficient and robust solution for\n",
            "handling complex, heterogeneous data environments in generative AI applications.\n",
            "Keywords Multi-Agent RAG Systems · Retrieval-Augmented Generation · Large Language Models · Database\n",
            "Integration · Generative AI\n",
            "arXiv:2412.05838v1  [cs.AI]  8 Dec 2024\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) have significantly advanced natural language processing by enabling sophisticated\n",
            "query interpretation and text generation. [1, 8] Despite their capabilities, LLMs are limited by their reliance on static\n",
            "pre-trained datasets, which restricts their ability to incorporate dynamic, domain-specific, or private data into their\n",
            "responses. Retrieval-Augmented Generation (RAG) systems address this challenge by integrating external data retrieval\n",
            "with generative processes, providing more context-aware and accurate outputs.\n",
            "Traditional RAG systems typically employ single-agent architectures where a single system is responsible for query\n",
            "generation, data retrieval, and response synthesis. While effective for basic use cases, these monolithic designs often\n",
            "face limitations when dealing with diverse data sources, such as relational databases, document stores, and graph-based\n",
            "data [19]. These systems also require elaborate prompts containing schemas, examples, and user queries, leading to\n",
            "inefficiencies in token usage, increased processing latency, and potential inaccuracies in query handling.\n",
            "To overcome these challenges, this paper proposes a Multi-Agent RAG system [22, 23]. Unlike traditional approaches,\n",
            "this system delegates the task of query generation to specialized agents, each tailored to a specific type of database.\n",
            "These agents generate optimized, database-specific queries without directly executing or retrieving data. Queries are\n",
            "executed in a separate execution environment, ensuring compatibility with diverse data storage systems. The retrieved\n",
            "context is then com\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2501.09136v1\" date=\"2025-01-15\" authors=\"Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei\"/>\n",
            "<Title>\n",
            "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\n",
            "by enabling human like text generation and natural language understanding.\n",
            "However, their reliance on static training data limits their ability to respond\n",
            "to dynamic, real time queries, resulting in outdated or inaccurate outputs.\n",
            "Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\n",
            "by integrating real time data retrieval to provide contextually relevant and\n",
            "up-to-date responses. Despite its promise, traditional RAG systems are\n",
            "constrained by static workflows and lack the adaptability required for\n",
            "multistep reasoning and complex task management.\n",
            "  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\n",
            "limitations by embedding autonomous AI agents into the RAG pipeline. These\n",
            "agents leverage agentic design patterns reflection, planning, tool use, and\n",
            "multiagent collaboration to dynamically manage retrieval strategies,\n",
            "iteratively refine contextual understanding, and adapt workflows to meet\n",
            "complex task requirements. This integration enables Agentic RAG systems to\n",
            "deliver unparalleled flexibility, scalability, and context awareness across\n",
            "diverse applications.\n",
            "  This survey provides a comprehensive exploration of Agentic RAG, beginning\n",
            "with its foundational principles and the evolution of RAG paradigms. It\n",
            "presents a detailed taxonomy of Agentic RAG architectures, highlights key\n",
            "applications in industries such as healthcare, finance, and education, and\n",
            "examines practical implementation strategies. Additionally, it addresses\n",
            "challenges in scaling these systems, ensuring ethical decision making, and\n",
            "optimizing performance for real-world applications, while providing detailed\n",
            "insights into frameworks and tools for implementing Agentic RAG\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\n",
            "AGENTIC RAG\n",
            "Aditi Singh\n",
            "Department of Computer Science\n",
            "Cleveland State University\n",
            "Cleveland, OH, USA\n",
            "a.singh22@csuohio.edu\n",
            "Abul Ehtesham\n",
            "The Davey Tree Expert Company\n",
            "Kent, OH, USA\n",
            "abul.ehtesham@davey.com\n",
            "Saket Kumar\n",
            "The MathWorks Inc\n",
            "Natick, MA, USA\n",
            "saketk@mathworks.com\n",
            "Tala Talaei Khoei\n",
            "Khoury College of Computer Science\n",
            "Roux Institute at Northeastern University\n",
            "Portland, ME, USA\n",
            "t.talaeikhoei@northeastern.edu\n",
            "ABSTRACT\n",
            "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human-\n",
            "like text generation and natural language understanding. However, their reliance on static training\n",
            "data limits their ability to respond to dynamic, real-time queries, resulting in outdated or inaccurate\n",
            "outputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\n",
            "integrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\n",
            "its promise, traditional RAG systems are constrained by static workflows and lack the adaptability\n",
            "required for multi-step reasoning and complex task management.\n",
            "Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embed-\n",
            "ding autonomous AI agents into the RAG pipeline. These agents leverage agentic design pat-\n",
            "terns—reflection, planning, tool use, and multi-agent collaboration—to dynamically manage retrieval\n",
            "strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task\n",
            "requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility,\n",
            "scalability, and context-awareness across diverse applications.\n",
            "This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational\n",
            "principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG\n",
            "architectures, highlights key applications in industries such as healthcare, finance, and education, and\n",
            "examines practical implementation strategies. Additionally, it addresses challenges in scaling these\n",
            "systems, ensuring ethical decision-making, and optimizing performance for real-world applications,\n",
            "while providing detailed insights into frameworks and tools for implementing Agentic RAG 1.\n",
            "Keywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·\n",
            "Retrieval-Augmented Generation (RAG) · Agentic RAG · Autonomous AI Agents · Reflection · Planning · Tool\n",
            "Use · Multi-Agent Collaboration · Agentic Patterns · Contextual Understanding · Dynamic Adaptability · Scalability ·\n",
            "Real-Time Data Retrieval · Taxonomy of Agentic RAG · Healthcare Applications · Finance Applications · Educational\n",
            "Applications · Ethical AI Decision-Making · Performance Optimization · Multi-Step Reasoning\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) [1, 2] [3], such as OpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMA, have signifi-\n",
            "cantly transformed artificial intelligence (AI) with their ability to generate human-like text and perform complex natural\n",
            "1GitHub link: https://github.com/asinghcsu/AgenticRAG-Survey\n",
            "arXiv:2501.09136v1  [cs.AI]  15 Jan 2025\n",
            "language processing tasks. These models have driven innovation across diverse domains, including conversational\n",
            "agents [4], automated content creation, and real-time translation. Recent advancements have extended their capabilities\n",
            "to multimodal tasks, such as text-to-image and text-to-video generation [5], enabling the creation and editing of videos\n",
            "and images from detailed prompts [6], which broadens the potential applications of generative AI.\n",
            "Despite these advancements, LLMs face significant limitations due to their reliance on static pre-training data. This\n",
            "reliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\n",
            "scenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\n",
            "responses to maintain contextual relevance\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2410.17783v1\" date=\"2024-10-23\" authors=\"Salman Rakin, Md. A. R. Shibly, Zahin M. Hossain, Zeeshan Khan, Md. Mostofa Akbar\"/>\n",
            "<Title>\n",
            "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "While ongoing advancements in Large Language Models have demonstrated\n",
            "remarkable success across various NLP tasks, Retrieval Augmented Generation\n",
            "Model stands out to be highly effective on downstream applications like\n",
            "Question Answering. Recently, RAG-end2end model further optimized the\n",
            "architecture and achieved notable performance improvements on domain\n",
            "adaptation. However, the effectiveness of these RAG-based architectures remains\n",
            "relatively unexplored when fine-tuned on specialized domains such as customer\n",
            "service for building a reliable conversational AI system. Furthermore, a\n",
            "critical challenge persists in reducing the occurrence of hallucinations while\n",
            "maintaining high domain-specific accuracy. In this paper, we investigated the\n",
            "performance of diverse RAG and RAG-like architectures through domain adaptation\n",
            "and evaluated their ability to generate accurate and relevant response grounded\n",
            "in the contextual knowledge base. To facilitate the evaluation of the models,\n",
            "we constructed a novel dataset HotelConvQA, sourced from wide range of\n",
            "hotel-related conversations and fine-tuned all the models on our domain\n",
            "specific dataset. We also addressed a critical research gap on determining the\n",
            "impact of domain adaptation on reducing hallucinations across different RAG\n",
            "architectures, an aspect that was not properly measured in prior work. Our\n",
            "evaluation shows positive results in all metrics by employing domain\n",
            "adaptation, demonstrating strong performance on QA tasks and providing insights\n",
            "into their efficacy in reducing hallucinations. Our findings clearly indicate\n",
            "that domain adaptation not only enhances the models' performance on QA tasks\n",
            "but also significantly reduces hallucination across all evaluated RAG\n",
            "architectures.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models\n",
            "for Question Answering and Reducing Hallucination\n",
            "Salman Rakin1, Md. A.R. Shibly2, Zahin M. Hossain3, Zeeshan Khan, Dr. Md. Mostofa Akbar\n",
            "salmankaderrakin@gmail.com, shibly.ar@gmail.com, zahinhasan2510@gmail.com, zeeshan@surroundapps.com,\n",
            "mostofa@cse.buet.ac.bd,\n",
            "Abstract\n",
            "While ongoing advancements in Large Language Models\n",
            "have demonstrated remarkable success across various NLP\n",
            "tasks, Retrieval Augmented Generation Model stands out to\n",
            "be highly effective on downstream applications like Question\n",
            "Answering. Recently, RAG-end2end model further optimized\n",
            "the architecture and achieved notable performance improve-\n",
            "ments on domain adaptation. However, the effectiveness of\n",
            "these RAG-based architectures remains relatively unexplored\n",
            "when fine-tuned on specialized domains such as customer\n",
            "service for building a reliable conversational AI system. Fur-\n",
            "thermore, a critical challenge persists in reducing the oc-\n",
            "currence of hallucinations while maintaining high domain-\n",
            "specific accuracy. In this paper, we investigated the perfor-\n",
            "mance of diverse RAG and RAG-like architectures through\n",
            "domain adaptation and evaluated their ability to generate\n",
            "accurate and relevant response grounded in the contextual\n",
            "knowledge base. To facilitate the evaluation of the models,\n",
            "we constructed a novel dataset HotelConvQA, sourced from\n",
            "wide range of hotel-related conversations and fine-tuned all\n",
            "the models on our domain specific dataset. We also addressed\n",
            "a critical research gap on determining the impact of domain\n",
            "adaptation on reducing hallucinations across different RAG\n",
            "architectures, an aspect that was not properly measured in\n",
            "prior work. Our evaluation shows positive results in all met-\n",
            "rics by employing domain adaptation, demonstrating strong\n",
            "performance on QA tasks and providing insights into their\n",
            "efficacy in reducing hallucinations. Our findings clearly in-\n",
            "dicate that domain adaptation not only enhances the models’\n",
            "performance on QA tasks but also significantly reduces hal-\n",
            "lucination across all evaluated RAG architectures.\n",
            "1\n",
            "Introduction\n",
            "Large Language Models (LLMs) generally store a vast\n",
            "amount of data encoded as factual knowledge in their\n",
            "parameters through fine-tuning on large corpora\n",
            "(Lewis\n",
            "et al. 2020c). In recent years, LLMs have demonstrated\n",
            "significant advancements in natural language processing\n",
            "tasks including question answering, summarization, and\n",
            "dialogue systems. Besides, LLMs play a pivotal role in de-\n",
            "veloping powerful agents, serving as essential components\n",
            "for reasoning and are fundamental for adaptation to new\n",
            "observations such as GPT in the context of Conversational\n",
            "AI (Wu et al. 2023) (Li, Yuan, and Zhang 2024a). Recently,\n",
            "Retrieval Augmented Generation (RAG) models have\n",
            "demonstrated significant improvements compared to LLMs\n",
            "by leveraging external knowledge sources because of its\n",
            "capability to combine pretrained parametric memory with\n",
            "explicit non-parametric memory (Lewis et al. 2020c) (Xu\n",
            "et al. 2024) (Johnson et al. 2023). Unlike the inherent built-\n",
            "in knowledge architectures of Large Language Models with\n",
            "limited access to external knowledge, RAG models have\n",
            "proven to be a promising approach for generating infor-\n",
            "mative and context-aware responses. Recent advancements\n",
            "in the field of retrieval-augmented generation models have\n",
            "shown great potential for improving the factual accuracy\n",
            "and coherence of language models’ responses rather than\n",
            "fine-tuning the entire LLM repeatedly (Rackauckas 2024).\n",
            "Moreover, similar retrieval augmented architectures like\n",
            "REALM (Guu et al. 2020) utilizes a fixed retriever compo-\n",
            "nent during the fine-tuning process and RETRO (Borgeaud\n",
            "et al. 2021) suggested that frozen BART embedding will be\n",
            "adequate for general-purpose retrieval from Wikipedia-like\n",
            "datasets. On the other hand, recent advancement on the\n",
            "original architecture of RAG, RAG-end2end (Siriwardhana\n",
            "et al. 2023) observed that updating both the passage\n",
            "encoder and query encode\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Nguyen. A real-world application where Modular RAG shows clear advantages over Naive RAG is in the deployment of question-answering systems, especially in knowledge-intensive domains like healthcare.\n",
            "\n",
            "For instance, in cancer patient question-answering systems, Modular RAG can provide more accurate and reliable responses by effectively managing large volumes of biomedical data. The modularity allows for the integration of advanced retrieval methods and real-time data updates, ensuring that the system can handle complex queries with precision and relevance. This is crucial for delivering timely and accurate information, reducing the risk of inaccuracies that could potentially impact patient care decisions [4].\n",
            "\n",
            "In contrast, a Naive RAG approach might struggle in such scenarios due to its limitations in processing complex queries and dealing with redundant or noisy information, which could lead to less reliable outputs [1]. Therefore, the adaptability and enhanced processing capabilities of Modular RAG make it a superior choice for production-level applications requiring high reliability and contextual accuracy.\n",
            "\n",
            "Sources:\n",
            "[1] livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\n",
            "[4] arxiv.org/abs/2412.14751v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello, I'm Alex Thompson, an analyst exploring the intricacies of AI deployment in real-world scenarios. Dr. Nguyen, could you start by explaining how Modular RAG specifically differs from the traditional Naive RAG in the context of AI systems?\n",
            "AI: Certainly, Dr. Nguyen. Modular RAG and traditional Naive RAG differ significantly in their frameworks and capabilities:\n",
            "\n",
            "1. **Framework Structure**:\n",
            "   - **Naive RAG**: This approach combines document retrieval with language model generation in a straightforward manner. It relies heavily on the similarity of data chunks to retrieve information, which can lead to performance issues with complex queries or data that has substantial variability [1].\n",
            "   - **Modular RAG**: In contrast, Modular RAG decomposes complex RAG systems into independent modules and specialized operators, offering a highly reconfigurable framework. This modular approach goes beyond the linear \"retrieve-then-generate\" process of Naive RAG by incorporating routing, scheduling, and fusion mechanisms, allowing for a more sophisticated and adaptable system [2].\n",
            "\n",
            "2. **Handling of Information**:\n",
            "   - **Naive RAG**: It tends to have a shallow understanding of queries, often resulting in retrieval redundancy and noise. This can lead to the generation of erroneous or hallucinated responses due to the direct use of all retrieved chunks [1].\n",
            "   - **Modular RAG**: By using a modular design, it allows for more precise information processing and reduces noise. This results in more accurate and context-aware outputs, crucial for tasks requiring current or specialized knowledge [2].\n",
            "\n",
            "3. **Reconfigurability and Adaptability**:\n",
            "   - **Naive RAG**: Generally lacks flexibility and struggles with the integration of advanced retrieval and generation methods [3].\n",
            "   - **Modular RAG**: Its LEGO-like reconfigurable framework enables easy adaptation to different application scenarios, making it suitable for production-level deployment where robustness and reliability are critical [2].\n",
            "\n",
            "In terms of benefits at the production level, Modular RAG offers enhanced reliability and robustness compared to Naive RAG. Its ability to adapt to specific application needs and reduce noise in retrieved information makes it particularly valuable in environments where accuracy and contextual relevance are paramount.\n",
            "\n",
            "Sources:\n",
            "[1] livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\n",
            "[2] arxiv.org/abs/2407.21059v1\n",
            "[3] zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "AI: Thank you for that detailed explanation. Could you provide a specific example of a real-world application where Modular RAG has demonstrated a clear advantage over Naive RAG, particularly in terms of robustness and reliability?\n",
            "AI: Certainly, Dr. Nguyen. A real-world application where Modular RAG shows clear advantages over Naive RAG is in the deployment of question-answering systems, especially in knowledge-intensive domains like healthcare.\n",
            "\n",
            "For instance, in cancer patient question-answering systems, Modular RAG can provide more accurate and reliable responses by effectively managing large volumes of biomedical data. The modularity allows for the integration of advanced retrieval methods and real-time data updates, ensuring that the system can handle complex queries with precision and relevance. This is crucial for delivering timely and accurate information, reducing the risk of inaccuracies that could potentially impact patient care decisions [4].\n",
            "\n",
            "In contrast, a Naive RAG approach might struggle in such scenarios due to its limitations in processing complex queries and dealing with redundant or noisy information, which could lead to less reliable outputs [1]. Therefore, the adaptability and enhanced processing capabilities of Modular RAG make it a superior choice for production-level applications requiring high reliability and contextual accuracy.\n",
            "\n",
            "Sources:\n",
            "[1] livebook.manning.com/book/a-simple-guide-to-retrieval-augmented-generation/chapter-6/v-4\n",
            "[4] arxiv.org/abs/2412.14751v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
            "<Title>\n",
            "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
            "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
            "increasing demands of application scenarios have driven the evolution of RAG,\n",
            "leading to the integration of advanced retrievers, LLMs and other complementary\n",
            "technologies, which in turn has amplified the intricacy of RAG systems.\n",
            "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process of\n",
            "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
            "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
            "decomposing complex RAG systems into independent modules and specialized\n",
            "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a more advanced\n",
            "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
            "extensive research, this paper further identifies prevalent RAG\n",
            "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
            "analysis of their respective implementation nuances. Modular RAG presents\n",
            "innovative opportunities for the conceptualization and deployment of RAG\n",
            "systems. Finally, the paper explores the potential emergence of new operators\n",
            "and paradigms, establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment of RAG\n",
            "technologies.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "1\n",
            "Modular RAG: Transforming RAG Systems into\n",
            "LEGO-like Reconfigurable Frameworks\n",
            "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
            "Abstract—Retrieval-augmented\n",
            "Generation\n",
            "(RAG)\n",
            "has\n",
            "markedly enhanced the capabilities of Large Language Models\n",
            "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
            "demands of application scenarios have driven the evolution\n",
            "of RAG, leading to the integration of advanced retrievers,\n",
            "LLMs and other complementary technologies, which in turn\n",
            "has amplified the intricacy of RAG systems. However, the rapid\n",
            "advancements are outpacing the foundational RAG paradigm,\n",
            "with many methods struggling to be unified under the process\n",
            "of “retrieve-then-generate”. In this context, this paper examines\n",
            "the limitations of the existing RAG paradigm and introduces\n",
            "the modular RAG framework. By decomposing complex RAG\n",
            "systems into independent modules and specialized operators, it\n",
            "facilitates a highly reconfigurable framework. Modular RAG\n",
            "transcends the traditional linear architecture, embracing a\n",
            "more advanced design that integrates routing, scheduling, and\n",
            "fusion mechanisms. Drawing on extensive research, this paper\n",
            "further identifies prevalent RAG patterns—linear, conditional,\n",
            "branching, and looping—and offers a comprehensive analysis\n",
            "of their respective implementation nuances. Modular RAG\n",
            "presents\n",
            "innovative\n",
            "opportunities\n",
            "for\n",
            "the\n",
            "conceptualization\n",
            "and deployment of RAG systems. Finally, the paper explores\n",
            "the potential emergence of new operators and paradigms,\n",
            "establishing a solid theoretical foundation and a practical\n",
            "roadmap for the continued evolution and practical deployment\n",
            "of RAG technologies.\n",
            "Index Terms—Retrieval-augmented generation, large language\n",
            "model, modular system, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE Language Models (LLMs) have demonstrated\n",
            "remarkable capabilities, yet they still face numerous\n",
            "challenges, such as hallucination and the lag in information up-\n",
            "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
            "ing external knowledge bases, provides LLMs with important\n",
            "contextual information, significantly enhancing their perfor-\n",
            "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
            "an enhancement method, has been widely applied in various\n",
            "practical application scenarios, including knowledge question\n",
            "answering, recommendation systems, customer service, and\n",
            "personal assistants. [3]–[6]\n",
            "During the nascent stages of RAG , its core framework is\n",
            "constituted by indexing, retrieval, and generation, a paradigm\n",
            "referred to as Naive RAG [7]. However, as the complexity\n",
            "of tasks and the demands of applications have escalated, the\n",
            "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
            "Systems, Tongji University, Shanghai, 201210, China.\n",
            "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
            "Computer Science, Fudan University, Shanghai, 200438, China.\n",
            "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
            "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
            "Wang. E-mail: carter.whfcarter@gmail.com)\n",
            "limitations of Naive RAG have become increasingly apparent.\n",
            "As depicted in Figure 1, it predominantly hinges on the\n",
            "straightforward similarity of chunks, result in poor perfor-\n",
            "mance when confronted with complex queries and chunks with\n",
            "substantial variability. The primary challenges of Naive RAG\n",
            "include: 1) Shallow Understanding of Queries. The semantic\n",
            "similarity between a query and document chunk is not always\n",
            "highly consistent. Relying solely on similarity calculations\n",
            "for retrieval lacks an in-depth exploration of the relationship\n",
            "between the query and the document [8]. 2) Retrieval Re-\n",
            "dundancy and Noise. Feeding all retrieved chunks directly\n",
            "into LLMs is not always beneficial. Research indicates that\n",
            "an excess of redundant and noisy information may interfere\n",
            "with the LLM’s identification of key information, thereby\n",
            "increasing the risk of generating erroneous and hallucinated\n",
            "responses. [9]\n",
            "To overcome the aforementioned limitations, \n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2401.05856v1\" date=\"2024-01-11\" authors=\"Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\"/>\n",
            "<Title>\n",
            "Seven Failure Points When Engineering a Retrieval Augmented Generation System\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "Software engineers are increasingly adding semantic search capabilities to\n",
            "applications using a strategy known as Retrieval Augmented Generation (RAG). A\n",
            "RAG system involves finding documents that semantically match a query and then\n",
            "passing the documents to a large language model (LLM) such as ChatGPT to\n",
            "extract the right answer using an LLM. RAG systems aim to: a) reduce the\n",
            "problem of hallucinated responses from LLMs, b) link sources/references to\n",
            "generated responses, and c) remove the need for annotating documents with\n",
            "meta-data. However, RAG systems suffer from limitations inherent to information\n",
            "retrieval systems and from reliance on LLMs. In this paper, we present an\n",
            "experience report on the failure points of RAG systems from three case studies\n",
            "from separate domains: research, education, and biomedical. We share the\n",
            "lessons learned and present 7 failure points to consider when designing a RAG\n",
            "system. The two key takeaways arising from our work are: 1) validation of a RAG\n",
            "system is only feasible during operation, and 2) the robustness of a RAG system\n",
            "evolves rather than designed in at the start. We conclude with a list of\n",
            "potential research directions on RAG systems for the software engineering\n",
            "community.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            "Seven Failure Points When Engineering a Retrieval Augmented\n",
            "Generation System\n",
            "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek\n",
            "{scott.barnett,stefanus.kurniawan,srikanth.thudumu,zach.brannelly,mohamed.abdelrazek}@deakin.edu.au\n",
            "Applied Artificial Intelligence Institute\n",
            "Geelong, Australia\n",
            "ABSTRACT\n",
            "Software engineers are increasingly adding semantic search capabil-\n",
            "ities to applications using a strategy known as Retrieval Augmented\n",
            "Generation (RAG). A RAG system involves finding documents that\n",
            "semantically match a query and then passing the documents to a\n",
            "large language model (LLM) such as ChatGPT to extract the right\n",
            "answer using an LLM. RAG systems aim to: a) reduce the problem\n",
            "of hallucinated responses from LLMs, b) link sources/references\n",
            "to generated responses, and c) remove the need for annotating\n",
            "documents with meta-data. However, RAG systems suffer from lim-\n",
            "itations inherent to information retrieval systems and from reliance\n",
            "on LLMs. In this paper, we present an experience report on the\n",
            "failure points of RAG systems from three case studies from separate\n",
            "domains: research, education, and biomedical. We share the lessons\n",
            "learned and present 7 failure points to consider when designing a\n",
            "RAG system. The two key takeaways arising from our work are: 1)\n",
            "validation of a RAG system is only feasible during operation, and\n",
            "2) the robustness of a RAG system evolves rather than designed in\n",
            "at the start. We conclude with a list of potential research directions\n",
            "on RAG systems for the software engineering community.\n",
            "CCS CONCEPTS\n",
            "• Software and its engineering →Empirical software valida-\n",
            "tion.\n",
            "KEYWORDS\n",
            "Retrieval Augmented Generation, RAG, SE4AI, Case Study\n",
            "ACM Reference Format:\n",
            "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-\n",
            "hamed Abdelrazek . 2024. Seven Failure Points When Engineering a Retrieval\n",
            "Augmented Generation System. In Proceedings of 3rd International Confer-\n",
            "ence on AI Engineering — Software Engineering for AI (CAIN 2024). ACM,\n",
            "New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n",
            "1\n",
            "INTRODUCTION\n",
            "The new advancements of Large Language Models (LLMs), includ-\n",
            "ing ChatGPT, have given software engineers new capabilities to\n",
            "Permission to make digital or hard copies of all or part of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for profit or commercial advantage and that copies bear this notice and the full citation\n",
            "on the first page. Copyrights for components of this work owned by others than ACM\n",
            "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
            "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
            "fee. Request permissions from permissions@acm.org.\n",
            "CAIN 2024, April 2024, Lisbon, Portugal\n",
            "© 2024 Association for Computing Machinery.\n",
            "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\n",
            "https://doi.org/10.1145/nnnnnnn.nnnnnnn\n",
            "build new HCI solutions, complete complex tasks, summarise docu-\n",
            "ments, answer questions in a given artefact(s), and generate new\n",
            "content. However, LLMs suffer from limitations when it comes\n",
            "to up-to-date knowledge or domain-specific knowledge currently\n",
            "captured in company’s repositories. Two options to address this\n",
            "problem are: a) Finetuning LLMs (continue training an LLM using\n",
            "domain specific artifacts) which requires managing or serving a\n",
            "fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)\n",
            "Systems that rely on LLMs for generation of answers using existing\n",
            "(extensible) knowledge artifacts. Both options have pros and cons\n",
            "related to privacy/security of data, scalability, cost, skills required,\n",
            "etc. In this paper, we focus on the RAG option.\n",
            "Retrieval-Augmented Generation (RAG) systems offer a com-\n",
            "pelling solution to this challenge. By integrating retrieval mecha-\n",
            "nisms with the generative capabilities of LLMs, RAG systems can\n",
            "synthesise contextually relevant, accurate, and up-to-date informa-\n",
            "tio\n",
            "</Content>\n",
            "</Document>\n",
            "\n",
            "---\n",
            "\n",
            "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
            "<Title>\n",
            "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
            "</Title>\n",
            "\n",
            "<Summary>\n",
            "With the growing demand for offline PDF chatbots in automotive industrial\n",
            "production environments, optimizing the deployment of large language models\n",
            "(LLMs) in local, low-performance settings has become increasingly important.\n",
            "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
            "for processing complex automotive industry documents using locally deployed\n",
            "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
            "optimization approach for Ollama's local RAG implementation. Our method\n",
            "addresses key challenges in automotive document processing, including\n",
            "multi-column layouts and technical specifications. We introduce improvements in\n",
            "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
            "unique characteristics of automotive industry documents. Additionally, we\n",
            "design custom classes supporting embedding pipelines and an agent supporting\n",
            "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
            "constructed a proprietary dataset comprising typical automotive industry\n",
            "documents, including technical reports and corporate regulations. We compared\n",
            "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
            "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
            "demonstrate significant improvements in context precision, context recall,\n",
            "answer relevancy, and faithfulness, with particularly notable performance on\n",
            "the automotive industry dataset. Our optimization scheme provides an effective\n",
            "solution for deploying local RAG systems in the automotive sector, addressing\n",
            "the specific needs of PDF chatbots in industrial production environments. This\n",
            "research has important implications for advancing information processing and\n",
            "intelligent production in the automotive industry.\n",
            "</Summary>\n",
            "\n",
            "<Content>\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
            "Locally Deployed Ollama Models \n",
            "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
            "A Case Study with Locally Deployed Ollama Models \n",
            "Fei Liu * \n",
            "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
            "Zejun Kang \n",
            "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
            "Xing Han \n",
            "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
            "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
            "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
            "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
            "locally deployed Ollama models. \n",
            "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
            "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
            "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
            "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
            "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
            "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
            "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
            "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
            "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
            "industry dataset. \n",
            "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
            "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
            "information processing and intelligent production in the automotive industry. \n",
            " \n",
            "* Place the footnote text for the author (if applicable) here.  \n",
            "CCS CONCEPTS • Computing methodologies • Artificial intelligence • Natural language processing • Natural language \n",
            "generation \n",
            " \n",
            "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
            "1 INTRODUCTION \n",
            "1.1 Research Background \n",
            "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
            "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
            "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
            "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
            "question-answering capabilities in industrial settings [3]. \n",
            "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
            "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
            "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
            "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
            "computational resources and data privacy [5]. \n",
            "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
            "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
            "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
            "Lew\n",
            "</Content>\n",
            "</Document>\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly! One clear example of how Modular RAG's adaptability enhances AI system performance is in handling complex, heterogeneous data environments. In a traditional Naive RAG setup, the system might struggle with diverse data sources due to its monolithic architecture, which isn't optimized for different types of data retrieval processes [4]. \n",
            "\n",
            "In contrast, Modular RAG employs a multi-agent framework where specialized agents handle specific tasks such as query generation, data retrieval, and response synthesis. Each agent is optimized for particular data sources—be it relational databases, NoSQL systems, or document-based systems. This modular approach allows the system to efficiently manage and integrate diverse datasets, ensuring that each component focuses on its specialized task, thereby reducing overhead and improving response accuracy [4].\n",
            "\n",
            "For example, in an enterprise setting where data might be spread across various formats and databases, Modular RAG can dynamically adjust its operations to retrieve the most relevant information from each source. This capability is particularly beneficial in industries like finance or healthcare, where accessing accurate and up-to-date information is crucial. The modularity allows for seamless integration and scalability, making it ideal for production-level applications that require robust and adaptive data handling capabilities [4].\n",
            "\n",
            "Source:\n",
            "[4] http://arxiv.org/abs/2412.05838v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello, my name is Alex Carter, and I'm an analyst eager to delve deeper into the world of AI models. Thank you for taking the time to speak with me, Prof. Hinton. Let's start with the basics: Could you explain what Modular RAG is and how it fundamentally differs from traditional Naive RAG?\n",
            "AI: Certainly, Prof. Hinton. Modular RAG, or Modular Retrieval-Augmented Generation, is an advanced approach that builds upon the foundational principles of Naive RAG. Naive RAG combines information retrieval with natural language generation to produce responses to queries. It typically uses retrieval models to rank indexed data based on relevance to the input query, generating text that aims to be coherent and contextually relevant [1].\n",
            "\n",
            "However, Naive RAG has certain limitations, such as inflexibility and inefficiencies when handling diverse and dynamic datasets [3]. This is where Modular RAG comes into play. Modular RAG enhances the traditional approach by incorporating dynamic and flexible components that allow for better adaptation to varying datasets and contexts. For instance, Modular RAG can fine-tune embeddings to capture task-specific semantics or domain knowledge, thus improving the quality of both the retrieved information and the generated responses [1].\n",
            "\n",
            "One of the key benefits of using Modular RAG at the production level is its enhanced efficiency and scalability. By using dynamic embedding techniques, Modular RAG can adaptively adjust embeddings during inference based on the context of the query or retrieved information. This adaptability makes it more suitable for applications that require current or specialized knowledge, as it can deliver more accurate and context-aware outputs [2].\n",
            "\n",
            "Overall, the modularity and flexibility of Modular RAG make it a more robust and scalable solution for production-level applications compared to traditional Naive RAG.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "AI: Thank you for breaking that down. I'm curious about the adaptability you mentioned. Can you provide a specific example of how Modular RAG's adaptability can enhance an AI system's performance in a real-world application?\n",
            "AI: Certainly! One clear example of how Modular RAG's adaptability enhances AI system performance is in handling complex, heterogeneous data environments. In a traditional Naive RAG setup, the system might struggle with diverse data sources due to its monolithic architecture, which isn't optimized for different types of data retrieval processes [4]. \n",
            "\n",
            "In contrast, Modular RAG employs a multi-agent framework where specialized agents handle specific tasks such as query generation, data retrieval, and response synthesis. Each agent is optimized for particular data sources—be it relational databases, NoSQL systems, or document-based systems. This modular approach allows the system to efficiently manage and integrate diverse datasets, ensuring that each component focuses on its specialized task, thereby reducing overhead and improving response accuracy [4].\n",
            "\n",
            "For example, in an enterprise setting where data might be spread across various formats and databases, Modular RAG can dynamically adjust its operations to retrieve the most relevant information from each source. This capability is particularly beneficial in industries like finance or healthcare, where accessing accurate and up-to-date information is crucial. The modularity allows for seamless integration and scalability, making it ideal for production-level applications that require robust and adaptive data handling capabilities [4].\n",
            "\n",
            "Source:\n",
            "[4] http://arxiv.org/abs/2412.05838v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly. In production environments where complex, knowledge-intensive tasks are required, Modular RAG demonstrates its strengths over Naive RAG. One example could be in the context of a customer service application that handles a wide range of inquiries, each potentially requiring access to different datasets or sources of information.\n",
            "\n",
            "In such a scenario, Naive RAG might struggle due to its reliance on straightforward similarity calculations for retrieval, which can result in poor performance when dealing with complex queries or diverse data chunks. This can lead to retrieval redundancy and noise, where the system retrieves too much irrelevant information, impacting the quality of the generated responses [4].\n",
            "\n",
            "Modular RAG, by contrast, allows for a more nuanced and efficient approach. With its modular framework, the system can incorporate specialized modules for different types of queries or data sources. For example, using dynamic embedding techniques and advanced retriever modules, Modular RAG can better handle the variability in data and query complexity, ensuring that only the most relevant information is retrieved and used in generating responses [5]. This results in more accurate and contextually appropriate answers, improving the overall effectiveness of the customer service application.\n",
            "\n",
            "Moreover, the ability to customize and reconfigure components of the Modular RAG system means that as the application scales or as new types of inquiries are introduced, the system can be quickly adapted without the need for extensive re-engineering [4]. This adaptability is a key advantage in maintaining performance and efficiency at scale.\n",
            "\n",
            "Sources:\n",
            "[4] http://arxiv.org/abs/2407.21059v1\n",
            "[5] https://drjulija.github.io/posts/rag/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello, my name is Alex Reed, and I'm an analyst exploring the intricacies of AI scalability. I'm very interested in understanding how Modular RAG differs from traditional Naive RAG and the benefits it offers, particularly in production environments. Dr. Thompson, could you elaborate on the key differences between Modular RAG and Naive RAG? Specifically, how does the modular approach enhance scalability?\n",
            "AI: Certainly, let's explore the differences between Modular RAG and traditional Naive RAG, along with the benefits of using Modular RAG in production environments.\n",
            "\n",
            "Traditional Naive RAG combines document retrieval with language model generation to produce context-aware responses. It typically involves a basic framework where retrieval models rank indexed data based on relevance to an input query, thereby aiding the generation of text that is coherent and contextually relevant [1]. However, Naive RAG lacks advanced optimization features, making it less efficient in handling complex queries or domain-specific tasks.\n",
            "\n",
            "On the other hand, Modular RAG introduces a more flexible and scalable approach. It allows customization of components such as query generation, retrieval, and memory management to suit specific tasks. This modularity enables the integration of advanced techniques like multi-query modules and pre-retrieval optimization strategies, such as step-back prompting and Hypothetical Embeddings (HyDE), which enhance query accuracy and result quality [3]. \n",
            "\n",
            "The scalability benefits of Modular RAG at the production level are significant. By customizing and optimizing each component, it reduces computational overhead and improves performance efficiency. This approach is particularly beneficial for applications that require current or specialized knowledge, as it allows for precise similarity searches and more accurate, context-aware outputs [2]. The ability to fine-tune embeddings for task-specific semantics or domain knowledge further enhances the quality of information retrieval and generated responses, making Modular RAG a more robust choice for scalable AI applications.\n",
            "\n",
            "Sources:\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "[3] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e\n",
            "AI: Thank you for that detailed explanation, Dr. Thompson. You've mentioned the computational overhead and performance efficiency improvements with Modular RAG. Can you provide a specific example of a production scenario where Modular RAG has significantly outperformed Naive RAG?\n",
            "AI: Certainly. In production environments where complex, knowledge-intensive tasks are required, Modular RAG demonstrates its strengths over Naive RAG. One example could be in the context of a customer service application that handles a wide range of inquiries, each potentially requiring access to different datasets or sources of information.\n",
            "\n",
            "In such a scenario, Naive RAG might struggle due to its reliance on straightforward similarity calculations for retrieval, which can result in poor performance when dealing with complex queries or diverse data chunks. This can lead to retrieval redundancy and noise, where the system retrieves too much irrelevant information, impacting the quality of the generated responses [4].\n",
            "\n",
            "Modular RAG, by contrast, allows for a more nuanced and efficient approach. With its modular framework, the system can incorporate specialized modules for different types of queries or data sources. For example, using dynamic embedding techniques and advanced retriever modules, Modular RAG can better handle the variability in data and query complexity, ensuring that only the most relevant information is retrieved and used in generating responses [5]. This results in more accurate and contextually appropriate answers, improving the overall effectiveness of the customer service application.\n",
            "\n",
            "Moreover, the ability to customize and reconfigure components of the Modular RAG system means that as the application scales or as new types of inquiries are introduced, the system can be quickly adapted without the need for extensive re-engineering [4]. This adaptability is a key advantage in maintaining performance and efficiency at scale.\n",
            "\n",
            "Sources:\n",
            "[4] http://arxiv.org/abs/2407.21059v1\n",
            "[5] https://drjulija.github.io/posts/rag/\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: expert\n",
            "\n",
            "Certainly, Dr. Patel. A notable case study illustrating the advantages of Modular RAG over Naive RAG is found in the automotive industry. In this sector, an optimization of RAG techniques was implemented to enhance PDF chatbots using locally deployed Ollama models. This study focused on addressing challenges inherent in processing complex automotive documents, like multi-column layouts and technical specifications.\n",
            "\n",
            "**Example: Automotive Industry PDF Chatbots**:\n",
            "- **Problem**: Automotive industry environments require efficient processing of extensive technical documentation, often in PDF, which poses challenges in terms of layout complexity and computational resources.\n",
            "- **Solution**: The implementation of a Modular RAG system allowed for the integration of custom classes supporting embedding pipelines and an agent for self-RAG, based on LangGraph best practices. This setup was tailored to the unique document characteristics of the automotive sector.\n",
            "- **Results**: Compared to a Naive RAG baseline, the optimized Modular RAG model demonstrated significant improvements in context precision, context recall, answer relevancy, and faithfulness. The study highlighted particularly notable performance improvements in handling automotive industry documents, thus providing a more effective solution for local RAG systems in industrial production environments [5].\n",
            "\n",
            "This case underscores the flexibility and efficiency of Modular RAG systems in real-world applications, where they can be fine-tuned to meet specific industry needs, thereby surpassing the capabilities of traditional Naive RAG setups.\n",
            "\n",
            "Source:\n",
            "[5] arxiv.org/abs/2408.05933v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32minterview\u001b[0m:\n",
            "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
            "AI: Hello Dr. Patel, my name is Alex Green, and I'm a technology writer keen on delving deeper into the nuances of AI architectures. I understand that you have a focus on efficiency and optimization, particularly regarding Modular RAG and Naive RAG. Could you explain how Modular RAG differs from traditional Naive RAG in terms of efficiency and resource management? Are there any specific examples or case studies you could share to illustrate these differences?\n",
            "AI: Certainly, Dr. Patel. Modular RAG significantly differs from traditional Naive RAG in its approach to efficiency and resource management, particularly at the production level.\n",
            "\n",
            "1. **Modular Architecture**: Modular RAG is characterized by its flexible and compartmentalized design, which allows for the integration of various specialized modules. This modularity means that different components, such as query rewriting, knowledge filtering, and memory management, can be optimized independently. This contrasts with Naive RAG, which typically follows a more linear 'retrieve-then-read' pipeline without such flexibility [2].\n",
            "\n",
            "2. **Enhanced Retrieval Efficiency**: In Modular RAG, components like the Query Rewriter+ and Knowledge Filter are used to enhance retrieval efficiency. The Query Rewriter+ generates multiple queries to overcome information plateaus and eliminates ambiguities, while the Knowledge Filter removes irrelevant information. These enhancements lead to more precise and efficient retrieval processes compared to the more straightforward retrieval methods in Naive RAG [2].\n",
            "\n",
            "3. **Resource Optimization**: Modular RAG systems include modules like the Memory Knowledge Reservoir and the Retriever Trigger, which optimize resource usage by dynamically expanding the knowledge base and managing access to external knowledge sources. This reduces redundant retrievals and improves the overall efficiency of the system, which is a notable improvement over the resource management in Naive RAG [2].\n",
            "\n",
            "4. **Scalability and Cost-Effectiveness**: Modular RAG frameworks, such as EACO-RAG, leverage edge-assisted architectures to distribute workloads and optimize retrieval processes. This results in reduced delays and lower resource consumption, making them more scalable and cost-effective compared to traditional centralized systems used in Naive RAG [3].\n",
            "\n",
            "These differences in architecture and process mean that Modular RAG systems are better suited for production environments where efficiency, scalability, and cost-effectiveness are critical. They offer improved accuracy and context-aware outputs, essential for applications requiring current or specialized knowledge [4].\n",
            "\n",
            "Sources:\n",
            "[1] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
            "[2] arxiv.org/abs/2407.10670v1\n",
            "[3] arxiv.org/abs/2410.20299v1\n",
            "[4] zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\n",
            "AI: Thank you for that detailed explanation, Dr. Patel. Could you provide any specific examples or case studies where Modular RAG has been implemented successfully, demonstrating its advantages over Naive RAG in a real-world application?\n",
            "AI: Certainly, Dr. Patel. A notable case study illustrating the advantages of Modular RAG over Naive RAG is found in the automotive industry. In this sector, an optimization of RAG techniques was implemented to enhance PDF chatbots using locally deployed Ollama models. This study focused on addressing challenges inherent in processing complex automotive documents, like multi-column layouts and technical specifications.\n",
            "\n",
            "**Example: Automotive Industry PDF Chatbots**:\n",
            "- **Problem**: Automotive industry environments require efficient processing of extensive technical documentation, often in PDF, which poses challenges in terms of layout complexity and computational resources.\n",
            "- **Solution**: The implementation of a Modular RAG system allowed for the integration of custom classes supporting embedding pipelines and an agent for self-RAG, based on LangGraph best practices. This setup was tailored to the unique document characteristics of the automotive sector.\n",
            "- **Results**: Compared to a Naive RAG baseline, the optimized Modular RAG model demonstrated significant improvements in context precision, context recall, answer relevancy, and faithfulness. The study highlighted particularly notable performance improvements in handling automotive industry documents, thus providing a more effective solution for local RAG systems in industrial production environments [5].\n",
            "\n",
            "This case underscores the flexibility and efficiency of Modular RAG systems in real-world applications, where they can be fine-tuned to meet specific industry needs, thereby surpassing the capabilities of traditional Naive RAG setups.\n",
            "\n",
            "Source:\n",
            "[5] arxiv.org/abs/2408.05933v1\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Harnessing Modular RAG for AI Systems: A Comparative Analysis with Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly evolving field of artificial intelligence, the integration of advanced AI systems into real-world applications is a subject of significant interest and potential. Among the emerging methodologies, Modular Retrieval-Augmented Generation (RAG) frameworks have been spotlighted for their ability to enhance Large Language Models (LLMs) by structuring complex retrieval processes into reconfigurable, independent modules. This report delves into the comparative advantages of Modular RAG systems over traditional Naive RAG approaches, drawing insights from recent studies and applications.\n",
            "\n",
            "The novelty of Modular RAG lies in its design flexibility and robustness, offering a departure from the linear and often rigid architecture of Naive RAG. Modular RAG frameworks incorporate sophisticated routing, scheduling, and fusion mechanisms, addressing the challenges posed by the increasingly intricate demands of AI applications. This modular approach not only improves the adaptability and scalability of AI systems but also enhances performance by reducing noise and redundancy in data retrieval processes.\n",
            "\n",
            "Key insights from recent research include:\n",
            "1. **[1]** The foundational differences between Naive RAG and Modular RAG, highlighting how the latter overcomes limitations related to query understanding and data noise.\n",
            "2. **[2]** The introduction of a theoretical model for token-level harmonization in RAG, which seeks to balance the benefits of external data against potential detriments caused by noisy inputs.\n",
            "3. **[3]** Practical implementations of Modular RAG in industrial settings, showcasing improvements in accuracy and contextual understanding in knowledge-intensive tasks.\n",
            "4. **[4]** The role of Modular RAG in enhancing AI applications across domains, including agriculture and biomedical systems, by providing a framework for more accurate, context-aware responses.\n",
            "\n",
            "Overall, Modular RAG presents a promising advancement in the field of AI, offering a robust solution for integrating LLMs into various domains, thereby paving the way for more precise and reliable AI-driven insights.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Understanding Modular RAG: A Shift from Naive Frameworks\n",
            "\n",
            "The Modular RAG framework represents a significant evolution from the traditional Naive RAG systems, which primarily rely on a linear \"retrieve-then-generate\" architecture. Naive RAG systems, though foundational, often suffer from shallow query understanding and excessive noise due to redundant data retrieval. In contrast, Modular RAG offers a reconfigurable framework that decomposes complex processes into independent modules. This modularity facilitates enhancements in routing, scheduling, and fusion, crucial for handling diverse and intricate AI tasks [1].\n",
            "\n",
            "- **Key Features of Modular RAG:**\n",
            "  - **Independent Modules:** Each module focuses on specific tasks like retrieval, processing, or generation, allowing for targeted improvements and easier troubleshooting.\n",
            "  - **Advanced Design Integration:** Incorporates mechanisms for routing and scheduling, ensuring efficient data flow and reducing computational overhead.\n",
            "  - **Adaptability:** Easily reconfigurable to suit different application demands, enhancing versatility across various industries.\n",
            "\n",
            "#### Theoretical Underpinnings: Token-Level Harmonization\n",
            "\n",
            "A significant advancement in understanding RAG systems is the theoretical framework proposed for token-level harmonization. This model conceptualizes RAG as a fusion of the LLM's knowledge and retrieved texts, identifying the trade-offs between the benefits of external data (enhanced context and accuracy) and potential detriments (noise and misinformation). The theory underlines the importance of balancing these elements to optimize RAG performance without additional training or utility evaluators [2].\n",
            "\n",
            "- **Benefits of Token-Level Harmonization:**\n",
            "  - **Predictive Capability:** Allows for estimation of RAG effects on token prediction, enhancing reliability.\n",
            "  - **Data-Driven Improvements:** Supports data-driven approaches while providing a theoretical basis for understanding and mitigating detriments.\n",
            "\n",
            "#### Applications and Impact: From Agriculture to Healthcare\n",
            "\n",
            "The application of Modular RAG spans various sectors, each benefiting from its unique capability to integrate and utilize domain-specific knowledge effectively.\n",
            "\n",
            "- **Agriculture:** By incorporating location-specific data, Modular RAG systems have demonstrated a marked improvement in providing actionable insights to farmers, enhancing decision-making processes with geographic-specific knowledge [1].\n",
            "- **Biomedical Systems:** In healthcare, particularly for cancer patient question-answering systems, Modular RAG frameworks have been instrumental in improving accuracy and reliability by optimizing query pipelines and leveraging domain-specific databases [3].\n",
            "\n",
            "#### Challenges and Future Directions\n",
            "\n",
            "Despite its advantages, the implementation of Modular RAG systems is not without challenges. The complexity of designing and maintaining reconfigurable modules requires significant expertise and resources. Furthermore, the integration of real-time data and the need for continual updates and optimizations pose ongoing challenges.\n",
            "\n",
            "- **Future Research Directions:**\n",
            "  - **Enhanced Real-Time Data Integration:** Developing methods to incorporate dynamic data effectively into Modular RAG systems.\n",
            "  - **Scalability and Maintenance:** Exploring solutions to streamline the scalability of complex modular systems without compromising performance.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2401.08406v3  \n",
            "[2] http://arxiv.org/abs/2406.00944v2  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Harnessing Modular RAG for AI Systems: A Comparative Analysis with Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the rapidly evolving field of artificial intelligence, the integration of advanced AI systems into real-world applications is a subject of significant interest and potential. Among the emerging methodologies, Modular Retrieval-Augmented Generation (RAG) frameworks have been spotlighted for their ability to enhance Large Language Models (LLMs) by structuring complex retrieval processes into reconfigurable, independent modules. This report delves into the comparative advantages of Modular RAG systems over traditional Naive RAG approaches, drawing insights from recent studies and applications.\n",
            "\n",
            "The novelty of Modular RAG lies in its design flexibility and robustness, offering a departure from the linear and often rigid architecture of Naive RAG. Modular RAG frameworks incorporate sophisticated routing, scheduling, and fusion mechanisms, addressing the challenges posed by the increasingly intricate demands of AI applications. This modular approach not only improves the adaptability and scalability of AI systems but also enhances performance by reducing noise and redundancy in data retrieval processes.\n",
            "\n",
            "Key insights from recent research include:\n",
            "1. **[1]** The foundational differences between Naive RAG and Modular RAG, highlighting how the latter overcomes limitations related to query understanding and data noise.\n",
            "2. **[2]** The introduction of a theoretical model for token-level harmonization in RAG, which seeks to balance the benefits of external data against potential detriments caused by noisy inputs.\n",
            "3. **[3]** Practical implementations of Modular RAG in industrial settings, showcasing improvements in accuracy and contextual understanding in knowledge-intensive tasks.\n",
            "4. **[4]** The role of Modular RAG in enhancing AI applications across domains, including agriculture and biomedical systems, by providing a framework for more accurate, context-aware responses.\n",
            "\n",
            "Overall, Modular RAG presents a promising advancement in the field of AI, offering a robust solution for integrating LLMs into various domains, thereby paving the way for more precise and reliable AI-driven insights.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Understanding Modular RAG: A Shift from Naive Frameworks\n",
            "\n",
            "The Modular RAG framework represents a significant evolution from the traditional Naive RAG systems, which primarily rely on a linear \"retrieve-then-generate\" architecture. Naive RAG systems, though foundational, often suffer from shallow query understanding and excessive noise due to redundant data retrieval. In contrast, Modular RAG offers a reconfigurable framework that decomposes complex processes into independent modules. This modularity facilitates enhancements in routing, scheduling, and fusion, crucial for handling diverse and intricate AI tasks [1].\n",
            "\n",
            "- **Key Features of Modular RAG:**\n",
            "  - **Independent Modules:** Each module focuses on specific tasks like retrieval, processing, or generation, allowing for targeted improvements and easier troubleshooting.\n",
            "  - **Advanced Design Integration:** Incorporates mechanisms for routing and scheduling, ensuring efficient data flow and reducing computational overhead.\n",
            "  - **Adaptability:** Easily reconfigurable to suit different application demands, enhancing versatility across various industries.\n",
            "\n",
            "#### Theoretical Underpinnings: Token-Level Harmonization\n",
            "\n",
            "A significant advancement in understanding RAG systems is the theoretical framework proposed for token-level harmonization. This model conceptualizes RAG as a fusion of the LLM's knowledge and retrieved texts, identifying the trade-offs between the benefits of external data (enhanced context and accuracy) and potential detriments (noise and misinformation). The theory underlines the importance of balancing these elements to optimize RAG performance without additional training or utility evaluators [2].\n",
            "\n",
            "- **Benefits of Token-Level Harmonization:**\n",
            "  - **Predictive Capability:** Allows for estimation of RAG effects on token prediction, enhancing reliability.\n",
            "  - **Data-Driven Improvements:** Supports data-driven approaches while providing a theoretical basis for understanding and mitigating detriments.\n",
            "\n",
            "#### Applications and Impact: From Agriculture to Healthcare\n",
            "\n",
            "The application of Modular RAG spans various sectors, each benefiting from its unique capability to integrate and utilize domain-specific knowledge effectively.\n",
            "\n",
            "- **Agriculture:** By incorporating location-specific data, Modular RAG systems have demonstrated a marked improvement in providing actionable insights to farmers, enhancing decision-making processes with geographic-specific knowledge [1].\n",
            "- **Biomedical Systems:** In healthcare, particularly for cancer patient question-answering systems, Modular RAG frameworks have been instrumental in improving accuracy and reliability by optimizing query pipelines and leveraging domain-specific databases [3].\n",
            "\n",
            "#### Challenges and Future Directions\n",
            "\n",
            "Despite its advantages, the implementation of Modular RAG systems is not without challenges. The complexity of designing and maintaining reconfigurable modules requires significant expertise and resources. Furthermore, the integration of real-time data and the need for continual updates and optimizations pose ongoing challenges.\n",
            "\n",
            "- **Future Research Directions:**\n",
            "  - **Enhanced Real-Time Data Integration:** Developing methods to incorporate dynamic data effectively into Modular RAG systems.\n",
            "  - **Scalability and Maintenance:** Exploring solutions to streamline the scalability of complex modular systems without compromising performance.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2401.08406v3  \n",
            "[2] http://arxiv.org/abs/2406.00944v2  \n",
            "[3] http://arxiv.org/abs/2407.21059v1  \n",
            "[4] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Scaling AI Models: The Case for Modular RAG Over Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "The evolution of Retrieval-Augmented Generation (RAG) systems has been a focal point for enhancing the scalability and performance of large language models (LLMs). Initially, Naive RAG models laid the groundwork by integrating document retrieval with language model generation, facilitating contextually aware responses. However, as demands for more efficient and adaptable AI systems have grown, the limitations of Naive RAG have become apparent. This report seeks to understand these limitations and explore how Modular RAG presents a more scalable alternative by breaking down complex systems into independent, reconfigurable modules.\n",
            "\n",
            "Key insights from various sources reveal several novel aspects of this evolution. The modular approach not only addresses the inefficiencies of traditional Naive RAG systems but also introduces innovative opportunities for customization and optimization, which are critical for scaling AI in production environments. The ability to modify and adapt different components of a RAG system without overhauling the entire architecture is particularly appealing for applications that require quick adaptability to new information or scenarios.\n",
            "\n",
            "Below is a list of the source documents analyzed, providing detailed insights into the scalability aspects of RAG systems:\n",
            "\n",
            "1. [Superteams Blog on Implementing Naive, Advanced, and Modular RAG](https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag)\n",
            "2. [Zilliz Blog on Advancing LLMs with RAG Approaches](https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches)\n",
            "3. [Medium Article on Naive, Advanced, and Modular RAG](https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e)\n",
            "4. [Arxiv Paper on Modular RAG: Transforming RAG Systems](http://arxiv.org/abs/2407.21059v1)\n",
            "5. [Arxiv Paper on RAG-Instruct](http://arxiv.org/abs/2501.00353v1)\n",
            "6. [Arxiv Paper on ARAGOG: Advanced RAG Output Grading](http://arxiv.org/abs/2404.01037v1)\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### The Naive RAG Foundation\n",
            "\n",
            "Naive RAG models represent the initial step in integrating retrieval mechanisms with language generation. These systems rely heavily on the semantic similarity between queries and document chunks to retrieve relevant information, which is then used to generate responses. While this approach provides a basic framework for context-aware response generation, it suffers from several limitations:\n",
            "\n",
            "- **Shallow Query Understanding**: Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics. This can lead to inaccurate or irrelevant retrievals, as the models may fail to capture deeper semantic relationships [1].\n",
            "  \n",
            "- **Retrieval Redundancy and Noise**: The indiscriminate feeding of retrieved data into LLMs can introduce noise, leading to erroneous outputs. This is particularly problematic when dealing with large volumes of unstructured data, as it increases computational overhead without necessarily improving output quality [4].\n",
            "\n",
            "#### Advancements with Modular RAG\n",
            "\n",
            "Modular RAG systems address these limitations by decomposing the RAG architecture into independent, customizable modules. This approach offers several advantages:\n",
            "\n",
            "- **Reconfigurability**: By allowing each component of the system—such as query generation, retrieval, and memory management—to be independently modified, Modular RAG systems can be tailored to specific tasks or domains. This flexibility is essential for scalable deployment in diverse environments [4][5].\n",
            "\n",
            "- **Integration of Advanced Techniques**: Modular RAG systems often incorporate sophisticated techniques such as dynamic embedding adjustment and routing mechanisms. These methods enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [2][3].\n",
            "\n",
            "- **Enhanced Scalability**: The modular nature of these systems facilitates easier scaling. As new data or requirements emerge, specific modules can be updated or replaced without affecting the entire system. This modularity is crucial for maintaining performance while reducing computational overhead in large-scale applications [3][6].\n",
            "\n",
            "#### Case Studies and Practical Implementations\n",
            "\n",
            "Several case studies highlight the effectiveness of Modular RAG systems:\n",
            "\n",
            "- **RAG-Instruct**: This approach leverages diverse instruction data to enhance LLM capabilities. By simulating various RAG scenarios, it provides robust performance across different tasks, demonstrating the potential of Modular RAG for scalable instruction synthesis [5].\n",
            "\n",
            "- **ARAGOG**: Focused on optimizing retrieval precision, ARAGOG employs advanced grading techniques to assess and improve RAG output quality. The study underscores the importance of modular approaches in achieving high retrieval accuracy and contextual relevance [6].\n",
            "\n",
            "#### Future Directions and Challenges\n",
            "\n",
            "While Modular RAG systems offer significant improvements over Naive RAG, several challenges remain:\n",
            "\n",
            "- **Complexity of Implementation**: The increased complexity of modular systems requires careful design and management to ensure that all components work harmoniously without introducing bottlenecks or inefficiencies [4].\n",
            "\n",
            "- **Balancing Flexibility and Performance**: While modular systems are inherently more flexible, ensuring that this flexibility does not come at the cost of performance is a critical consideration for developers [5].\n",
            "\n",
            "- **Continuous Evolution**: As AI and machine learning fields continue to evolve, Modular RAG systems must adapt to incorporate new technologies and methodologies. This ongoing evolution requires sustained research and development efforts [4].\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2501.00353v1  \n",
            "[6] http://arxiv.org/abs/2404.01037v1  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Scaling AI Models: The Case for Modular RAG Over Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "The evolution of Retrieval-Augmented Generation (RAG) systems has been a focal point for enhancing the scalability and performance of large language models (LLMs). Initially, Naive RAG models laid the groundwork by integrating document retrieval with language model generation, facilitating contextually aware responses. However, as demands for more efficient and adaptable AI systems have grown, the limitations of Naive RAG have become apparent. This report seeks to understand these limitations and explore how Modular RAG presents a more scalable alternative by breaking down complex systems into independent, reconfigurable modules.\n",
            "\n",
            "Key insights from various sources reveal several novel aspects of this evolution. The modular approach not only addresses the inefficiencies of traditional Naive RAG systems but also introduces innovative opportunities for customization and optimization, which are critical for scaling AI in production environments. The ability to modify and adapt different components of a RAG system without overhauling the entire architecture is particularly appealing for applications that require quick adaptability to new information or scenarios.\n",
            "\n",
            "Below is a list of the source documents analyzed, providing detailed insights into the scalability aspects of RAG systems:\n",
            "\n",
            "1. [Superteams Blog on Implementing Naive, Advanced, and Modular RAG](https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag)\n",
            "2. [Zilliz Blog on Advancing LLMs with RAG Approaches](https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches)\n",
            "3. [Medium Article on Naive, Advanced, and Modular RAG](https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e)\n",
            "4. [Arxiv Paper on Modular RAG: Transforming RAG Systems](http://arxiv.org/abs/2407.21059v1)\n",
            "5. [Arxiv Paper on RAG-Instruct](http://arxiv.org/abs/2501.00353v1)\n",
            "6. [Arxiv Paper on ARAGOG: Advanced RAG Output Grading](http://arxiv.org/abs/2404.01037v1)\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### The Naive RAG Foundation\n",
            "\n",
            "Naive RAG models represent the initial step in integrating retrieval mechanisms with language generation. These systems rely heavily on the semantic similarity between queries and document chunks to retrieve relevant information, which is then used to generate responses. While this approach provides a basic framework for context-aware response generation, it suffers from several limitations:\n",
            "\n",
            "- **Shallow Query Understanding**: Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics. This can lead to inaccurate or irrelevant retrievals, as the models may fail to capture deeper semantic relationships [1].\n",
            "  \n",
            "- **Retrieval Redundancy and Noise**: The indiscriminate feeding of retrieved data into LLMs can introduce noise, leading to erroneous outputs. This is particularly problematic when dealing with large volumes of unstructured data, as it increases computational overhead without necessarily improving output quality [4].\n",
            "\n",
            "#### Advancements with Modular RAG\n",
            "\n",
            "Modular RAG systems address these limitations by decomposing the RAG architecture into independent, customizable modules. This approach offers several advantages:\n",
            "\n",
            "- **Reconfigurability**: By allowing each component of the system—such as query generation, retrieval, and memory management—to be independently modified, Modular RAG systems can be tailored to specific tasks or domains. This flexibility is essential for scalable deployment in diverse environments [4][5].\n",
            "\n",
            "- **Integration of Advanced Techniques**: Modular RAG systems often incorporate sophisticated techniques such as dynamic embedding adjustment and routing mechanisms. These methods enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [2][3].\n",
            "\n",
            "- **Enhanced Scalability**: The modular nature of these systems facilitates easier scaling. As new data or requirements emerge, specific modules can be updated or replaced without affecting the entire system. This modularity is crucial for maintaining performance while reducing computational overhead in large-scale applications [3][6].\n",
            "\n",
            "#### Case Studies and Practical Implementations\n",
            "\n",
            "Several case studies highlight the effectiveness of Modular RAG systems:\n",
            "\n",
            "- **RAG-Instruct**: This approach leverages diverse instruction data to enhance LLM capabilities. By simulating various RAG scenarios, it provides robust performance across different tasks, demonstrating the potential of Modular RAG for scalable instruction synthesis [5].\n",
            "\n",
            "- **ARAGOG**: Focused on optimizing retrieval precision, ARAGOG employs advanced grading techniques to assess and improve RAG output quality. The study underscores the importance of modular approaches in achieving high retrieval accuracy and contextual relevance [6].\n",
            "\n",
            "#### Future Directions and Challenges\n",
            "\n",
            "While Modular RAG systems offer significant improvements over Naive RAG, several challenges remain:\n",
            "\n",
            "- **Complexity of Implementation**: The increased complexity of modular systems requires careful design and management to ensure that all components work harmoniously without introducing bottlenecks or inefficiencies [4].\n",
            "\n",
            "- **Balancing Flexibility and Performance**: While modular systems are inherently more flexible, ensuring that this flexibility does not come at the cost of performance is a critical consideration for developers [5].\n",
            "\n",
            "- **Continuous Evolution**: As AI and machine learning fields continue to evolve, Modular RAG systems must adapt to incorporate new technologies and methodologies. This ongoing evolution requires sustained research and development efforts [4].\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://medium.com/@anixlynch/naive-rag-advanced-rag-modular-rag-b18b8669193e  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2501.00353v1  \n",
            "[6] http://arxiv.org/abs/2404.01037v1  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Optimizing AI Architecture: The Efficiency of Modular RAG Over Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the realm of AI architecture, particularly concerning the efficiency and optimization of Retrieval-Augmented Generation (RAG) systems, Modular RAG has emerged as a significant advancement over its predecessor, Naive RAG. This report delves into various studies and expert analyses to elucidate how Modular RAG improves efficiency, resource management, and scalability, addressing the challenges previously encountered with Naive RAG.\n",
            "\n",
            "The core of RAG systems integrates document retrieval with language model generation, aiming to enhance the accuracy and contextual relevance of AI-generated responses. Naive RAG, while foundational, has been limited by its linear process and inefficiencies in handling complex queries or dynamically updating information. Modular RAG offers a more sophisticated, reconfigurable framework, overcoming these limitations through the use of specialized modules and enhanced retrieval mechanisms.\n",
            "\n",
            "Key insights from the analyzed sources reveal several novel and intriguing developments:\n",
            "\n",
            "1. **Graph-Based Enhancements:** A study implements an advanced RAG system using Graph technology, which improves the retrieval and synthesis of diverse data, leading to more accurate and contextually enriched responses [1].\n",
            "   \n",
            "2. **Four-Module Synergy:** Another research introduces a four-module synergy that significantly boosts the quality and efficiency of RAG systems, addressing issues such as redundant retrieval and irrelevant information [2].\n",
            "   \n",
            "3. **Edge-Assisted Architectures:** Edge-assisted RAG systems like EACO-RAG demonstrate how distributing processing loads across network nodes can reduce latency and improve resource utilization, making them more scalable and cost-effective [3].\n",
            "   \n",
            "4. **Modular Framework Advantages:** Modular RAG's decomposition into independent modules facilitates a LEGO-like reconfigurable system, addressing the complexity and integration challenges of contemporary RAG systems [4].\n",
            "\n",
            "These insights underscore Modular RAG's potential to transform AI applications by enhancing scalability, adaptability, and efficiency. The detailed exploration and validation across various datasets and real-world applications highlight its practical relevance and superior performance compared to Naive RAG.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Graph-Based RAG Systems\n",
            "\n",
            "The study by Jeong [1] outlines an innovative approach by integrating Graph technology into RAG systems, termed as an advanced Agent-Based RAG. This system addresses the shortcomings of Naive RAG—primarily its inability to update real-time data post-configuration and its reliance on pre-loaded, sometimes outdated, information. By employing LangGraph, this advanced system can evaluate the reliability of retrieved data more effectively, synthesizing it to produce accurate and contextually relevant responses. This methodology not only enhances the accuracy but also provides practical guidelines for corporate implementations, marking a significant improvement over traditional RAG systems.\n",
            "\n",
            "#### Modular Synergy in RAG Systems\n",
            "\n",
            "The research by Shi et al. [2] introduces a four-module strategy to optimize RAG efficiency and quality: Query Rewriter+, Knowledge Filter, Memory Knowledge Reservoir, and Retriever Trigger. These modules address critical issues such as redundant retrievals and irrelevant knowledge, which are prevalent in Naive RAG systems. The Query Rewriter+ module generates multiple search queries, overcoming the limitations of single-query systems by enhancing the retrieval quality. The Knowledge Filter removes irrelevant information, ensuring that the system remains focused on pertinent data. Together, these modules enhance modular RAG's capability to dynamically expand its knowledge base and optimize resource use, pivotal for large-scale deployments.\n",
            "\n",
            "#### Edge-Assisted RAG Systems\n",
            "\n",
            "Li and colleagues [3] explore the EACO-RAG system, which leverages edge computing to improve RAG scalability. This system distributes vector datasets across edge nodes, optimizing retrieval processes and reducing communication overhead. EACO-RAG employs a multi-armed bandit framework to balance performance and cost efficiently. By significantly reducing delay and resource consumption while enhancing response accuracy, this edge-assisted approach outperforms centralized RAG systems, effectively addressing the scalability challenges faced by Naive RAG systems.\n",
            "\n",
            "#### Modular RAG Framework\n",
            "\n",
            "Gao et al. [4] present a comprehensive examination of Modular RAG, highlighting its transformation of RAG systems into highly reconfigurable frameworks. By decomposing RAG systems into independent modules, Modular RAG offers flexibility and adaptability, akin to LEGO blocks. This enables the integration of advanced retrievers and complementary technologies, facilitating a more robust and efficient system. The study identifies prevalent RAG patterns, such as linear and conditional pathways, and explores their implementation nuances, offering a roadmap for future RAG system development.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2407.19994v3  \n",
            "[2] http://arxiv.org/abs/2407.10670v1  \n",
            "[3] http://arxiv.org/abs/2410.20299v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[6] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[7] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Optimizing AI Architecture: The Efficiency of Modular RAG Over Naive RAG\n",
            "\n",
            "### Summary\n",
            "\n",
            "In the realm of AI architecture, particularly concerning the efficiency and optimization of Retrieval-Augmented Generation (RAG) systems, Modular RAG has emerged as a significant advancement over its predecessor, Naive RAG. This report delves into various studies and expert analyses to elucidate how Modular RAG improves efficiency, resource management, and scalability, addressing the challenges previously encountered with Naive RAG.\n",
            "\n",
            "The core of RAG systems integrates document retrieval with language model generation, aiming to enhance the accuracy and contextual relevance of AI-generated responses. Naive RAG, while foundational, has been limited by its linear process and inefficiencies in handling complex queries or dynamically updating information. Modular RAG offers a more sophisticated, reconfigurable framework, overcoming these limitations through the use of specialized modules and enhanced retrieval mechanisms.\n",
            "\n",
            "Key insights from the analyzed sources reveal several novel and intriguing developments:\n",
            "\n",
            "1. **Graph-Based Enhancements:** A study implements an advanced RAG system using Graph technology, which improves the retrieval and synthesis of diverse data, leading to more accurate and contextually enriched responses [1].\n",
            "   \n",
            "2. **Four-Module Synergy:** Another research introduces a four-module synergy that significantly boosts the quality and efficiency of RAG systems, addressing issues such as redundant retrieval and irrelevant information [2].\n",
            "   \n",
            "3. **Edge-Assisted Architectures:** Edge-assisted RAG systems like EACO-RAG demonstrate how distributing processing loads across network nodes can reduce latency and improve resource utilization, making them more scalable and cost-effective [3].\n",
            "   \n",
            "4. **Modular Framework Advantages:** Modular RAG's decomposition into independent modules facilitates a LEGO-like reconfigurable system, addressing the complexity and integration challenges of contemporary RAG systems [4].\n",
            "\n",
            "These insights underscore Modular RAG's potential to transform AI applications by enhancing scalability, adaptability, and efficiency. The detailed exploration and validation across various datasets and real-world applications highlight its practical relevance and superior performance compared to Naive RAG.\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Graph-Based RAG Systems\n",
            "\n",
            "The study by Jeong [1] outlines an innovative approach by integrating Graph technology into RAG systems, termed as an advanced Agent-Based RAG. This system addresses the shortcomings of Naive RAG—primarily its inability to update real-time data post-configuration and its reliance on pre-loaded, sometimes outdated, information. By employing LangGraph, this advanced system can evaluate the reliability of retrieved data more effectively, synthesizing it to produce accurate and contextually relevant responses. This methodology not only enhances the accuracy but also provides practical guidelines for corporate implementations, marking a significant improvement over traditional RAG systems.\n",
            "\n",
            "#### Modular Synergy in RAG Systems\n",
            "\n",
            "The research by Shi et al. [2] introduces a four-module strategy to optimize RAG efficiency and quality: Query Rewriter+, Knowledge Filter, Memory Knowledge Reservoir, and Retriever Trigger. These modules address critical issues such as redundant retrievals and irrelevant knowledge, which are prevalent in Naive RAG systems. The Query Rewriter+ module generates multiple search queries, overcoming the limitations of single-query systems by enhancing the retrieval quality. The Knowledge Filter removes irrelevant information, ensuring that the system remains focused on pertinent data. Together, these modules enhance modular RAG's capability to dynamically expand its knowledge base and optimize resource use, pivotal for large-scale deployments.\n",
            "\n",
            "#### Edge-Assisted RAG Systems\n",
            "\n",
            "Li and colleagues [3] explore the EACO-RAG system, which leverages edge computing to improve RAG scalability. This system distributes vector datasets across edge nodes, optimizing retrieval processes and reducing communication overhead. EACO-RAG employs a multi-armed bandit framework to balance performance and cost efficiently. By significantly reducing delay and resource consumption while enhancing response accuracy, this edge-assisted approach outperforms centralized RAG systems, effectively addressing the scalability challenges faced by Naive RAG systems.\n",
            "\n",
            "#### Modular RAG Framework\n",
            "\n",
            "Gao et al. [4] present a comprehensive examination of Modular RAG, highlighting its transformation of RAG systems into highly reconfigurable frameworks. By decomposing RAG systems into independent modules, Modular RAG offers flexibility and adaptability, akin to LEGO blocks. This enables the integration of advanced retrievers and complementary technologies, facilitating a more robust and efficient system. The study identifies prevalent RAG patterns, such as linear and conditional pathways, and explores their implementation nuances, offering a roadmap for future RAG system development.\n",
            "\n",
            "### Sources\n",
            "\n",
            "[1] http://arxiv.org/abs/2407.19994v3  \n",
            "[2] http://arxiv.org/abs/2407.10670v1  \n",
            "[3] http://arxiv.org/abs/2410.20299v1  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[6] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[7] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Enhancing AI Systems: The Modular RAG Advantage\n",
            "\n",
            "### Summary\n",
            "\n",
            "The ongoing evolution of AI models, particularly in the realm of Retrieval-Augmented Generation (RAG), is pivotal for advancing the capabilities of Large Language Models (LLMs). RAG systems integrate real-time data retrieval with language generation processes, addressing the limitations of static datasets traditionally used by LLMs. This integration is crucial for maintaining contextual relevance and accuracy in AI outputs, especially in dynamic environments. Notably, the Modular RAG approach has emerged as a superior alternative to traditional Naive RAG systems, offering enhanced efficiency and scalability for production-level applications [1][2][3].\n",
            "\n",
            "A key insight from recent analyses is that Naive RAG systems, while foundational, often suffer from inflexibility and inefficiencies when handling diverse datasets [1]. They typically utilize a single-agent architecture for query generation, data retrieval, and response synthesis, which can lead to performance bottlenecks and reduced accuracy [4]. In contrast, Modular RAG systems leverage a distributed multi-agent framework. This approach involves specialized agents optimized for specific data sources, such as relational databases and document stores, each focusing on its task within a modular framework [4]. This specialization and modularity significantly enhance query efficiency, reduce processing overhead, and improve response accuracy.\n",
            "\n",
            "Moreover, the advancement of Modular RAG includes the integration of autonomous AI agents, which manage retrieval strategies dynamically and adapt workflows to meet complex task requirements [5]. These agents employ agentic design patterns, such as reflection and planning, to iteratively refine contextual understanding [5]. This ensures more context-aware and accurate outputs across various applications, including healthcare, finance, and education.\n",
            "\n",
            "The comparative analysis of Naive and Modular RAG systems highlights the transformative potential of the latter in industrial applications, particularly where real-time data integration and adaptability are critical. This shift towards a multi-agent, modular approach not only enhances scalability but also addresses ethical AI challenges by optimizing performance for real-world applications [5].\n",
            "\n",
            "**Source Documents:**\n",
            "1. Superteams Blog on Naive and Modular RAG [1]\n",
            "2. Zilliz Blog on Advanced RAG Approaches [2]\n",
            "3. Adasci Article on Modular RAG Improvements [3]\n",
            "4. ArXiv Paper on Multi-Agent RAG Systems [4]\n",
            "5. ArXiv Paper on Agentic RAG Survey [5]\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Understanding Naive RAG Systems\n",
            "\n",
            "Naive RAG systems serve as a foundational paradigm in the integration of retrieval mechanisms with generative language models. These systems primarily employ retrieval models that rank indexed data based on its relevance to the input query. The generative component then synthesizes responses using the retrieved context, which is essential for producing coherent and contextually relevant outputs [1]. This paradigm is particularly effective for basic question-answering tasks, where the system must accurately incorporate relevant points from retrieved documents into the generated answers [2].\n",
            "\n",
            "However, the limitations of Naive RAG are significant. The reliance on a single-agent architecture can lead to inefficiencies, especially when dealing with diverse datasets. Such systems often face bottlenecks due to their monolithic design, which consolidates query generation, data retrieval, and response synthesis into a single process. This inflexibility diminishes their effectiveness in dynamic environments where data sources and types vary considerably [3][4].\n",
            "\n",
            "#### Advancements with Modular RAG\n",
            "\n",
            "Modular RAG represents a significant leap forward, addressing the limitations of traditional Naive RAG systems. By adopting a multi-agent architecture, Modular RAG systems distribute tasks across specialized agents, each optimized for specific data sources. This modularity allows for more granular control over the RAG process, enabling systems to fine-tune operations based on task requirements [4]. For example, a Modular RAG framework can begin with a query expansion module to refine user inputs, followed by a retrieval module that fetches relevant data chunks [6].\n",
            "\n",
            "The introduction of autonomous AI agents within the Modular RAG framework further enhances its capabilities. These agents leverage agentic design patterns to dynamically manage retrieval strategies and adapt workflows. This dynamic adaptability is crucial for applications requiring real-time data integration and complex task management [5]. By iterating on contextual understanding and refining retrieval strategies, Modular RAG systems deliver more accurate and context-aware responses, making them ideal for industrial applications such as healthcare and finance [5].\n",
            "\n",
            "#### Applications and Implications\n",
            "\n",
            "The scalability and adaptability of Modular RAG systems make them particularly suited for production-level applications where efficiency and precision are paramount. For instance, in the healthcare sector, Modular RAG can be used to integrate real-time patient data with medical literature, providing healthcare professionals with up-to-date and contextually relevant information [5]. Similarly, in finance, these systems can dynamically incorporate market data into financial analyses and predictions, enhancing decision-making processes [5].\n",
            "\n",
            "Moreover, the modular nature of these systems allows for continuous improvement and customization. Developers can replace or optimize individual modules without disrupting the overall system flow, ensuring that the RAG framework remains robust and adaptable to evolving data demands [6]. This flexibility also facilitates compliance with ethical AI standards by ensuring that systems can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
            "\n",
            "#### Challenges and Future Directions\n",
            "\n",
            "Despite the advantages of Modular RAG, challenges remain in ensuring seamless integration across diverse data types and maintaining system efficiency. Sophisticated data processing and retrieval strategies are required to manage the complexity of modular interactions and ensure that each component operates harmoniously within the system [6]. Additionally, as these systems become more prevalent, issues related to ethical decision-making and the potential for bias in AI outputs must be addressed to ensure fairness and transparency in AI applications [5].\n",
            "\n",
            "Looking forward, the future of AI systems lies in enhancing the modularity and adaptability of RAG frameworks. Continued research into multi-agent architectures and the development of more sophisticated agentic patterns will be crucial for overcoming current limitations and unlocking the full potential of retrieval-augmented AI systems.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[4] http://arxiv.org/abs/2412.05838v1  \n",
            "[5] http://arxiv.org/abs/2501.09136v1  \n",
            "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "## Enhancing AI Systems: The Modular RAG Advantage\n",
            "\n",
            "### Summary\n",
            "\n",
            "The ongoing evolution of AI models, particularly in the realm of Retrieval-Augmented Generation (RAG), is pivotal for advancing the capabilities of Large Language Models (LLMs). RAG systems integrate real-time data retrieval with language generation processes, addressing the limitations of static datasets traditionally used by LLMs. This integration is crucial for maintaining contextual relevance and accuracy in AI outputs, especially in dynamic environments. Notably, the Modular RAG approach has emerged as a superior alternative to traditional Naive RAG systems, offering enhanced efficiency and scalability for production-level applications [1][2][3].\n",
            "\n",
            "A key insight from recent analyses is that Naive RAG systems, while foundational, often suffer from inflexibility and inefficiencies when handling diverse datasets [1]. They typically utilize a single-agent architecture for query generation, data retrieval, and response synthesis, which can lead to performance bottlenecks and reduced accuracy [4]. In contrast, Modular RAG systems leverage a distributed multi-agent framework. This approach involves specialized agents optimized for specific data sources, such as relational databases and document stores, each focusing on its task within a modular framework [4]. This specialization and modularity significantly enhance query efficiency, reduce processing overhead, and improve response accuracy.\n",
            "\n",
            "Moreover, the advancement of Modular RAG includes the integration of autonomous AI agents, which manage retrieval strategies dynamically and adapt workflows to meet complex task requirements [5]. These agents employ agentic design patterns, such as reflection and planning, to iteratively refine contextual understanding [5]. This ensures more context-aware and accurate outputs across various applications, including healthcare, finance, and education.\n",
            "\n",
            "The comparative analysis of Naive and Modular RAG systems highlights the transformative potential of the latter in industrial applications, particularly where real-time data integration and adaptability are critical. This shift towards a multi-agent, modular approach not only enhances scalability but also addresses ethical AI challenges by optimizing performance for real-world applications [5].\n",
            "\n",
            "**Source Documents:**\n",
            "1. Superteams Blog on Naive and Modular RAG [1]\n",
            "2. Zilliz Blog on Advanced RAG Approaches [2]\n",
            "3. Adasci Article on Modular RAG Improvements [3]\n",
            "4. ArXiv Paper on Multi-Agent RAG Systems [4]\n",
            "5. ArXiv Paper on Agentic RAG Survey [5]\n",
            "\n",
            "### Comprehensive Analysis\n",
            "\n",
            "#### Understanding Naive RAG Systems\n",
            "\n",
            "Naive RAG systems serve as a foundational paradigm in the integration of retrieval mechanisms with generative language models. These systems primarily employ retrieval models that rank indexed data based on its relevance to the input query. The generative component then synthesizes responses using the retrieved context, which is essential for producing coherent and contextually relevant outputs [1]. This paradigm is particularly effective for basic question-answering tasks, where the system must accurately incorporate relevant points from retrieved documents into the generated answers [2].\n",
            "\n",
            "However, the limitations of Naive RAG are significant. The reliance on a single-agent architecture can lead to inefficiencies, especially when dealing with diverse datasets. Such systems often face bottlenecks due to their monolithic design, which consolidates query generation, data retrieval, and response synthesis into a single process. This inflexibility diminishes their effectiveness in dynamic environments where data sources and types vary considerably [3][4].\n",
            "\n",
            "#### Advancements with Modular RAG\n",
            "\n",
            "Modular RAG represents a significant leap forward, addressing the limitations of traditional Naive RAG systems. By adopting a multi-agent architecture, Modular RAG systems distribute tasks across specialized agents, each optimized for specific data sources. This modularity allows for more granular control over the RAG process, enabling systems to fine-tune operations based on task requirements [4]. For example, a Modular RAG framework can begin with a query expansion module to refine user inputs, followed by a retrieval module that fetches relevant data chunks [6].\n",
            "\n",
            "The introduction of autonomous AI agents within the Modular RAG framework further enhances its capabilities. These agents leverage agentic design patterns to dynamically manage retrieval strategies and adapt workflows. This dynamic adaptability is crucial for applications requiring real-time data integration and complex task management [5]. By iterating on contextual understanding and refining retrieval strategies, Modular RAG systems deliver more accurate and context-aware responses, making them ideal for industrial applications such as healthcare and finance [5].\n",
            "\n",
            "#### Applications and Implications\n",
            "\n",
            "The scalability and adaptability of Modular RAG systems make them particularly suited for production-level applications where efficiency and precision are paramount. For instance, in the healthcare sector, Modular RAG can be used to integrate real-time patient data with medical literature, providing healthcare professionals with up-to-date and contextually relevant information [5]. Similarly, in finance, these systems can dynamically incorporate market data into financial analyses and predictions, enhancing decision-making processes [5].\n",
            "\n",
            "Moreover, the modular nature of these systems allows for continuous improvement and customization. Developers can replace or optimize individual modules without disrupting the overall system flow, ensuring that the RAG framework remains robust and adaptable to evolving data demands [6]. This flexibility also facilitates compliance with ethical AI standards by ensuring that systems can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
            "\n",
            "#### Challenges and Future Directions\n",
            "\n",
            "Despite the advantages of Modular RAG, challenges remain in ensuring seamless integration across diverse data types and maintaining system efficiency. Sophisticated data processing and retrieval strategies are required to manage the complexity of modular interactions and ensure that each component operates harmoniously within the system [6]. Additionally, as these systems become more prevalent, issues related to ethical decision-making and the potential for bias in AI outputs must be addressed to ensure fairness and transparency in AI applications [5].\n",
            "\n",
            "Looking forward, the future of AI systems lies in enhancing the modularity and adaptability of RAG frameworks. Continued research into multi-agent architectures and the development of more sophisticated agentic patterns will be crucial for overcoming current limitations and unlocking the full potential of retrieval-augmented AI systems.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[4] http://arxiv.org/abs/2412.05838v1  \n",
            "[5] http://arxiv.org/abs/2501.09136v1  \n",
            "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_introduction\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mintroduction\u001b[0m:\n",
            "# Enhancing AI Systems: The Modular RAG Advantage\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In the dynamic landscape of artificial intelligence, the evolution of Retrieval-Augmented Generation (RAG) systems marks a pivotal advancement in the capabilities of Large Language Models (LLMs). Traditional Naive RAG systems laid the groundwork by integrating document retrieval with generative processes, facilitating the generation of contextually relevant outputs. However, as technological demands have grown, the limitations of Naive RAG—such as inefficiencies and a lack of adaptability—have become apparent. This report delves into the innovative Modular RAG approach, which offers a more efficient and scalable solution by employing a distributed multi-agent framework. This modular architecture allows for specialized agents optimized for specific data sources, enhancing query efficiency and improving response accuracy.\n",
            "\n",
            "The analysis begins by examining the foundational aspects of Naive RAG systems, highlighting their strengths and inherent limitations. It then transitions into the advancements brought forth by Modular RAG, emphasizing its multi-agent design and the integration of autonomous AI agents that dynamically manage retrieval strategies. The report further explores the practical applications of Modular RAG in various industries, such as healthcare and finance, showcasing its scalability and adaptability at the production level. Finally, it addresses the challenges and future directions for Modular RAG systems, ensuring they meet ethical AI standards while continuing to evolve and integrate new technologies.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_conclusion\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mconclusion\u001b[0m:\n",
            "## Conclusion\n",
            "\n",
            "In the rapidly evolving landscape of AI technology, the shift from Naive RAG to Modular RAG frameworks marks a significant advancement in optimizing the performance and scalability of Retrieval-Augmented Generation systems. This report has thoroughly examined the comparative benefits of Modular RAG, underscoring its ability to address the inefficiencies inherent in Naive RAG systems. Naive RAG’s foundational architecture, while pivotal, struggles with inflexibility and processing bottlenecks, limiting its effectiveness in dynamic environments.\n",
            "\n",
            "Modular RAG introduces a multi-agent, distributed framework that enhances efficiency through specialized agents optimized for diverse data sources. This modularity allows for refined query processes, reduced overhead, and improved accuracy, as each agent focuses on specific tasks within the framework. The integration of autonomous AI agents further advances this system by dynamically adapting retrieval strategies to meet complex task requirements, thereby ensuring more context-aware and accurate outputs.\n",
            "\n",
            "Applications across various industries, such as healthcare and finance, demonstrate Modular RAG's scalability and adaptability in production environments, offering real-time data integration and enhanced decision-making capabilities. However, the challenges of modular interactions and ethical considerations remain areas for future exploration. Continued research and development will be essential to fully realize the potential of Modular RAG systems, paving the way for more robust and efficient AI applications that can adapt to the ever-changing demands of real-world scenarios.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mwrite_report\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mcontent\u001b[0m:\n",
            "## Insights\n",
            "\n",
            "### Background\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) systems are pivotal in enhancing Large Language Models (LLMs) by integrating real-time data retrieval with language generation. Traditionally, Naive RAG systems have laid the groundwork by providing a basic framework that uses retrieval mechanisms to rank the relevance of indexed data to an input query, followed by a generative component that synthesizes responses. This method, effective for simple question-answering tasks, ensures that AI-generated outputs are coherent and contextually relevant. However, the Naive RAG model's reliance on a single-agent architecture presents significant limitations, particularly in handling diverse datasets and dynamic environments, leading to performance bottlenecks and reduced accuracy [1][3][5].\n",
            "\n",
            "The advent of Modular RAG systems marks a significant evolution in this domain. Unlike their Naive counterparts, Modular RAG systems employ a distributed multi-agent framework, where tasks are divided among specialized agents optimized for specific data sources, such as relational databases and document stores. This modularity allows for enhanced efficiency, scalability, and flexibility, making these systems well-suited for production-level applications [2][4]. By integrating autonomous AI agents that dynamically manage retrieval strategies and adapt workflows, Modular RAG systems achieve superior context-aware and accurate outputs across various fields, including healthcare, finance, and education [5].\n",
            "\n",
            "Overall, the shift from Naive to Modular RAG systems represents a transformative development in AI, offering a robust solution for integrating LLMs into diverse applications, thereby paving the way for more precise, efficient, and reliable AI-driven insights [6].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "The development of Modular RAG systems can be traced back to the foundational Naive RAG models, which established the basic integration of retrieval mechanisms with language generation processes. These systems, however, face challenges related to shallow query understanding and retrieval redundancy, leading to inefficiencies in data processing and increased noise in outputs [1][2]. Over time, advancements in RAG systems have focused on overcoming these limitations through innovative approaches such as modular architecture, dynamic retrieval strategies, and enhanced data integration techniques.\n",
            "\n",
            "Recent studies highlight the transformative potential of Modular RAG systems in various applications. For instance, several research efforts have explored the introduction of token-level harmonization models, which balance the benefits of external data integration against the potential drawbacks of noise and misinformation [2]. Moreover, practical implementations of Modular RAG have demonstrated significant improvements in accuracy and contextual understanding, particularly in knowledge-intensive tasks and industrial settings [4].\n",
            "\n",
            "Additionally, the use of sophisticated techniques such as dynamic embedding adjustment and routing mechanisms has been shown to enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [3][5]. Case studies in fields such as agriculture and healthcare further underscore the practical advantages of Modular RAG systems, showcasing their ability to integrate domain-specific knowledge effectively and provide actionable insights [1][3].\n",
            "\n",
            "These advancements suggest a promising future for Modular RAG systems, as they continue to evolve and integrate new technologies to address the growing demands of AI applications.\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by the evolution from Naive to Modular RAG systems is the inefficiency and inflexibility inherent in the monolithic design of traditional RAG models. Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics, leading to inaccurate or irrelevant retrievals. This limitation is exacerbated by the indiscriminate feeding of retrieved data into language models, which can introduce noise and increase computational overhead without necessarily enhancing output quality [1][3].\n",
            "\n",
            "In dynamic environments where data sources and types vary significantly, the single-agent architecture of Naive RAG systems becomes a bottleneck, hindering their effectiveness and scalability. This issue is particularly critical in production-level applications that require real-time data integration and adaptability to new information or scenarios. The need for a more flexible, efficient, and scalable solution is evident across various domains, including healthcare, finance, and education, where precision and contextual relevance are paramount [2][5].\n",
            "\n",
            "To address these challenges, a shift towards a Modular RAG framework is proposed. By decomposing the RAG architecture into independent, reconfigurable modules, this approach aims to enhance query efficiency, reduce processing overhead, and improve response accuracy. The introduction of autonomous AI agents within this modular framework further contributes to dynamic adaptability, ensuring that retrieval strategies and workflows are optimized to meet the complex requirements of diverse tasks [4][5].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "The methodological advancements in Modular RAG systems focus on transforming the traditional RAG architecture into a more flexible and scalable framework. This transformation is achieved through the decomposition of the RAG system into independent modules, each responsible for specific tasks such as query generation, data retrieval, and response synthesis. This modular approach allows for targeted improvements and customization, enabling the system to adapt to specific tasks or domains without requiring an overhaul of the entire architecture [4][6].\n",
            "\n",
            "A key aspect of this methodology is the integration of autonomous AI agents that employ agentic design patterns, such as reflection and planning, to dynamically manage retrieval strategies and adapt workflows. These agents iteratively refine contextual understanding, ensuring more accurate and context-aware responses across various applications [5]. Additionally, the use of advanced techniques such as dynamic embedding adjustment and routing mechanisms enhances the precision and relevance of the retrieved information, further improving the quality of generated responses [3][5].\n",
            "\n",
            "The implementation of token-level harmonization models provides a theoretical framework for balancing the benefits of external data integration with the potential drawbacks of noise and misinformation. This model allows for estimation of RAG effects on token prediction, supporting data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Overall, the methodology behind Modular RAG systems represents a significant leap forward, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "Implementing Modular RAG systems involves several key components and configurations that distinguish them from traditional Naive RAG models. The modular architecture is designed to facilitate reconfigurability and scalability, enabling each component to be independently modified or replaced as needed. This flexibility is essential for maintaining performance and adapting to new data or requirements in large-scale applications [4][6].\n",
            "\n",
            "The deployment of autonomous AI agents within the Modular RAG framework is a critical aspect of its implementation. These agents are responsible for managing retrieval strategies and adapting workflows dynamically, leveraging agentic design patterns to ensure optimal performance across various tasks. The integration of advanced techniques such as dynamic embedding adjustment and routing mechanisms further enhances the system's efficiency and accuracy [5].\n",
            "\n",
            "Additionally, the implementation of token-level harmonization models provides a theoretical basis for balancing external data integration with the potential drawbacks of noise and misinformation. This model supports data-driven improvements while minimizing detriments, ensuring that the system remains robust and reliable [2].\n",
            "\n",
            "Practical implementations of Modular RAG systems have demonstrated their effectiveness in various domains, including healthcare, finance, and education. By integrating domain-specific knowledge and optimizing query pipelines, these systems provide actionable insights and enhance decision-making processes across diverse applications [1][3]. The modular nature of these systems also facilitates compliance with ethical AI standards, ensuring that they can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "The experimental validation of Modular RAG systems involves a comprehensive evaluation of their performance across various datasets and applications. These experiments are designed to assess the effectiveness of the modular architecture and the integration of autonomous AI agents in enhancing query efficiency, response accuracy, and overall scalability [4][6].\n",
            "\n",
            "A key focus of these experiments is the comparison of Modular RAG systems with traditional Naive RAG models. By evaluating the systems' ability to handle complex queries, integrate real-time data, and adapt to diverse environments, researchers can quantify the improvements offered by the modular approach. Metrics such as retrieval precision, response accuracy, and computational overhead are commonly used to assess performance [1][3].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional insights into the balance between external data integration and potential drawbacks such as noise and misinformation. These models are evaluated for their predictive capability and their ability to support data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Case studies in fields such as agriculture and healthcare further demonstrate the practical advantages of Modular RAG systems. By incorporating domain-specific knowledge and leveraging advanced retrieval strategies, these systems provide actionable insights and enhance decision-making processes in real-world applications [1][3]. The results of these experiments underscore the potential of Modular RAG systems to transform AI applications by enhancing scalability, adaptability, and efficiency.\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of experiments and case studies on Modular RAG systems consistently demonstrate their superiority over traditional Naive RAG models in terms of efficiency, accuracy, and scalability. By decomposing the RAG architecture into independent modules, Modular RAG systems achieve significant improvements in query efficiency and response accuracy, reducing processing overhead and enhancing overall performance [4][6].\n",
            "\n",
            "The integration of autonomous AI agents within the Modular RAG framework further contributes to these improvements. By dynamically managing retrieval strategies and adapting workflows, these agents ensure that the system remains context-aware and responsive to complex task requirements. This adaptability is particularly beneficial in applications that require real-time data integration and precision, such as healthcare, finance, and education [5].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional support for these findings, highlighting the balance between external data integration and potential drawbacks such as noise and misinformation. These models enhance the reliability and accuracy of the system's outputs, supporting data-driven improvements without compromising performance [2].\n",
            "\n",
            "Case studies in various domains further validate the practical advantages of Modular RAG systems. In agriculture, these systems provide geographic-specific insights, enhancing decision-making processes for farmers. In healthcare, they improve the accuracy and reliability of question-answering systems for cancer patients, optimizing query pipelines and leveraging domain-specific databases [1][3].\n",
            "\n",
            "Overall, the results of these studies underscore the transformative potential of Modular RAG systems, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2501.09136v1  \n",
            "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372  \n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mfinalize_report\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mfinal_report\u001b[0m:\n",
            "# Enhancing AI Systems: The Modular RAG Advantage\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In the dynamic landscape of artificial intelligence, the evolution of Retrieval-Augmented Generation (RAG) systems marks a pivotal advancement in the capabilities of Large Language Models (LLMs). Traditional Naive RAG systems laid the groundwork by integrating document retrieval with generative processes, facilitating the generation of contextually relevant outputs. However, as technological demands have grown, the limitations of Naive RAG—such as inefficiencies and a lack of adaptability—have become apparent. This report delves into the innovative Modular RAG approach, which offers a more efficient and scalable solution by employing a distributed multi-agent framework. This modular architecture allows for specialized agents optimized for specific data sources, enhancing query efficiency and improving response accuracy.\n",
            "\n",
            "The analysis begins by examining the foundational aspects of Naive RAG systems, highlighting their strengths and inherent limitations. It then transitions into the advancements brought forth by Modular RAG, emphasizing its multi-agent design and the integration of autonomous AI agents that dynamically manage retrieval strategies. The report further explores the practical applications of Modular RAG in various industries, such as healthcare and finance, showcasing its scalability and adaptability at the production level. Finally, it addresses the challenges and future directions for Modular RAG systems, ensuring they meet ethical AI standards while continuing to evolve and integrate new technologies.\n",
            "\n",
            "---\n",
            "\n",
            "## Main Idea\n",
            "\n",
            "\n",
            "\n",
            "### Background\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) systems are pivotal in enhancing Large Language Models (LLMs) by integrating real-time data retrieval with language generation. Traditionally, Naive RAG systems have laid the groundwork by providing a basic framework that uses retrieval mechanisms to rank the relevance of indexed data to an input query, followed by a generative component that synthesizes responses. This method, effective for simple question-answering tasks, ensures that AI-generated outputs are coherent and contextually relevant. However, the Naive RAG model's reliance on a single-agent architecture presents significant limitations, particularly in handling diverse datasets and dynamic environments, leading to performance bottlenecks and reduced accuracy [1][3][5].\n",
            "\n",
            "The advent of Modular RAG systems marks a significant evolution in this domain. Unlike their Naive counterparts, Modular RAG systems employ a distributed multi-agent framework, where tasks are divided among specialized agents optimized for specific data sources, such as relational databases and document stores. This modularity allows for enhanced efficiency, scalability, and flexibility, making these systems well-suited for production-level applications [2][4]. By integrating autonomous AI agents that dynamically manage retrieval strategies and adapt workflows, Modular RAG systems achieve superior context-aware and accurate outputs across various fields, including healthcare, finance, and education [5].\n",
            "\n",
            "Overall, the shift from Naive to Modular RAG systems represents a transformative development in AI, offering a robust solution for integrating LLMs into diverse applications, thereby paving the way for more precise, efficient, and reliable AI-driven insights [6].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "The development of Modular RAG systems can be traced back to the foundational Naive RAG models, which established the basic integration of retrieval mechanisms with language generation processes. These systems, however, face challenges related to shallow query understanding and retrieval redundancy, leading to inefficiencies in data processing and increased noise in outputs [1][2]. Over time, advancements in RAG systems have focused on overcoming these limitations through innovative approaches such as modular architecture, dynamic retrieval strategies, and enhanced data integration techniques.\n",
            "\n",
            "Recent studies highlight the transformative potential of Modular RAG systems in various applications. For instance, several research efforts have explored the introduction of token-level harmonization models, which balance the benefits of external data integration against the potential drawbacks of noise and misinformation [2]. Moreover, practical implementations of Modular RAG have demonstrated significant improvements in accuracy and contextual understanding, particularly in knowledge-intensive tasks and industrial settings [4].\n",
            "\n",
            "Additionally, the use of sophisticated techniques such as dynamic embedding adjustment and routing mechanisms has been shown to enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [3][5]. Case studies in fields such as agriculture and healthcare further underscore the practical advantages of Modular RAG systems, showcasing their ability to integrate domain-specific knowledge effectively and provide actionable insights [1][3].\n",
            "\n",
            "These advancements suggest a promising future for Modular RAG systems, as they continue to evolve and integrate new technologies to address the growing demands of AI applications.\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by the evolution from Naive to Modular RAG systems is the inefficiency and inflexibility inherent in the monolithic design of traditional RAG models. Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics, leading to inaccurate or irrelevant retrievals. This limitation is exacerbated by the indiscriminate feeding of retrieved data into language models, which can introduce noise and increase computational overhead without necessarily enhancing output quality [1][3].\n",
            "\n",
            "In dynamic environments where data sources and types vary significantly, the single-agent architecture of Naive RAG systems becomes a bottleneck, hindering their effectiveness and scalability. This issue is particularly critical in production-level applications that require real-time data integration and adaptability to new information or scenarios. The need for a more flexible, efficient, and scalable solution is evident across various domains, including healthcare, finance, and education, where precision and contextual relevance are paramount [2][5].\n",
            "\n",
            "To address these challenges, a shift towards a Modular RAG framework is proposed. By decomposing the RAG architecture into independent, reconfigurable modules, this approach aims to enhance query efficiency, reduce processing overhead, and improve response accuracy. The introduction of autonomous AI agents within this modular framework further contributes to dynamic adaptability, ensuring that retrieval strategies and workflows are optimized to meet the complex requirements of diverse tasks [4][5].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "The methodological advancements in Modular RAG systems focus on transforming the traditional RAG architecture into a more flexible and scalable framework. This transformation is achieved through the decomposition of the RAG system into independent modules, each responsible for specific tasks such as query generation, data retrieval, and response synthesis. This modular approach allows for targeted improvements and customization, enabling the system to adapt to specific tasks or domains without requiring an overhaul of the entire architecture [4][6].\n",
            "\n",
            "A key aspect of this methodology is the integration of autonomous AI agents that employ agentic design patterns, such as reflection and planning, to dynamically manage retrieval strategies and adapt workflows. These agents iteratively refine contextual understanding, ensuring more accurate and context-aware responses across various applications [5]. Additionally, the use of advanced techniques such as dynamic embedding adjustment and routing mechanisms enhances the precision and relevance of the retrieved information, further improving the quality of generated responses [3][5].\n",
            "\n",
            "The implementation of token-level harmonization models provides a theoretical framework for balancing the benefits of external data integration with the potential drawbacks of noise and misinformation. This model allows for estimation of RAG effects on token prediction, supporting data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Overall, the methodology behind Modular RAG systems represents a significant leap forward, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "Implementing Modular RAG systems involves several key components and configurations that distinguish them from traditional Naive RAG models. The modular architecture is designed to facilitate reconfigurability and scalability, enabling each component to be independently modified or replaced as needed. This flexibility is essential for maintaining performance and adapting to new data or requirements in large-scale applications [4][6].\n",
            "\n",
            "The deployment of autonomous AI agents within the Modular RAG framework is a critical aspect of its implementation. These agents are responsible for managing retrieval strategies and adapting workflows dynamically, leveraging agentic design patterns to ensure optimal performance across various tasks. The integration of advanced techniques such as dynamic embedding adjustment and routing mechanisms further enhances the system's efficiency and accuracy [5].\n",
            "\n",
            "Additionally, the implementation of token-level harmonization models provides a theoretical basis for balancing external data integration with the potential drawbacks of noise and misinformation. This model supports data-driven improvements while minimizing detriments, ensuring that the system remains robust and reliable [2].\n",
            "\n",
            "Practical implementations of Modular RAG systems have demonstrated their effectiveness in various domains, including healthcare, finance, and education. By integrating domain-specific knowledge and optimizing query pipelines, these systems provide actionable insights and enhance decision-making processes across diverse applications [1][3]. The modular nature of these systems also facilitates compliance with ethical AI standards, ensuring that they can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "The experimental validation of Modular RAG systems involves a comprehensive evaluation of their performance across various datasets and applications. These experiments are designed to assess the effectiveness of the modular architecture and the integration of autonomous AI agents in enhancing query efficiency, response accuracy, and overall scalability [4][6].\n",
            "\n",
            "A key focus of these experiments is the comparison of Modular RAG systems with traditional Naive RAG models. By evaluating the systems' ability to handle complex queries, integrate real-time data, and adapt to diverse environments, researchers can quantify the improvements offered by the modular approach. Metrics such as retrieval precision, response accuracy, and computational overhead are commonly used to assess performance [1][3].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional insights into the balance between external data integration and potential drawbacks such as noise and misinformation. These models are evaluated for their predictive capability and their ability to support data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Case studies in fields such as agriculture and healthcare further demonstrate the practical advantages of Modular RAG systems. By incorporating domain-specific knowledge and leveraging advanced retrieval strategies, these systems provide actionable insights and enhance decision-making processes in real-world applications [1][3]. The results of these experiments underscore the potential of Modular RAG systems to transform AI applications by enhancing scalability, adaptability, and efficiency.\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of experiments and case studies on Modular RAG systems consistently demonstrate their superiority over traditional Naive RAG models in terms of efficiency, accuracy, and scalability. By decomposing the RAG architecture into independent modules, Modular RAG systems achieve significant improvements in query efficiency and response accuracy, reducing processing overhead and enhancing overall performance [4][6].\n",
            "\n",
            "The integration of autonomous AI agents within the Modular RAG framework further contributes to these improvements. By dynamically managing retrieval strategies and adapting workflows, these agents ensure that the system remains context-aware and responsive to complex task requirements. This adaptability is particularly beneficial in applications that require real-time data integration and precision, such as healthcare, finance, and education [5].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional support for these findings, highlighting the balance between external data integration and potential drawbacks such as noise and misinformation. These models enhance the reliability and accuracy of the system's outputs, supporting data-driven improvements without compromising performance [2].\n",
            "\n",
            "Case studies in various domains further validate the practical advantages of Modular RAG systems. In agriculture, these systems provide geographic-specific insights, enhancing decision-making processes for farmers. In healthcare, they improve the accuracy and reliability of question-answering systems for cancer patients, optimizing query pipelines and leveraging domain-specific databases [1][3].\n",
            "\n",
            "Overall, the results of these studies underscore the transformative potential of Modular RAG systems, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2501.09136v1  \n",
            "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
            "\n",
            "---\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "In the rapidly evolving landscape of AI technology, the shift from Naive RAG to Modular RAG frameworks marks a significant advancement in optimizing the performance and scalability of Retrieval-Augmented Generation systems. This report has thoroughly examined the comparative benefits of Modular RAG, underscoring its ability to address the inefficiencies inherent in Naive RAG systems. Naive RAG’s foundational architecture, while pivotal, struggles with inflexibility and processing bottlenecks, limiting its effectiveness in dynamic environments.\n",
            "\n",
            "Modular RAG introduces a multi-agent, distributed framework that enhances efficiency through specialized agents optimized for diverse data sources. This modularity allows for refined query processes, reduced overhead, and improved accuracy, as each agent focuses on specific tasks within the framework. The integration of autonomous AI agents further advances this system by dynamically adapting retrieval strategies to meet complex task requirements, thereby ensuring more context-aware and accurate outputs.\n",
            "\n",
            "Applications across various industries, such as healthcare and finance, demonstrate Modular RAG's scalability and adaptability in production environments, offering real-time data integration and enhanced decision-making capabilities. However, the challenges of modular interactions and ethical considerations remain areas for future exploration. Continued research and development will be essential to fully realize the potential of Modular RAG systems, paving the way for more robust and efficient AI applications that can adapt to the ever-changing demands of real-world scenarios.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Resume graph execution\n",
        "invoke_graph(graph, None, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f589f94",
      "metadata": {},
      "source": [
        "Here's how to display the final research report: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f076b709",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Enhancing AI Systems: The Modular RAG Advantage\n",
              "\n",
              "## Introduction\n",
              "\n",
              "In the dynamic landscape of artificial intelligence, the evolution of Retrieval-Augmented Generation (RAG) systems marks a pivotal advancement in the capabilities of Large Language Models (LLMs). Traditional Naive RAG systems laid the groundwork by integrating document retrieval with generative processes, facilitating the generation of contextually relevant outputs. However, as technological demands have grown, the limitations of Naive RAG—such as inefficiencies and a lack of adaptability—have become apparent. This report delves into the innovative Modular RAG approach, which offers a more efficient and scalable solution by employing a distributed multi-agent framework. This modular architecture allows for specialized agents optimized for specific data sources, enhancing query efficiency and improving response accuracy.\n",
              "\n",
              "The analysis begins by examining the foundational aspects of Naive RAG systems, highlighting their strengths and inherent limitations. It then transitions into the advancements brought forth by Modular RAG, emphasizing its multi-agent design and the integration of autonomous AI agents that dynamically manage retrieval strategies. The report further explores the practical applications of Modular RAG in various industries, such as healthcare and finance, showcasing its scalability and adaptability at the production level. Finally, it addresses the challenges and future directions for Modular RAG systems, ensuring they meet ethical AI standards while continuing to evolve and integrate new technologies.\n",
              "\n",
              "---\n",
              "\n",
              "## Main Idea\n",
              "\n",
              "\n",
              "\n",
              "### Background\n",
              "\n",
              "Retrieval-Augmented Generation (RAG) systems are pivotal in enhancing Large Language Models (LLMs) by integrating real-time data retrieval with language generation. Traditionally, Naive RAG systems have laid the groundwork by providing a basic framework that uses retrieval mechanisms to rank the relevance of indexed data to an input query, followed by a generative component that synthesizes responses. This method, effective for simple question-answering tasks, ensures that AI-generated outputs are coherent and contextually relevant. However, the Naive RAG model's reliance on a single-agent architecture presents significant limitations, particularly in handling diverse datasets and dynamic environments, leading to performance bottlenecks and reduced accuracy [1][3][5].\n",
              "\n",
              "The advent of Modular RAG systems marks a significant evolution in this domain. Unlike their Naive counterparts, Modular RAG systems employ a distributed multi-agent framework, where tasks are divided among specialized agents optimized for specific data sources, such as relational databases and document stores. This modularity allows for enhanced efficiency, scalability, and flexibility, making these systems well-suited for production-level applications [2][4]. By integrating autonomous AI agents that dynamically manage retrieval strategies and adapt workflows, Modular RAG systems achieve superior context-aware and accurate outputs across various fields, including healthcare, finance, and education [5].\n",
              "\n",
              "Overall, the shift from Naive to Modular RAG systems represents a transformative development in AI, offering a robust solution for integrating LLMs into diverse applications, thereby paving the way for more precise, efficient, and reliable AI-driven insights [6].\n",
              "\n",
              "### Related Work\n",
              "\n",
              "The development of Modular RAG systems can be traced back to the foundational Naive RAG models, which established the basic integration of retrieval mechanisms with language generation processes. These systems, however, face challenges related to shallow query understanding and retrieval redundancy, leading to inefficiencies in data processing and increased noise in outputs [1][2]. Over time, advancements in RAG systems have focused on overcoming these limitations through innovative approaches such as modular architecture, dynamic retrieval strategies, and enhanced data integration techniques.\n",
              "\n",
              "Recent studies highlight the transformative potential of Modular RAG systems in various applications. For instance, several research efforts have explored the introduction of token-level harmonization models, which balance the benefits of external data integration against the potential drawbacks of noise and misinformation [2]. Moreover, practical implementations of Modular RAG have demonstrated significant improvements in accuracy and contextual understanding, particularly in knowledge-intensive tasks and industrial settings [4].\n",
              "\n",
              "Additionally, the use of sophisticated techniques such as dynamic embedding adjustment and routing mechanisms has been shown to enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [3][5]. Case studies in fields such as agriculture and healthcare further underscore the practical advantages of Modular RAG systems, showcasing their ability to integrate domain-specific knowledge effectively and provide actionable insights [1][3].\n",
              "\n",
              "These advancements suggest a promising future for Modular RAG systems, as they continue to evolve and integrate new technologies to address the growing demands of AI applications.\n",
              "\n",
              "### Problem Definition\n",
              "\n",
              "The primary challenge addressed by the evolution from Naive to Modular RAG systems is the inefficiency and inflexibility inherent in the monolithic design of traditional RAG models. Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics, leading to inaccurate or irrelevant retrievals. This limitation is exacerbated by the indiscriminate feeding of retrieved data into language models, which can introduce noise and increase computational overhead without necessarily enhancing output quality [1][3].\n",
              "\n",
              "In dynamic environments where data sources and types vary significantly, the single-agent architecture of Naive RAG systems becomes a bottleneck, hindering their effectiveness and scalability. This issue is particularly critical in production-level applications that require real-time data integration and adaptability to new information or scenarios. The need for a more flexible, efficient, and scalable solution is evident across various domains, including healthcare, finance, and education, where precision and contextual relevance are paramount [2][5].\n",
              "\n",
              "To address these challenges, a shift towards a Modular RAG framework is proposed. By decomposing the RAG architecture into independent, reconfigurable modules, this approach aims to enhance query efficiency, reduce processing overhead, and improve response accuracy. The introduction of autonomous AI agents within this modular framework further contributes to dynamic adaptability, ensuring that retrieval strategies and workflows are optimized to meet the complex requirements of diverse tasks [4][5].\n",
              "\n",
              "### Methodology\n",
              "\n",
              "The methodological advancements in Modular RAG systems focus on transforming the traditional RAG architecture into a more flexible and scalable framework. This transformation is achieved through the decomposition of the RAG system into independent modules, each responsible for specific tasks such as query generation, data retrieval, and response synthesis. This modular approach allows for targeted improvements and customization, enabling the system to adapt to specific tasks or domains without requiring an overhaul of the entire architecture [4][6].\n",
              "\n",
              "A key aspect of this methodology is the integration of autonomous AI agents that employ agentic design patterns, such as reflection and planning, to dynamically manage retrieval strategies and adapt workflows. These agents iteratively refine contextual understanding, ensuring more accurate and context-aware responses across various applications [5]. Additionally, the use of advanced techniques such as dynamic embedding adjustment and routing mechanisms enhances the precision and relevance of the retrieved information, further improving the quality of generated responses [3][5].\n",
              "\n",
              "The implementation of token-level harmonization models provides a theoretical framework for balancing the benefits of external data integration with the potential drawbacks of noise and misinformation. This model allows for estimation of RAG effects on token prediction, supporting data-driven improvements while minimizing detriments [2].\n",
              "\n",
              "Overall, the methodology behind Modular RAG systems represents a significant leap forward, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
              "\n",
              "### Implementation Details\n",
              "\n",
              "Implementing Modular RAG systems involves several key components and configurations that distinguish them from traditional Naive RAG models. The modular architecture is designed to facilitate reconfigurability and scalability, enabling each component to be independently modified or replaced as needed. This flexibility is essential for maintaining performance and adapting to new data or requirements in large-scale applications [4][6].\n",
              "\n",
              "The deployment of autonomous AI agents within the Modular RAG framework is a critical aspect of its implementation. These agents are responsible for managing retrieval strategies and adapting workflows dynamically, leveraging agentic design patterns to ensure optimal performance across various tasks. The integration of advanced techniques such as dynamic embedding adjustment and routing mechanisms further enhances the system's efficiency and accuracy [5].\n",
              "\n",
              "Additionally, the implementation of token-level harmonization models provides a theoretical basis for balancing external data integration with the potential drawbacks of noise and misinformation. This model supports data-driven improvements while minimizing detriments, ensuring that the system remains robust and reliable [2].\n",
              "\n",
              "Practical implementations of Modular RAG systems have demonstrated their effectiveness in various domains, including healthcare, finance, and education. By integrating domain-specific knowledge and optimizing query pipelines, these systems provide actionable insights and enhance decision-making processes across diverse applications [1][3]. The modular nature of these systems also facilitates compliance with ethical AI standards, ensuring that they can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
              "\n",
              "### Experiments\n",
              "\n",
              "The experimental validation of Modular RAG systems involves a comprehensive evaluation of their performance across various datasets and applications. These experiments are designed to assess the effectiveness of the modular architecture and the integration of autonomous AI agents in enhancing query efficiency, response accuracy, and overall scalability [4][6].\n",
              "\n",
              "A key focus of these experiments is the comparison of Modular RAG systems with traditional Naive RAG models. By evaluating the systems' ability to handle complex queries, integrate real-time data, and adapt to diverse environments, researchers can quantify the improvements offered by the modular approach. Metrics such as retrieval precision, response accuracy, and computational overhead are commonly used to assess performance [1][3].\n",
              "\n",
              "The implementation of token-level harmonization models provides additional insights into the balance between external data integration and potential drawbacks such as noise and misinformation. These models are evaluated for their predictive capability and their ability to support data-driven improvements while minimizing detriments [2].\n",
              "\n",
              "Case studies in fields such as agriculture and healthcare further demonstrate the practical advantages of Modular RAG systems. By incorporating domain-specific knowledge and leveraging advanced retrieval strategies, these systems provide actionable insights and enhance decision-making processes in real-world applications [1][3]. The results of these experiments underscore the potential of Modular RAG systems to transform AI applications by enhancing scalability, adaptability, and efficiency.\n",
              "\n",
              "### Results\n",
              "\n",
              "The results of experiments and case studies on Modular RAG systems consistently demonstrate their superiority over traditional Naive RAG models in terms of efficiency, accuracy, and scalability. By decomposing the RAG architecture into independent modules, Modular RAG systems achieve significant improvements in query efficiency and response accuracy, reducing processing overhead and enhancing overall performance [4][6].\n",
              "\n",
              "The integration of autonomous AI agents within the Modular RAG framework further contributes to these improvements. By dynamically managing retrieval strategies and adapting workflows, these agents ensure that the system remains context-aware and responsive to complex task requirements. This adaptability is particularly beneficial in applications that require real-time data integration and precision, such as healthcare, finance, and education [5].\n",
              "\n",
              "The implementation of token-level harmonization models provides additional support for these findings, highlighting the balance between external data integration and potential drawbacks such as noise and misinformation. These models enhance the reliability and accuracy of the system's outputs, supporting data-driven improvements without compromising performance [2].\n",
              "\n",
              "Case studies in various domains further validate the practical advantages of Modular RAG systems. In agriculture, these systems provide geographic-specific insights, enhancing decision-making processes for farmers. In healthcare, they improve the accuracy and reliability of question-answering systems for cancer patients, optimizing query pipelines and leveraging domain-specific databases [1][3].\n",
              "\n",
              "Overall, the results of these studies underscore the transformative potential of Modular RAG systems, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
              "\n",
              "### Sources\n",
              "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
              "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
              "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
              "[4] http://arxiv.org/abs/2407.21059v1  \n",
              "[5] http://arxiv.org/abs/2501.09136v1  \n",
              "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
              "\n",
              "---\n",
              "\n",
              "## Conclusion\n",
              "\n",
              "In the rapidly evolving landscape of AI technology, the shift from Naive RAG to Modular RAG frameworks marks a significant advancement in optimizing the performance and scalability of Retrieval-Augmented Generation systems. This report has thoroughly examined the comparative benefits of Modular RAG, underscoring its ability to address the inefficiencies inherent in Naive RAG systems. Naive RAG’s foundational architecture, while pivotal, struggles with inflexibility and processing bottlenecks, limiting its effectiveness in dynamic environments.\n",
              "\n",
              "Modular RAG introduces a multi-agent, distributed framework that enhances efficiency through specialized agents optimized for diverse data sources. This modularity allows for refined query processes, reduced overhead, and improved accuracy, as each agent focuses on specific tasks within the framework. The integration of autonomous AI agents further advances this system by dynamically adapting retrieval strategies to meet complex task requirements, thereby ensuring more context-aware and accurate outputs.\n",
              "\n",
              "Applications across various industries, such as healthcare and finance, demonstrate Modular RAG's scalability and adaptability in production environments, offering real-time data integration and enhanced decision-making capabilities. However, the challenges of modular interactions and ethical considerations remain areas for future exploration. Continued research and development will be essential to fully realize the potential of Modular RAG systems, paving the way for more robust and efficient AI applications that can adapt to the ever-changing demands of real-world scenarios."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# Get final graph state\n",
        "final_state = graph.get_state(config)\n",
        "\n",
        "# Retrieve final report\n",
        "report = final_state.values.get(\"final_report\")\n",
        "\n",
        "# Display report in markdown format\n",
        "display(Markdown(report))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d81f0bfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Enhancing AI Systems: The Modular RAG Advantage\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In the dynamic landscape of artificial intelligence, the evolution of Retrieval-Augmented Generation (RAG) systems marks a pivotal advancement in the capabilities of Large Language Models (LLMs). Traditional Naive RAG systems laid the groundwork by integrating document retrieval with generative processes, facilitating the generation of contextually relevant outputs. However, as technological demands have grown, the limitations of Naive RAG—such as inefficiencies and a lack of adaptability—have become apparent. This report delves into the innovative Modular RAG approach, which offers a more efficient and scalable solution by employing a distributed multi-agent framework. This modular architecture allows for specialized agents optimized for specific data sources, enhancing query efficiency and improving response accuracy.\n",
            "\n",
            "The analysis begins by examining the foundational aspects of Naive RAG systems, highlighting their strengths and inherent limitations. It then transitions into the advancements brought forth by Modular RAG, emphasizing its multi-agent design and the integration of autonomous AI agents that dynamically manage retrieval strategies. The report further explores the practical applications of Modular RAG in various industries, such as healthcare and finance, showcasing its scalability and adaptability at the production level. Finally, it addresses the challenges and future directions for Modular RAG systems, ensuring they meet ethical AI standards while continuing to evolve and integrate new technologies.\n",
            "\n",
            "---\n",
            "\n",
            "## Main Idea\n",
            "\n",
            "\n",
            "\n",
            "### Background\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) systems are pivotal in enhancing Large Language Models (LLMs) by integrating real-time data retrieval with language generation. Traditionally, Naive RAG systems have laid the groundwork by providing a basic framework that uses retrieval mechanisms to rank the relevance of indexed data to an input query, followed by a generative component that synthesizes responses. This method, effective for simple question-answering tasks, ensures that AI-generated outputs are coherent and contextually relevant. However, the Naive RAG model's reliance on a single-agent architecture presents significant limitations, particularly in handling diverse datasets and dynamic environments, leading to performance bottlenecks and reduced accuracy [1][3][5].\n",
            "\n",
            "The advent of Modular RAG systems marks a significant evolution in this domain. Unlike their Naive counterparts, Modular RAG systems employ a distributed multi-agent framework, where tasks are divided among specialized agents optimized for specific data sources, such as relational databases and document stores. This modularity allows for enhanced efficiency, scalability, and flexibility, making these systems well-suited for production-level applications [2][4]. By integrating autonomous AI agents that dynamically manage retrieval strategies and adapt workflows, Modular RAG systems achieve superior context-aware and accurate outputs across various fields, including healthcare, finance, and education [5].\n",
            "\n",
            "Overall, the shift from Naive to Modular RAG systems represents a transformative development in AI, offering a robust solution for integrating LLMs into diverse applications, thereby paving the way for more precise, efficient, and reliable AI-driven insights [6].\n",
            "\n",
            "### Related Work\n",
            "\n",
            "The development of Modular RAG systems can be traced back to the foundational Naive RAG models, which established the basic integration of retrieval mechanisms with language generation processes. These systems, however, face challenges related to shallow query understanding and retrieval redundancy, leading to inefficiencies in data processing and increased noise in outputs [1][2]. Over time, advancements in RAG systems have focused on overcoming these limitations through innovative approaches such as modular architecture, dynamic retrieval strategies, and enhanced data integration techniques.\n",
            "\n",
            "Recent studies highlight the transformative potential of Modular RAG systems in various applications. For instance, several research efforts have explored the introduction of token-level harmonization models, which balance the benefits of external data integration against the potential drawbacks of noise and misinformation [2]. Moreover, practical implementations of Modular RAG have demonstrated significant improvements in accuracy and contextual understanding, particularly in knowledge-intensive tasks and industrial settings [4].\n",
            "\n",
            "Additionally, the use of sophisticated techniques such as dynamic embedding adjustment and routing mechanisms has been shown to enhance the precision and relevance of retrieved information, thereby improving the overall quality of generated responses [3][5]. Case studies in fields such as agriculture and healthcare further underscore the practical advantages of Modular RAG systems, showcasing their ability to integrate domain-specific knowledge effectively and provide actionable insights [1][3].\n",
            "\n",
            "These advancements suggest a promising future for Modular RAG systems, as they continue to evolve and integrate new technologies to address the growing demands of AI applications.\n",
            "\n",
            "### Problem Definition\n",
            "\n",
            "The primary challenge addressed by the evolution from Naive to Modular RAG systems is the inefficiency and inflexibility inherent in the monolithic design of traditional RAG models. Naive RAG systems often struggle with complex queries due to their reliance on straightforward similarity metrics, leading to inaccurate or irrelevant retrievals. This limitation is exacerbated by the indiscriminate feeding of retrieved data into language models, which can introduce noise and increase computational overhead without necessarily enhancing output quality [1][3].\n",
            "\n",
            "In dynamic environments where data sources and types vary significantly, the single-agent architecture of Naive RAG systems becomes a bottleneck, hindering their effectiveness and scalability. This issue is particularly critical in production-level applications that require real-time data integration and adaptability to new information or scenarios. The need for a more flexible, efficient, and scalable solution is evident across various domains, including healthcare, finance, and education, where precision and contextual relevance are paramount [2][5].\n",
            "\n",
            "To address these challenges, a shift towards a Modular RAG framework is proposed. By decomposing the RAG architecture into independent, reconfigurable modules, this approach aims to enhance query efficiency, reduce processing overhead, and improve response accuracy. The introduction of autonomous AI agents within this modular framework further contributes to dynamic adaptability, ensuring that retrieval strategies and workflows are optimized to meet the complex requirements of diverse tasks [4][5].\n",
            "\n",
            "### Methodology\n",
            "\n",
            "The methodological advancements in Modular RAG systems focus on transforming the traditional RAG architecture into a more flexible and scalable framework. This transformation is achieved through the decomposition of the RAG system into independent modules, each responsible for specific tasks such as query generation, data retrieval, and response synthesis. This modular approach allows for targeted improvements and customization, enabling the system to adapt to specific tasks or domains without requiring an overhaul of the entire architecture [4][6].\n",
            "\n",
            "A key aspect of this methodology is the integration of autonomous AI agents that employ agentic design patterns, such as reflection and planning, to dynamically manage retrieval strategies and adapt workflows. These agents iteratively refine contextual understanding, ensuring more accurate and context-aware responses across various applications [5]. Additionally, the use of advanced techniques such as dynamic embedding adjustment and routing mechanisms enhances the precision and relevance of the retrieved information, further improving the quality of generated responses [3][5].\n",
            "\n",
            "The implementation of token-level harmonization models provides a theoretical framework for balancing the benefits of external data integration with the potential drawbacks of noise and misinformation. This model allows for estimation of RAG effects on token prediction, supporting data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Overall, the methodology behind Modular RAG systems represents a significant leap forward, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Implementation Details\n",
            "\n",
            "Implementing Modular RAG systems involves several key components and configurations that distinguish them from traditional Naive RAG models. The modular architecture is designed to facilitate reconfigurability and scalability, enabling each component to be independently modified or replaced as needed. This flexibility is essential for maintaining performance and adapting to new data or requirements in large-scale applications [4][6].\n",
            "\n",
            "The deployment of autonomous AI agents within the Modular RAG framework is a critical aspect of its implementation. These agents are responsible for managing retrieval strategies and adapting workflows dynamically, leveraging agentic design patterns to ensure optimal performance across various tasks. The integration of advanced techniques such as dynamic embedding adjustment and routing mechanisms further enhances the system's efficiency and accuracy [5].\n",
            "\n",
            "Additionally, the implementation of token-level harmonization models provides a theoretical basis for balancing external data integration with the potential drawbacks of noise and misinformation. This model supports data-driven improvements while minimizing detriments, ensuring that the system remains robust and reliable [2].\n",
            "\n",
            "Practical implementations of Modular RAG systems have demonstrated their effectiveness in various domains, including healthcare, finance, and education. By integrating domain-specific knowledge and optimizing query pipelines, these systems provide actionable insights and enhance decision-making processes across diverse applications [1][3]. The modular nature of these systems also facilitates compliance with ethical AI standards, ensuring that they can be tailored to meet specific regulatory requirements and performance benchmarks [5].\n",
            "\n",
            "### Experiments\n",
            "\n",
            "The experimental validation of Modular RAG systems involves a comprehensive evaluation of their performance across various datasets and applications. These experiments are designed to assess the effectiveness of the modular architecture and the integration of autonomous AI agents in enhancing query efficiency, response accuracy, and overall scalability [4][6].\n",
            "\n",
            "A key focus of these experiments is the comparison of Modular RAG systems with traditional Naive RAG models. By evaluating the systems' ability to handle complex queries, integrate real-time data, and adapt to diverse environments, researchers can quantify the improvements offered by the modular approach. Metrics such as retrieval precision, response accuracy, and computational overhead are commonly used to assess performance [1][3].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional insights into the balance between external data integration and potential drawbacks such as noise and misinformation. These models are evaluated for their predictive capability and their ability to support data-driven improvements while minimizing detriments [2].\n",
            "\n",
            "Case studies in fields such as agriculture and healthcare further demonstrate the practical advantages of Modular RAG systems. By incorporating domain-specific knowledge and leveraging advanced retrieval strategies, these systems provide actionable insights and enhance decision-making processes in real-world applications [1][3]. The results of these experiments underscore the potential of Modular RAG systems to transform AI applications by enhancing scalability, adaptability, and efficiency.\n",
            "\n",
            "### Results\n",
            "\n",
            "The results of experiments and case studies on Modular RAG systems consistently demonstrate their superiority over traditional Naive RAG models in terms of efficiency, accuracy, and scalability. By decomposing the RAG architecture into independent modules, Modular RAG systems achieve significant improvements in query efficiency and response accuracy, reducing processing overhead and enhancing overall performance [4][6].\n",
            "\n",
            "The integration of autonomous AI agents within the Modular RAG framework further contributes to these improvements. By dynamically managing retrieval strategies and adapting workflows, these agents ensure that the system remains context-aware and responsive to complex task requirements. This adaptability is particularly beneficial in applications that require real-time data integration and precision, such as healthcare, finance, and education [5].\n",
            "\n",
            "The implementation of token-level harmonization models provides additional support for these findings, highlighting the balance between external data integration and potential drawbacks such as noise and misinformation. These models enhance the reliability and accuracy of the system's outputs, supporting data-driven improvements without compromising performance [2].\n",
            "\n",
            "Case studies in various domains further validate the practical advantages of Modular RAG systems. In agriculture, these systems provide geographic-specific insights, enhancing decision-making processes for farmers. In healthcare, they improve the accuracy and reliability of question-answering systems for cancer patients, optimizing query pipelines and leveraging domain-specific databases [1][3].\n",
            "\n",
            "Overall, the results of these studies underscore the transformative potential of Modular RAG systems, offering a robust solution for integrating LLMs into diverse applications and addressing the growing demands of AI-driven insights.\n",
            "\n",
            "### Sources\n",
            "[1] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
            "[2] https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches  \n",
            "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
            "[4] http://arxiv.org/abs/2407.21059v1  \n",
            "[5] http://arxiv.org/abs/2501.09136v1  \n",
            "[6] https://medium.com/@sahin.samia/modular-rag-using-llms-what-is-it-and-how-does-it-work-d482ebb3d372\n",
            "\n",
            "---\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "In the rapidly evolving landscape of AI technology, the shift from Naive RAG to Modular RAG frameworks marks a significant advancement in optimizing the performance and scalability of Retrieval-Augmented Generation systems. This report has thoroughly examined the comparative benefits of Modular RAG, underscoring its ability to address the inefficiencies inherent in Naive RAG systems. Naive RAG’s foundational architecture, while pivotal, struggles with inflexibility and processing bottlenecks, limiting its effectiveness in dynamic environments.\n",
            "\n",
            "Modular RAG introduces a multi-agent, distributed framework that enhances efficiency through specialized agents optimized for diverse data sources. This modularity allows for refined query processes, reduced overhead, and improved accuracy, as each agent focuses on specific tasks within the framework. The integration of autonomous AI agents further advances this system by dynamically adapting retrieval strategies to meet complex task requirements, thereby ensuring more context-aware and accurate outputs.\n",
            "\n",
            "Applications across various industries, such as healthcare and finance, demonstrate Modular RAG's scalability and adaptability in production environments, offering real-time data integration and enhanced decision-making capabilities. However, the challenges of modular interactions and ethical considerations remain areas for future exploration. Continued research and development will be essential to fully realize the potential of Modular RAG systems, paving the way for more robust and efficient AI applications that can adapt to the ever-changing demands of real-world scenarios.\n"
          ]
        }
      ],
      "source": [
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-A2cWC-y3-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
