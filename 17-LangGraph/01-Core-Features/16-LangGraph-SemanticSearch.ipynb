{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to Add Semantic Search to Agent's Memory\n",
    "\n",
    "- Author: [leebeanbin](https://github.com/leebeanbin)\n",
    "- Peer Review: [Jiwon Kim](https://github.com/brain604), [ByungGil Yoon](https://github.com/acho98)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/16-Evaluations/02-Evaluation-using-RAGAS.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/16-Evaluations/02-Evaluation-using-RAGAS.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "A guide on implementing `Semantic Search` in LangGraph memory systems, offering a different approach to memory management that enables context-aware and similarity-based retrieval.\n",
    "\n",
    "## Memory Management Approaches in LangGraph\n",
    "\n",
    "### Different Memory Systems\n",
    "- `Checkpointing`: State persistence with thread management\n",
    "- `Summarization`: Compression of long conversations\n",
    "- `Semantic Search`: Vector-based similarity retrieval\n",
    "- Note: Each system serves different use cases and requirements\n",
    "\n",
    "### Semantic Search Characteristics\n",
    "- Vector-based memory retrieval\n",
    "- Natural language querying\n",
    "- Contextual understanding\n",
    "- Similarity-based matching\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [What is Sementic Search?](#what-is-semantic-search)\n",
    "- [Advanced Memory Features](#advanced-memory-features)\n",
    "- [Using Semantic Search in Agents](#using-semantic-search-in-agents)\n",
    "- [Key Differences Between Basic Search and Semantic Search](#key-differences-between-basic-search-and-semantic-search)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Memory in LangGraph](https://langchain-ai.github.io/langgraph/concepts/memory/#editing-message-lists)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/)\n",
    "- [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.PutOp.index)\n",
    "\n",
    "----"
   ],
   "id": "45cf8684d63f659"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
    "\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ],
   "id": "65c2bad5f5ad8e39"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T01:04:18.918080Z",
     "start_time": "2025-01-24T01:04:17.600657Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:04:20.622163Z",
     "start_time": "2025-01-24T01:04:18.919905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"langgraph\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ],
   "id": "eac70d3d3221fc22",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can set API keys in a `.env` file or set them manually.\n",
    "\n",
    "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
   ],
   "id": "7ca312cd77e99685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"LLM-as-Judge\",\n",
    "    }\n",
    ")"
   ],
   "id": "73e78ad64d0f4c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:04:41.245137Z",
     "start_time": "2025-01-24T01:04:41.211117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ],
   "id": "dcd31642ea1f09be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is Semantic Search?\n",
    "\n",
    "`Semantic search` is a method that retrieves data based on its meaning and context, rather than just matching keywords. It uses vector embeddings to represent data and calculates relevance through mathematical techniques like cosine similarity."
   ],
   "id": "476fc9c60f41736a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparison : Basic Memory Search vs. Semantic Search\n",
    "\n",
    "#### Basic Memory Search"
   ],
   "id": "5993e1df95b61cd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:39:42.480991Z",
     "start_time": "2025-01-22T16:39:42.474988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic memory search\n",
    "memories = {\n",
    "    \"1\": \"I love machine learning\",\n",
    "    \"2\": \"Neural networks are fascinating\",\n",
    "    \"3\": \"I enjoy programming in Python\",\n",
    "    \"4\": \"Data science is my passion\",\n",
    "}\n",
    "\n",
    "# Simple keyword-based search\n",
    "query = \"machine learning\"\n",
    "results = [value for key, value in memories.items() if query in value]\n",
    "\n",
    "# Output\n",
    "for result in results:\n",
    "    print(f\"Memory: {result}\")"
   ],
   "id": "b524c5e62487f020",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I love machine learning\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`Basic Memory Search` returns data only if the query string \"machine learning\" is exactly present in the stored memory.\n",
    "The search fails if the query slightly differs from the stored text, even if the context is related."
   ],
   "id": "61baacc32b1b47f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sementic Search\n",
    "\n",
    "#### Basic Memory Configuration  \n",
    "\n",
    "To enable `Semantic Search`, we need to create a store with index configuration. The store will use embeddings to convert text into vectors for similarity searching.\n",
    "\n",
    "[Note] \n",
    "The `init_embeddings` function is currently in beta and its API may change. For production use, consider using stable embedding models or monitor API changes.\n"
   ],
   "id": "b1c102516a642a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:04:49.163464Z",
     "start_time": "2025-01-24T01:04:46.524274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")"
   ],
   "id": "375db18c96bb9447",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/xhv549ln7db228ly_v8l67t00000gn/T/ipykernel_10229/3437837375.py:5: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
      "  embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alternatively, you can use other embedding models:",
   "id": "cfc1ff3b853b65f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Option 1: Using OpenAI embeddings directly\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Option 2: Using HuggingFace embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ],
   "id": "812874f1e2b67e52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After setting up embeddings, create the store with `Semantic Search` enabled:",
   "id": "597a532ad208db3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:01.650666Z",
     "start_time": "2025-01-24T01:05:01.645438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create store with semantic search enabled\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")"
   ],
   "id": "fca10465a5d6a2d1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Creating and Searching Memories\n",
    "\n",
    "Let's store some example memories and see how `Semantic Search` works:"
   ],
   "id": "b5b195fcea51ab96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:09.532615Z",
     "start_time": "2025-01-24T01:05:04.080184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Store test memories\n",
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love machine learning\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"Neural networks are fascinating\"})\n",
    "store.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I enjoy programming in Python\"})\n",
    "store.put((\"user_123\", \"memories\"), \"4\", {\"text\": \"Data science is my passion\"})"
   ],
   "id": "6a900051d64ea302",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Search memories with natural language query:",
   "id": "195043c3d10dc8ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:10.285509Z",
     "start_time": "2025-01-24T01:05:09.537248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = store.search(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    query=\"Tell me about AI and ML\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "# Print results with similarity scores\n",
    "for memory in results:\n",
    "    print(f'Memory: {memory.value[\"text\"]} (similarity: {memory.score:.3f})')"
   ],
   "id": "47df7e76a0a53289",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I love machine learning (similarity: 0.509)\n",
      "Memory: Neural networks are fascinating (similarity: 0.399)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced Features\n",
    "\n",
    "### Multi-vector Indexing\n",
    "\n",
    "You can index different aspects of memories separately:"
   ],
   "id": "6a6533057f27ab4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:10.328964Z",
     "start_time": "2025-01-24T01:05:10.290346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_embeddings(embedding_model_name: str):\n",
    "    \"\"\"Creates the embedding model.\"\"\"\n",
    "    return init_embeddings(embedding_model_name)\n",
    "\n",
    "def create_memory_store(embeddings, dims):\n",
    "    \"\"\"Creates a memory store using the provided embeddings and dimensions.\"\"\"\n",
    "    return InMemoryStore(index={\"embed\": embeddings, \"dims\": dims})\n",
    "\n",
    "def store_memories(store: InMemoryStore, user_id: str, memories: List[Dict[str, str]]) -> InMemoryStore:\n",
    "    \"\"\"Stores multiple memories, recreating the store each time (for immutability).\"\"\"\n",
    "    for i, memory in enumerate(memories):\n",
    "        store.put((user_id, \"memories\"), f\"mem{i+1}\", memory)\n",
    "    return store # No need to copy anymore\n",
    "\n",
    "def search_memories(store: InMemoryStore, user_id: str, query: str, limit: int = 2) -> List:\n",
    "    \"\"\"Searches for semantically similar memories in the memory store.\"\"\"\n",
    "    return store.search((user_id, \"memories\"), query=query, limit=limit)\n",
    "\n",
    "def format_search_result(result) -> str:\n",
    "    \"\"\"Formats a single search result into a readable string.\"\"\"\n",
    "    return f\"\"\"Content: {result.value['content']}\n",
    "Context: {result.value['context']}\n",
    "Similarity Score: {result.score:.3f}\n",
    "{\"-\" * 20}\"\"\"\n",
    "\n",
    "def print_search_results(results: List):\n",
    "    \"\"\"Prints the search results.\"\"\"\n",
    "    print(\"\\n\".join(map(format_search_result, results)))"
   ],
   "id": "17024544b0a77139",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:16.716347Z",
     "start_time": "2025-01-24T01:05:16.192613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model_name = \"openai:text-embedding-3-small\"\n",
    "embeddings = create_embeddings(embedding_model_name)\n",
    "\n",
    "# Calculate dimensions ONCE\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "dims = len(test_embedding)\n",
    "\n",
    "memory_store = create_memory_store(embeddings, dims)\n",
    "\n",
    "mem_data = [\n",
    "    {\"content\": \"Transformer networks are a powerful architecture for natural language processing.\", \"context\": \"Reading about NLP models\"},\n",
    "    {\"content\": \"Recurrent Neural Networks (RNNs) are designed for sequential data like time series.\", \"context\": \"Studying deep learning\"},\n",
    "    {\"content\": \"Convolutional Neural Networks (CNNs) excel at image recognition tasks.\", \"context\": \"Working on computer vision projects\"},\n",
    "    {\"content\": \"Microservices architecture involves breaking down an application into small, independent services.\", \"context\": \"Discussing software design patterns\"},\n",
    "    {\"content\": \"Monolithic architecture is a traditional approach where all components of an application are tightly coupled.\", \"context\": \"Learning about software architecture\"},\n",
    "    {\"content\": \"BERT is a transformer-based model used for various NLP tasks.\", \"context\": \"Exploring pre-trained language models\"},\n",
    "    {\"content\": \"Fine-tuning BERT for sentiment analysis can improve accuracy.\", \"context\": \"Experimenting with NLP fine-tuning\"},\n",
    "    {\"content\": \"Software architecture focuses on the high-level structure and organization of a software system.\", \"context\": \"Software engineering course\"},\n",
    "    {\"content\": \"Choosing between monolith and microservices depends on factors like scalability and complexity.\", \"context\": \"Team discussion on system design\"}\n",
    "]"
   ],
   "id": "82e209448f7e0459",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:29.504784Z",
     "start_time": "2025-01-24T01:05:20.011860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory_store = store_memories(memory_store, \"user_123\", mem_data)\n",
    "\n",
    "search_query = \"machine learning architectures\"\n",
    "search_results = search_memories(memory_store, \"user_123\", search_query)\n",
    "print_search_results(search_results)"
   ],
   "id": "f14da65bd1aad911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Transformer networks are a powerful architecture for natural language processing.\n",
      "Context: Reading about NLP models\n",
      "Similarity Score: 0.390\n",
      "--------------------\n",
      "Content: Monolithic architecture is a traditional approach where all components of an application are tightly coupled.\n",
      "Context: Learning about software architecture\n",
      "Similarity Score: 0.366\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:05:32.004031Z",
     "start_time": "2025-01-24T01:05:31.376389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_query = \"software architecture design\"\n",
    "search_results = search_memories(memory_store, \"user_123\", search_query)\n",
    "print_search_results(search_results)"
   ],
   "id": "277123d34348d612",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Software architecture focuses on the high-level structure and organization of a software system.\n",
      "Context: Software engineering course\n",
      "Similarity Score: 0.531\n",
      "--------------------\n",
      "Content: Microservices architecture involves breaking down an application into small, independent services.\n",
      "Context: Discussing software design patterns\n",
      "Similarity Score: 0.504\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Semantic Search in Agents\n",
    "\n",
    "You can integrate `Semantic Search` into your agents :"
   ],
   "id": "5db60ccf183c970"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:11:09.456413Z",
     "start_time": "2025-01-24T01:11:09.399903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import START, MessagesState, StateGraph \n",
    "from langchain_opentutorial.graphs import visualize_graph \n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# Define the agent's behavior\n",
    "def chat_with_memory(state, *, store: BaseStore):\n",
    "    # Retrieve relevant memories based on the user's last message\n",
    "    query = state[\"messages\"][-1].content  # User's latest message\n",
    "    items = store.search(\n",
    "        (\"user_123\", \"memories\"), query=query, limit=3  # Retrieve top 3 matches\n",
    "    )\n",
    "\n",
    "    # Format retrieved memories with error handling\n",
    "    memories = []\n",
    "    for item in items:\n",
    "        try:\n",
    "            content = item.value['content']\n",
    "            context = item.value['context']\n",
    "            memories.append(f\"- {content} ({context})\")\n",
    "        except (KeyError, AttributeError):  # Handle potential errors\n",
    "            print(f\"Warning: Error processing memory item: {item}\")\n",
    "\n",
    "    memories_text = f\"## Related Memories:\\n{''.join(memories)}\" if memories else \"## No relevant memories found.\"\n",
    "\n",
    "    # Log the memories for debugging\n",
    "    print(\"Retrieved Memories:\")\n",
    "    print(memories_text)\n",
    "\n",
    "    # Generate response based on memories and the user's input\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories_text}\"},\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": [response]}\n"
   ],
   "id": "69096a7d5bb5e52",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Agent workflow :\n",
    "\n",
    "[Note] We will use the existing `memory_store data for subsequent examples."
   ],
   "id": "c059bd9cdbb17c22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:11:12.192017Z",
     "start_time": "2025-01-24T01:11:10.987500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a state graph for the agent\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(chat_with_memory)\n",
    "builder.add_edge(START, \"chat_with_memory\")\n",
    "\n",
    "# Compile the graph with the memory store\n",
    "agent_graph = builder.compile(store=memory_store)\n",
    "\n",
    "visualize_graph(agent_graph)"
   ],
   "id": "614f89bfc72d4e04",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:11:32.409316Z",
     "start_time": "2025-01-24T01:11:22.502981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# User initiates a conversation\n",
    "input_message = {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me more about machine learning architectures.\"}]}\n",
    "\n",
    "# Stream agent responses\n",
    "for message, metadata in agent_graph.stream(\n",
    "    input=input_message,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ],
   "id": "2e9ec56892005137",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Memories:\n",
      "## Related Memories:\n",
      "- Transformer networks are a powerful architecture for natural language processing. (Reading about NLP models)- Monolithic architecture is a traditional approach where all components of an application are tightly coupled. (Learning about software architecture)- Recurrent Neural Networks (RNNs) are designed for sequential data like time series. (Studying deep learning)\n",
      "Machine learning architectures refer to the structures and frameworks used to build and organize machine learning models. Different architectures are designed for different types of tasks and data. Here are some key concepts:\n",
      "\n",
      "1. **Monolithic Architecture**: This is a traditional approach where all components of a machine learning application are tightly coupled into a single unit. While simpler to manage initially, it can become difficult to scale and maintain as the application grows.\n",
      "\n",
      "2. **Modular Architecture**: In contrast to monolithic systems, modular architecture breaks the application into separate, interchangeable components (modules). This allows for easier updates, testing, and scaling.\n",
      "\n",
      "3. **Deep Learning Architectures**:\n",
      "   - **Convolutional Neural Networks (CNNs)**: Primarily used for image processing tasks, CNNs leverage convolutional layers to capture spatial hierarchies in data.\n",
      "   - **Recurrent Neural Networks (RNNs)**: Designed for sequential data, RNNs can maintain context over time, making them useful for tasks such as time series analysis and natural language processing.\n",
      "   - **Transformers**: A more recent architecture that has gained popularity in natural language processing and other tasks. Transformers use self-attention mechanisms to process sequences of data, allowing for parallelization and handling long-range dependencies effectively.\n",
      "\n",
      "4. **Ensemble Methods**: These architectures combine multiple models to improve performance. Techniques like bagging (e.g., Random Forest) and boosting (e.g., XGBoost) are common ensemble methods that can enhance predictive accuracy.\n",
      "\n",
      "5. **Graph Neural Networks (GNNs)**: GNNs are designed to work directly with graph structures, making them suitable for tasks involving relational data, such as social networks or molecular structures.\n",
      "\n",
      "6. **Autoencoders**: These are a type of neural network used for unsupervised learning. They consist of an encoder to compress the input into a lower-dimensional representation and a decoder to reconstruct the output. Autoencoders are often used for tasks like dimensionality reduction and anomaly detection.\n",
      "\n",
      "7. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networks, a generator and a discriminator, that compete against each other. GANs are widely used for generating realistic data samples, such as images.\n",
      "\n",
      "8. **Transfer Learning**: This architecture allows models pre-trained on large datasets to be fine-tuned for specific tasks with smaller datasets. This is particularly useful in domains where labeled data is scarce.\n",
      "\n",
      "These architectures can be combined and adapted to suit specific needs, and the choice of architecture often depends on the characteristics of the data and the problem being solved."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using Semantic Search with create_react_agent\n",
    "\n",
    "Enable `Semantic Search` in your agent by injecting the memory store into the state_modifier. This allows the agent to retrieve semantically relevant memories and include them in its context before responding. Additionally, you can use the store directly within a tool, allowing the agent to manually store or retrieve memories dynamically during conversations."
   ],
   "id": "a31b99e321a22eed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T01:11:41.693693Z",
     "start_time": "2025-01-24T01:11:33.452540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "def prepare_messages(state, *, store: BaseStore):\n",
    "    \"\"\"\n",
    "    Prepare messages for the LLM by searching memories based on the last user message.\n",
    "    \"\"\"\n",
    "    # Search based on the last user message\n",
    "    items = store.search(\n",
    "        (\"user_123\", \"memories\"),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "        limit=2\n",
    "    )\n",
    "\n",
    "    # Combine content and context from retrieved memories\n",
    "    memories = \"\\n\".join(\n",
    "        f\"- {item.value['content']} (Context: {item.value['context']})\"\n",
    "        for item in items\n",
    "    )\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "\n",
    "    # Include the retrieved memories as system-level context\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"}\n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "\n",
    "def upsert_memory(\n",
    "    content: str,\n",
    "    context: str,\n",
    "    *,\n",
    "    memory_id: Optional[uuid.UUID] = None,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert a memory into the store.\n",
    "    \"\"\"\n",
    "    mem_id = memory_id or uuid.uuid4()  # Generate a new UUID if not provided\n",
    "    store.put(\n",
    "        (\"user_123\", \"memories\"),\n",
    "        key=str(mem_id),\n",
    "        value={\n",
    "            \"content\": content,\n",
    "            \"context\": context,\n",
    "        },\n",
    "    )\n",
    "    return f\"Stored memory with ID: {mem_id}\"\n",
    "\n",
    "\n",
    "# Initialize the agent\n",
    "agent = create_react_agent(\n",
    "    init_chat_model(\"openai:gpt-4o-mini\"),\n",
    "    tools=[upsert_memory],  # Include the upsert memory tool\n",
    "    # Prepare messages for the LLM\n",
    "    state_modifier=prepare_messages,\n",
    "    store=memory_store,\n",
    ")\n",
    "\n",
    "# Example: Simulate a conversation\n",
    "for message, metadata in agent.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about transformer models\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ],
   "id": "84a74553bdbcb6f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have become the foundation for many state-of-the-art natural language processing (NLP) tasks. Here are some key features of transformer models:\n",
      "\n",
      "1. **Attention Mechanism**: Transformers utilize a mechanism called self-attention that allows the model to weigh the importance of different words in a sentence relative to each other. This enables the model to capture relationships between words regardless of their position in the input sequence.\n",
      "\n",
      "2. **Architecture**: A transformer model consists of an encoder and a decoder:\n",
      "   - The **encoder** processes the input data and generates a continuous representation.\n",
      "   - The **decoder** generates the output sequence based on the encoder's representation.\n",
      "\n",
      "3. **Parallelization**: Unlike recurrent neural networks (RNNs), which process sequences sequentially, transformers process the entire sequence at once. This allows for efficient training and makes them highly scalable.\n",
      "\n",
      "4. **Positional Encoding**: Since transformers do not have a built-in notion of order, they use positional encodings to provide information about the position of words in the sequence.\n",
      "\n",
      "5. **Pre-trained Models**: Many transformer models, such as BERT, GPT, and T5, are pre-trained on large corpuses of text and can be fine-tuned on specific tasks, making them versatile for various applications in NLP.\n",
      "\n",
      "6. **Applications**: Transformer models have been successfully applied to a wide range of tasks, including text classification, translation, summarization, question answering, and more.\n",
      "\n",
      "Overall, transformer models have revolutionized the field of NLP and have set new benchmarks in various tasks."
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Key Differences Between Basic Search and Semantic Search\n",
    "\n",
    "| **Feature**             | **Basic Memory Search**                        | **Semantic Search**                              |\n",
    "|-------------------------|-----------------------------------------------|------------------------------------------------|\n",
    "| **Search Method**        | Matches exact keywords                       | Finds data based on meaning and context        |\n",
    "| **Result Accuracy**      | Limited to exact matches                      | Retrieves data with related concepts or similar meaning |\n",
    "| **Underlying Technology**| Simple string matching                       | Vector embeddings, cosine similarity, etc.     |\n",
    "| **Query Example**        | `\"machine learning\"` → Result: `\"I love machine learning\"` | `\"Tell me about AI and ML\"` → Results: `\"I love machine learning\", \"Neural networks are fascinating\"` |\n",
    "| **Scalability**          | Slower with large datasets                   | Optimized for fast searches in large datasets |\n"
   ],
   "id": "eccf3ff6ae6fac77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
