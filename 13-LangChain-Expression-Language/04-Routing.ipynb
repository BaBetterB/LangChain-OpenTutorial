{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "- Author: [Lee Jungbin](https://github.com/leebeanbin)\n",
    "- Peer Review: [Teddy Lee](https://github.com/teddylee777), [Musang Kim](https://github.com/musangk)\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces key concepts and practical applications of `RunnableSequence` in LangChain. RunnableSequence is a powerful tool for creating sequential processing pipelines that enable structured data flow and transformation.\n",
    "\n",
    "Key features of RunnableSequence include:\n",
    "- Sequential execution of multiple operations\n",
    "- Automatic data passing between steps\n",
    "- Pipeline-level error handling\n",
    "- Integration with other Runnable components\n",
    "- Support for async operations\n",
    "\n",
    "Through detailed examples and best practices, you'll learn how to effectively use RunnableSequence to build robust and maintainable data processing pipelines.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Understanding RunnableSequence](#understanding-runnablesequence)\n",
    "- [Basic Usage](#basic-usage)\n",
    "- [Advanced Features](#advanced-features)\n",
    "- [Best Practices](#best-practices)\n",
    "- [Real-world Examples](#real-world-examples)\n",
    "- [Performance Optimization](#performance-optimization)\n",
    "\n",
    "### References\n",
    "- [RunnableSequence API Reference](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableSequence.html)\n",
    "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/interface)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:20:48.003391Z",
     "start_time": "2025-01-08T14:20:47.112772Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:23:09.041326Z",
     "start_time": "2025-01-08T14:23:07.811595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"pydantic\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:37.994065Z",
     "start_time": "2025-01-08T14:32:37.976718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"04-Routing\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:40.343808Z",
     "start_time": "2025-01-08T14:32:40.322503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "# Reload any variables that need to be overwritten from the previous cell\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding RunnableSequence\n",
    "\n",
    "`RunnableSequence` is a fundamental component in LangChain that enables the creation of sequential processing pipelines. It allows developers to chain multiple operations together where the output of one step becomes the input of the next step.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Sequential Processing**\n",
    "   - Ordered execution of operations\n",
    "   - Automatic data flow between steps\n",
    "   - Clear pipeline structure\n",
    "\n",
    "2. **Data Transformation**\n",
    "   - Input preprocessing\n",
    "   - State management\n",
    "   - Output formatting\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Pipeline-level error management\n",
    "   - Step-specific error recovery\n",
    "   - Fallback mechanisms\n",
    "\n",
    "Let's explore these concepts with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:44.972986Z",
     "start_time": "2025-01-08T14:32:42.969298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is a sample for processing purposes. It is likely being used as an example for a specific task or function. The content of the text is not specified beyond being a sample.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Basic Example: Text Processing Pipeline\n",
    "basic_chain = (\n",
    "    # Step 1: Input handling and prompt creation\n",
    "    PromptTemplate.from_template(\"Summarize this text in three sentences: {text}\")\n",
    "    # Step 2: LLM processing\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    # Step 3: Output parsing\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = basic_chain.invoke({\"text\": \"This is a sample text to process.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "RunnableSequence can be used in various ways, from simple text processing to complex data transformations. Here are some fundamental usage patterns.\n",
    "\n",
    "### 1. Simple Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:39:23.795567Z",
     "start_time": "2025-01-08T14:39:19.082900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content: Transformer architecture in Large Language Models (LLMs) is a crucial advancement in the field of artificial intelligence and machine learning. This architecture has revolutionized natural language processing tasks by enabling models to learn long-range dependencies and capture complex patterns in sequential data without the need for recurrent neural networks.\n",
      "\n",
      "Core technical concepts:\n",
      "The key technical concept in the Transformer architecture is the self-attention mechanism, which allows the model to weigh the importance of different input tokens when making predictions. This mechanism enables the model to process input sequences in parallel, making it more efficient than recurrent neural networks. The Transformer architecture consists of multiple layers of self-attention and feedforward neural networks, allowing the model to capture intricate relationships within the input data.\n",
      "\n",
      "Implementation details:\n",
      "The Transformer architecture is typically implemented using a stack of encoder and decoder layers. The encoder processes the input sequence, while the decoder generates the output sequence. Each layer in the Transformer consists of multiple multi-head self-attention modules followed by feedforward neural networks. The self-attention mechanism computes attention weights for each token in the input sequence, which are used to create context-aware representations for each token. The feedforward neural networks then process these representations to generate the final output.\n",
      "\n",
      "Real-world applications:\n",
      "Transformer architecture has been successfully applied to a wide range of natural language processing tasks, including machine translation, text summarization, sentiment analysis, and question answering. Models such as BERT, GPT-3, and T5 have demonstrated state-of-the-art performance on various benchmarks and have been widely adopted in industry applications.\n",
      "\n",
      "Technical challenges:\n",
      "Despite its success, the Transformer architecture poses several technical challenges. One major challenge is the high computational cost associated with training large-scale models with millions or billions of parameters. Training such models requires significant compute resources and can be prohibitively expensive for many researchers and organizations. Another challenge is the interpretation of model predictions, as the self-attention mechanism makes it challenging to understand how the model arrives at its decisions. Researchers are actively working on addressing these challenges through techniques such as model distillation, efficient attention mechanisms, and interpretability methods.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\"\"\"\n",
    "Basic Text Generation Pipeline\n",
    "- Purpose: Generate and process AI-related technical content\n",
    "- Features: Text generation, processing, and summarization\n",
    "- Use case: Creating technical AI documentation or explanations\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Define the text generation chain\n",
    "text_generation_chain = (\n",
    "    # Create prompt template for AI content generation\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Generate a detailed technical explanation about {topic} in AI/ML field.\n",
    "            Include:\n",
    "            - Core technical concepts\n",
    "            - Implementation details\n",
    "            - Real-world applications\n",
    "            - Technical challenges\n",
    "            \"\"\"\n",
    "    )\n",
    "    # Process with LLM using moderate temperature for creativity\n",
    "    | ChatOpenAI(temperature=0.7)\n",
    "    # Convert output to clean string\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = text_generation_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"Generated Content:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### 1. Structured Content Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:43:59.338461Z",
     "start_time": "2025-01-08T14:43:49.562180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis of the Content: 1. Core Technical Concepts:\n",
      "   - The main technical principle is the self-attention mechanism in the Transformer architecture.\n",
      "   - The key theoretical foundation is the ability of the model to focus on different parts of the input sequence by assigning weights to each input token based on its relevance to other tokens.\n",
      "\n",
      "2. Implementation Insights:\n",
      "   - The Transformer architecture consists of an encoder-decoder structure with multiple layers of self-attention mechanisms and feed-forward neural networks.\n",
      "   - Each layer includes a multi-head self-attention mechanism to attend to different parts of the input sequence simultaneously, and a feed-forward neural network to capture non-linear relationships in the data.\n",
      "\n",
      "3. Practical Applications:\n",
      "   - Real-world applications of Transformer architecture include natural language processing tasks such as machine translation, text summarization, and language modeling.\n",
      "   - Transformer-based models like BERT, GPT, and T5 have achieved state-of-the-art performance on various NLP benchmarks, showcasing the effectiveness of the architecture in handling sequential data.\n",
      "\n",
      "4. Technical Challenges:\n",
      "   - Scalability and computational efficiency are key challenges associated with the Transformer architecture.\n",
      "   - Mitigation strategies could involve optimizing the model architecture for efficiency, utilizing distributed computing resources, and exploring techniques for model compression and distillation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Content Analysis Pipeline\n",
    "- Purpose: Analyze the generated AI technical content\n",
    "- Features: Structured summarization and key point extraction\n",
    "- Use case: Understanding and organizing technical documentation\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Create the analysis chain\n",
    "analysis_chain = (\n",
    "    # Create prompt template for content analysis\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Analyze this technical AI content and provide a structured breakdown:\n",
    "            \n",
    "            Content to analyze:\n",
    "            {generated_content}\n",
    "            \n",
    "            Provide analysis in the following format:\n",
    "            1. Core Technical Concepts:\n",
    "               - List the main technical principles\n",
    "               - Explain key theoretical foundations\n",
    "            \n",
    "            2. Implementation Insights:\n",
    "               - Describe the architecture details\n",
    "               - Highlight critical components\n",
    "            \n",
    "            3. Practical Applications:\n",
    "               - Identify real-world use cases\n",
    "               - Note implementation considerations\n",
    "            \n",
    "            4. Technical Challenges:\n",
    "               - List potential difficulties\n",
    "               - Suggest mitigation strategies\n",
    "            \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create combined pipeline\n",
    "combined_chain = (\n",
    "    # Step 1: Generate technical content\n",
    "    {\"generated_content\": lambda x: text_generation_chain.invoke({\"topic\": x[\"topic\"]})}\n",
    "    # Step 2: Analyze the generated content\n",
    "    | analysis_chain\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = combined_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"\\nAnalysis of the Content:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:44:28.400112Z",
     "start_time": "2025-01-08T14:44:28.322889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive Analysis:\n",
      "\n",
      "Original Content: content='Transformer architecture in Large Language Models (LLMs) is a type of neural network architecture that has been widely adopted in natural language processing tasks due to its ability to capture long-range dependencies and relationships in sequential data. The Transformer architecture was first introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, and has since become a key component of state-of-the-art language models such as BERT, GPT, and T5.\\n\\nThe core idea behind the Transformer architecture is the use of self-attention mechanisms to compute representations of input sequences. Self-attention allows the model to weigh the importance of each token in the input sequence when generating a representation for a given token. This enables the model to focus on relevant parts of the input sequence and capture dependencies between tokens that are far apart in the sequence.\\n\\nThe Transformer architecture consists of multiple layers, each of which contains two main components: the self-attention mechanism and a feedforward neural network. In the self-attention mechanism, the input sequence is transformed into three different representations: query, key, and value. These representations are used to compute attention scores between each pair of tokens in the input sequence. The attention scores are then used to compute a weighted sum of the values, which forms the output of the self-attention mechanism.\\n\\nIn addition to the self-attention mechanism, each layer in the Transformer architecture also contains a feedforward neural network. The output of the self-attention mechanism is passed through a series of fully connected layers with activation functions such as ReLU or GELU. This allows the model to learn complex non-linear transformations of the input sequence.\\n\\nOne of the key advantages of the Transformer architecture is its parallelizability. Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which process sequences sequentially or in a fixed window, the Transformer architecture can process tokens in parallel. This makes it well-suited for training on large datasets and for scaling to longer input sequences.\\n\\nIn summary, the Transformer architecture in LLMs is a powerful neural network architecture that leverages self-attention mechanisms to capture long-range dependencies in sequential data. By using self-attention and feedforward neural networks in each layer, the Transformer architecture is able to learn complex representations of input sequences and has become a key component of state-of-the-art language models.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 477, 'prompt_tokens': 20, 'total_tokens': 497, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-079fdb46-9933-487b-a4e8-2a3b0d1b5039-0' usage_metadata={'input_tokens': 20, 'output_tokens': 477, 'total_tokens': 497, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "General Analysis: content=\"The technical content provided discusses the Transformer architecture in Large Language Models (LLMs) and its significance in the field of natural language processing (NLP). The content explains that the Transformer model, introduced in 2017, has become a fundamental component of various state-of-the-art NLP models such as BERT, GPT-3, and T5.\\n\\nThe key innovation of the Transformer architecture is the self-attention mechanism, which enables the model to weigh the importance of different input tokens and capture long-range dependencies in the input sequence. This mechanism enhances the model's ability to understand context and generate coherent text.\\n\\nThe Transformer architecture comprises an encoder and a decoder, each consisting of multiple layers of self-attention and feedforward neural networks. The encoder processes the input sequence through self-attention layers to learn contextual information, while the decoder uses the encoder's output to generate the output sequence, incorporating an additional attention mechanism to attend to the input sequence during generation.\\n\\nOne of the notable advantages of the Transformer architecture is its ability to parallelize computation, allowing it to scale to large datasets and achieve state-of-the-art performance on various NLP tasks. The self-attention mechanism also enables the model to capture complex patterns and relationships in the input sequence, enhancing its effectiveness in understanding and generating text.\\n\\nIn conclusion, the Transformer architecture in LLMs is a powerful deep learning model that has significantly impacted the NLP field. Its self-attention mechanism, parallel computation capability, and scalability to large datasets make it a popular choice for a wide range of NLP applications.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 313, 'prompt_tokens': 676, 'total_tokens': 989, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-6bd42515-da20-4da2-a73a-48a7638f861a-0' usage_metadata={'input_tokens': 676, 'output_tokens': 313, 'total_tokens': 989, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Technical Evaluation: {'technical_depth': {'complexity_score': 9, 'depth_analysis': 'The technical depth of the Transformer architecture, specifically the Attention is All You Need model, is highly advanced. The introduction of the self-attention mechanism allows for efficient capturing of long-range dependencies in input sequences, overcoming limitations of traditional RNNs and CNNs. The multi-head self-attention mechanism and position-wise feedforward neural networks in the encoder enable the model to consider all positions in the input sequence simultaneously and learn complex patterns effectively. The decoder, with its masked multi-head self-attention mechanism, is crucial for autoregressive tasks. Overall, the architecture demonstrates a deep understanding of sequential data processing.'}, 'implementation_evaluation': {'feasibility': 'The implementation of the Transformer architecture, while complex, is feasible with the availability of modern deep learning frameworks like TensorFlow and PyTorch. Pre-trained models such as BERT, GPT, and RoBERTa are readily accessible for fine-tuning on specific NLP tasks. However, training large-scale Transformer models may require significant computational resources and expertise.', 'required_resources': ['Deep learning framework (e.g., TensorFlow, PyTorch)', 'High-performance GPUs or TPUs for training large models', 'Access to pre-trained Transformer models for transfer learning']}, 'impact_assessment': {'technical_advantages': ['Efficient capturing of long-range dependencies in sequences', 'Ability to learn complex patterns in sequential data', 'Effective for various NLP tasks like language modeling, machine translation, and text classification'], 'industry_impact': \"The Transformer architecture has had a significant impact on the NLP industry by setting new benchmarks in performance for various tasks. Its widespread adoption by researchers and practitioners has led to advancements in NLP applications and the development of state-of-the-art models. The architecture's ability to handle complex language structures and dependencies has paved the way for more sophisticated NLP solutions.\"}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Structured Technical Evaluation Pipeline\n",
    "- Purpose: Detailed technical evaluation of AI content with specific metrics\n",
    "- Features: Metric-based analysis, structured output format\n",
    "- Use case: Technical documentation and evaluation reports\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Define Pydantic models\n",
    "class TechnicalDepth(BaseModel):\n",
    "    complexity_score: float = Field(description=\"Technical complexity score (1-10)\")\n",
    "    depth_analysis: str = Field(description=\"Analysis of technical depth\")\n",
    "\n",
    "\n",
    "class ImplementationEvaluation(BaseModel):\n",
    "    feasibility: str = Field(description=\"Implementation feasibility assessment\")\n",
    "    required_resources: List[str] = Field(\n",
    "        description=\"List of required technical resources\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImpactAssessment(BaseModel):\n",
    "    industry_impact: str = Field(description=\"Potential impact on industry\")\n",
    "    technical_advantages: List[str] = Field(description=\"Key technical advantages\")\n",
    "\n",
    "\n",
    "class EvaluationOutput(BaseModel):\n",
    "    technical_depth: TechnicalDepth\n",
    "    implementation_evaluation: ImplementationEvaluation\n",
    "    impact_assessment: ImpactAssessment\n",
    "\n",
    "\n",
    "# Text generation chain\n",
    "text_generation_chain = PromptTemplate.from_template(\n",
    "    \"Write a detailed technical explanation about {topic}.\"\n",
    ") | ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# Analysis chain\n",
    "analysis_chain = PromptTemplate.from_template(\n",
    "    \"Analyze the following technical content:\\n\\n{generated_content}\"\n",
    ") | ChatOpenAI(temperature=0)\n",
    "\n",
    "# Evaluation chain\n",
    "evaluation_parser = JsonOutputParser(pydantic_object=EvaluationOutput)\n",
    "\n",
    "evaluation_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Perform a detailed technical evaluation of the following AI content:\n",
    "        \n",
    "        {generated_content}\n",
    "        \n",
    "        Provide a structured evaluation following these criteria:\n",
    "        1. Technical Depth Assessment\n",
    "        2. Implementation Feasibility\n",
    "        3. Industry Impact Analysis\n",
    "        \n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | evaluation_parser\n",
    ")\n",
    "\n",
    "# Create comprehensive pipeline\n",
    "comprehensive_chain = RunnableParallel(\n",
    "    {\n",
    "        \"content\": lambda x: text_generation_chain.invoke({\"topic\": x[\"topic\"]}),\n",
    "        \"analysis\": lambda x: analysis_chain.invoke(\n",
    "            {\"generated_content\": text_generation_chain.invoke({\"topic\": x[\"topic\"]})}\n",
    "        ),\n",
    "        \"evaluation\": lambda x: evaluation_chain.invoke(\n",
    "            {\n",
    "                \"generated_content\": text_generation_chain.invoke(\n",
    "                    {\"topic\": x[\"topic\"]}\n",
    "                ),\n",
    "                \"format_instructions\": evaluation_parser.get_format_instructions(),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = comprehensive_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"\\nComprehensive Analysis:\")\n",
    "print(\"\\nOriginal Content:\", result[\"content\"])\n",
    "print(\"\\nGeneral Analysis:\", result[\"analysis\"])\n",
    "print(\"\\nTechnical Evaluation:\", result[\"evaluation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
