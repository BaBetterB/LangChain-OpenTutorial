{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableSequence\n",
    "\n",
    "- Author: [Lee Jungbin](https://github.com/leebeanbin)\n",
    "- Peer Review: [Teddy Lee](https://github.com/teddylee777), [김무상](https://github.com/musangk), [전창원](https://github.com/changwonjeon)\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces the concept and implementation of `RunnableSequence` in LangChain. RunnableSequence is a powerful tool that enables the creation of sequential processing pipelines, allowing for structured and efficient handling of AI-related tasks.\n",
    "\n",
    "Key Features of RunnableSequence:\n",
    "- Sequential processing pipeline creation\n",
    "- Automatic data flow management\n",
    "- Error handling and monitoring\n",
    "- Integration with other LangChain components\n",
    "- Support for async operations\n",
    "\n",
    "Through this tutorial, you'll learn how to effectively use RunnableSequence to build robust and maintainable AI applications.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Understanding RunnableSequence](#understanding-runnablesequence)\n",
    "- [Basic Pipeline Creation](#basic-pipeline-creation)\n",
    "- [Advanced Analysis Pipeline](#advanced-analysis-pipeline)\n",
    "- [Structured Evaluation Pipeline](#structured-evaluation-pipeline)\n",
    "- [Integration and Monitoring](#integration-and-monitoring)\n",
    "\n",
    "### References\n",
    "- [RunnableSequence API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html)\n",
    "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/interface)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:09:34.791241Z",
     "start_time": "2025-01-12T06:09:34.179621Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:24:44.108029Z",
     "start_time": "2025-01-12T06:24:42.365164Z"
    }
   },
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_openai\",\n",
    "        \"pydantic\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:37.994065Z",
     "start_time": "2025-01-08T14:32:37.976718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"04-Routing\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:19:18.866769Z",
     "start_time": "2025-01-12T06:19:18.848533Z"
    }
   },
   "source": [
    "# Load environment variables\n",
    "# Reload any variables that need to be overwritten from the previous cell\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding RunnableSequence\n",
    "\n",
    "`RunnableSequence` is a fundamental component in LangChain that enables the creation of sequential processing pipelines. It allows developers to chain multiple operations together where the output of one step becomes the input of the next step.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Sequential Processing**\n",
    "   - Ordered execution of operations\n",
    "   - Automatic data flow between steps\n",
    "   - Clear pipeline structure\n",
    "\n",
    "2. **Data Transformation**\n",
    "   - Input preprocessing\n",
    "   - State management\n",
    "   - Output formatting\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Pipeline-level error management\n",
    "   - Step-specific error recovery\n",
    "   - Fallback mechanisms\n",
    "\n",
    "Let's explore these concepts with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:15:08.386754Z",
     "start_time": "2025-01-12T06:15:05.979156Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Basic Example: Text Processing Pipeline\n",
    "basic_chain = (\n",
    "    # Step 1: Input handling and prompt creation\n",
    "    PromptTemplate.from_template(\"Summarize this text in three sentences: {text}\")\n",
    "    # Step 2: LLM processing\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    # Step 3: Output parsing\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = basic_chain.invoke({\"text\": \"This is a sample text to process.\"})\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is a sample for processing purposes. It is likely being used as an example for a specific task or function. The content of the text is not specified beyond being a sample.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pipeline Creation\n",
    "\n",
    "In this section, we'll explore how to create fundamental pipelines using RunnableSequence. We'll start with a simple text generation pipeline and gradually build more complex functionality.\n",
    "\n",
    "### Understanding Basic Pipeline Structure\n",
    "- Sequential Processing: How data flows through the pipeline\n",
    "- Component Integration: Combining different LangChain components\n",
    "- Data Transformation: Managing input/output between steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:24:09.219811Z",
     "start_time": "2025-01-12T08:24:05.509848Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\"\"\"\n",
    "Basic Text Generation Pipeline\n",
    "This demonstrates the fundamental way to chain components in RunnableSequence.\n",
    "\n",
    "Flow:\n",
    "1. PromptTemplate -> Creates the prompt with specific instructions\n",
    "2. ChatOpenAI -> Processes the prompt and generates content\n",
    "3. StrOutputParser -> Cleans and formats the output\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Define the basic text generation chain\n",
    "basic_generation_chain = (\n",
    "    # Create prompt template for AI content generation\n",
    "        PromptTemplate.from_template(\n",
    "            \"\"\"Generate a detailed technical explanation about {topic} in AI/ML field.\n",
    "            Include:\n",
    "            - Core technical concepts\n",
    "            - Implementation details\n",
    "            - Real-world applications\n",
    "            - Technical challenges\n",
    "            \"\"\"\n",
    "        )\n",
    "        # Process with LLM\n",
    "        | ChatOpenAI(temperature=0.7)\n",
    "        # Convert output to clean string\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "basic_result = basic_generation_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"Generated Content:\", result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content: Transformer architecture in Large Language Models (LLMs) has revolutionized the field of Natural Language Processing (NLP) by achieving state-of-the-art performance in various language understanding tasks. The core technical concept of the Transformer architecture lies in its ability to capture long-range dependencies in sequential data, such as text, without relying on recurrent or convolutional neural networks.\n",
      "\n",
      "The main components of a Transformer architecture include self-attention mechanisms and feed-forward neural networks. Self-attention allows the model to weigh the importance of different parts of the input sequence when generating the output representation. This mechanism enables the model to efficiently process long sequences by attending to relevant parts of the input at each position. The feed-forward neural networks introduce non-linearity and enable the model to learn complex relationships in the data.\n",
      "\n",
      "Implementation details of a Transformer architecture involve stacking multiple layers of self-attention and feed-forward networks to create a deep neural network. Each layer consists of a multi-head self-attention mechanism followed by a feed-forward neural network with residual connections and layer normalization. The model is trained using variations of the transformer architecture, such as BERT, GPT, and T5, have been applied to a wide range of NLP tasks, including text classification, language modeling, question answering, and machine translation.\n",
      "\n",
      "Real-world applications of Transformer architecture in LLMs include chatbots, sentiment analysis, document summarization, and language translation. These models have been used to improve search engine algorithms, automate customer support, and enhance language understanding in various applications.\n",
      "\n",
      "Some technical challenges in implementing Transformer architecture in LLMs include handling large amounts of data, training complex models, managing computational resources, and addressing issues related to overfitting and generalization. Researchers are actively working on developing more efficient transformer variants, such as sparse attention mechanisms, adaptive computation, and model distillation techniques, to address these challenges and improve the scalability and performance of LLMs in real-world applications.\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis Pipeline\n",
    "\n",
    "\n",
    "Building upon our basic pipeline, we'll now create a more sophisticated analysis system that processes and evaluates the generated content.\n",
    "\n",
    "### Key Features\n",
    "- State Management: Maintaining context throughout the pipeline\n",
    "- Structured Analysis: Organizing output in a clear format\n",
    "- Error Handling: Basic error management implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:37:28.149893Z",
     "start_time": "2025-01-12T08:37:28.032173Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Define the analysis prompt template\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Analyze this technical content and extract the most crucial insights:\n",
    "    \n",
    "    {generated_basic_content}\n",
    "    \n",
    "    Provide a concise analysis focusing only on the most important aspects:\n",
    "    (Importance : You should use Notion Syntax and try highliting with underlines, bold, emoji for title or something you describe context)\n",
    "    \n",
    "    Output format markdown outlet:\n",
    "    # Key Technical Analysis\n",
    "    \n",
    "    ## Core Concept Summary\n",
    "    [Extract and explain the 2-3 most fundamental concepts]\n",
    "    \n",
    "    ## Critical Implementation Insights\n",
    "    [Focus on crucial implementation details that make this technology work]\n",
    "    \n",
    "    ## Key Challenges & Solutions\n",
    "    [Identify the most significant challenges and their potential solutions]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Define the critical analysis chain\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=analysis_prompt,\n",
    "    middle=[ChatOpenAI(temperature=0)],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 3: Define the basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 4: Define the state initialization function\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"start_time\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "init_step = RunnableLambda(init_state)\n",
    "\n",
    "# Step 5: Define the content generation function\n",
    "def generated_basic_content(x):\n",
    "    content = basic_generation_chain.invoke({\"topic\": x[\"topic\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        # \"generated_basic_content\": content.content\n",
    "        # To create a comprehensive wrap-up, you can combine the previous basic result with new annotated analysis.\n",
    "        \"generated_basic_content\": basic_result\n",
    "    }\n",
    "\n",
    "generate_step = RunnableLambda(generated_basic_content)\n",
    "\n",
    "# Step 6: Define the analysis function\n",
    "def perform_analysis(x):\n",
    "    analysis = analysis_chain.invoke({\"generated_basic_content\": x[\"generated_basic_content\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        \"key_insights\": analysis\n",
    "    }\n",
    "\n",
    "analysis_step = RunnableLambda(perform_analysis)\n",
    "\n",
    "# Step 7: Define the output formatting function\n",
    "def format_output(x):\n",
    "    return {\n",
    "        \"timestamp\": x[\"start_time\"],\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"content\": x[\"generated_basic_content\"],\n",
    "        \"analysis\": x[\"key_insights\"],\n",
    "        \"formatted_output\": f\"\"\"\n",
    "# Technical Analysis Summary\n",
    "Generated: {x['start_time']}\n",
    "\n",
    "## Original Technical Content\n",
    "{x['generated_basic_content']}\n",
    "\n",
    "---\n",
    "\n",
    "{x['key_insights']}\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "format_step = RunnableLambda(format_output)\n",
    "\n",
    "# Step 8: Create the complete analysis pipeline\n",
    "analysis_pipeline = RunnableSequence(\n",
    "    first=init_step,\n",
    "    middle=[\n",
    "        generate_step,\n",
    "        analysis_step\n",
    "    ],\n",
    "    last=format_step\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p align=\"left\">\n",
    " <img src = \"/Users/leejungbin/Downloads/LangChain-OpenTutorial/13-LangChain-Expression-Language/img/Runnable_Pipeline.png\">\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:37:35.459087Z",
     "start_time": "2025-01-12T08:37:30.911877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "def run_analysis(topic: str):\n",
    "    result = analysis_pipeline.invoke({\"topic\": topic})\n",
    "\n",
    "    print(\"Analysis Timestamp:\", result[\"timestamp\"])\n",
    "    print(\"\\nTopic:\", result[\"topic\"])\n",
    "    print(\"\\nFormatted Output:\", result[\"formatted_output\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis(\"Transformer attention mechanisms\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Timestamp: 2025-01-12 17:37:30\n",
      "\n",
      "Topic: Transformer attention mechanisms\n",
      "\n",
      "Formatted Output: \n",
      "# Technical Analysis Summary\n",
      "Generated: 2025-01-12 17:37:30\n",
      "\n",
      "## Original Technical Content\n",
      "Transformer architecture in large language models (LLMs) is a breakthrough in the field of artificial intelligence and machine learning, particularly in natural language processing tasks. This architecture relies on self-attention mechanisms to process input sequences in parallel, making it highly efficient and effective for handling long-range dependencies.\n",
      "\n",
      "Core technical concepts:\n",
      "1. Self-attention: Transformers use self-attention mechanisms to weigh the importance of different tokens in the input sequence when generating the output. This allows the model to capture dependencies between distant tokens, unlike traditional RNNs or LSTMs which process inputs sequentially.\n",
      "2. Multi-head attention: Transformers employ multiple attention heads to capture different aspects of the input sequence simultaneously, leading to richer representations and improved performance.\n",
      "3. Feedforward neural networks: Transformers include feedforward neural networks to process the output of the attention mechanism and generate the final representations of the input sequence.\n",
      "\n",
      "Implementation details:\n",
      "Transformers consist of multiple layers, each containing a self-attention mechanism followed by a feedforward neural network. The input sequence is first embedded into a high-dimensional vector space, which is then fed into the transformer encoder. The encoder processes the input sequence through multiple layers of self-attention and feedforward neural networks to generate a contextualized representation of each token. This representation is then used for downstream tasks such as language modeling, translation, or sentiment analysis.\n",
      "\n",
      "Real-world applications:\n",
      "LLMs based on transformer architecture, such as BERT, GPT-3, and T5, have been widely adopted in various natural language processing tasks. These models have achieved state-of-the-art performance in tasks such as language modeling, sentiment analysis, question answering, and machine translation. They are also used in chatbots, recommendation systems, and content generation applications.\n",
      "\n",
      "Technical challenges:\n",
      "Despite their impressive performance, transformer-based LLMs have some technical challenges. One of the main challenges is the high computational and memory requirements of training and deploying these models, which limits their scalability. Another challenge is the interpretability of the learned representations, as transformers generate complex and opaque representations that are difficult to interpret. Additionally, fine-tuning transformers for specific tasks requires large amounts of labeled data, which can be a bottleneck in practical applications. Addressing these challenges is crucial for further advancing the field of transformer-based LLMs in AI/ML.\n",
      "\n",
      "---\n",
      "\n",
      "# Key Technical Analysis\n",
      "\n",
      "## Core Concept Summary\n",
      "- **Self-attention**: Transformers use self-attention mechanisms to capture dependencies between distant tokens in input sequences.\n",
      "- **Multi-head attention**: Employing multiple attention heads allows transformers to capture different aspects of the input sequence simultaneously.\n",
      "- **Feedforward neural networks**: Transformers use feedforward neural networks to process the output of the attention mechanism and generate final representations.\n",
      "\n",
      "## Critical Implementation Insights\n",
      "- Transformers consist of multiple layers with self-attention mechanisms and feedforward neural networks.\n",
      "- Input sequences are embedded into a high-dimensional vector space before being processed by the transformer encoder.\n",
      "- The encoder generates contextualized representations of each token for downstream tasks.\n",
      "\n",
      "## Key Challenges & Solutions\n",
      "- **High computational and memory requirements**: Addressing scalability issues by optimizing training and deployment processes.\n",
      "- **Interpretability of learned representations**: Developing methods to interpret complex and opaque representations generated by transformers.\n",
      "- **Data requirements for fine-tuning**: Exploring techniques to reduce the need for large amounts of labeled data for specific tasks.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structured Evaluation Pipeline\n",
    "\n",
    "In this section, we'll add structured evaluation capabilities to our pipeline, including proper error handling and validation.\n",
    "\n",
    "### Features\n",
    "- Structured Output: Using schema-based parsing\n",
    "- Validation: Input and output validation\n",
    "- Error Management: Comprehensive error handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:00:45.859918Z",
     "start_time": "2025-01-12T08:00:38.280834Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Structured Evaluation Pipeline\n",
    "\n",
    "This demonstrates:\n",
    "1. Custom output parsing with schema validation\n",
    "2. Error handling at each pipeline stage\n",
    "3. Comprehensive validation system\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Step 1: Define structured output schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"technical_evaluation\",\n",
    "        description=\"Technical evaluation of the content\",\n",
    "        type=\"object\",\n",
    "        properties={\n",
    "            \"core_concepts\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Key technical concepts identified\"\n",
    "            },\n",
    "            \"implementation_details\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"complexity\": {\"type\": \"string\"},\n",
    "                    \"requirements\": {\"type\": \"array\"},\n",
    "                    \"challenges\": {\"type\": \"array\"}\n",
    "                }\n",
    "            },\n",
    "            \"quality_metrics\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"technical_accuracy\": {\"type\": \"number\"},\n",
    "                    \"completeness\": {\"type\": \"number\"},\n",
    "                    \"clarity\": {\"type\": \"number\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "evaluation_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Step 2: Create basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 3: Create analysis chain\n",
    "analysis_prompt = RunnableLambda(lambda x: f\"\"\"Analyze the following content: {x['generated_content']}\"\"\")\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[analysis_prompt],\n",
    "    last=ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "# Step 4: Create evaluation chain\n",
    "evaluation_prompt = RunnableLambda(\n",
    "    lambda x: f\"\"\"\n",
    "    Evaluate the following AI technical content:\n",
    "    {x['generated_content']}\n",
    "    \n",
    "    Provide a structured evaluation following these criteria:\n",
    "    1. Identify and list core technical concepts\n",
    "    2. Assess implementation details\n",
    "    3. Rate quality metrics (1-10)\n",
    "    \n",
    "    {evaluation_parser.get_format_instructions()}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "evaluation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[evaluation_prompt, ChatOpenAI(temperature=0)],\n",
    "    last=evaluation_parser\n",
    ")\n",
    "\n",
    "# Helper function for error handling\n",
    "def try_or_error(func, error_list):\n",
    "    try:\n",
    "        return func()\n",
    "    except Exception as e:\n",
    "        error_list.append(str(e))\n",
    "        return None\n",
    "\n",
    "# Step 5: Create pipeline components\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"errors\": [],\n",
    "        \"start_time\": time.time()\n",
    "    }\n",
    "\n",
    "def generate_content(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"generated_content\": try_or_error(\n",
    "            lambda: basic_generation_chain.invoke({\"topic\": x[\"topic\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_analysis(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"analysis\": try_or_error(\n",
    "            lambda: analysis_chain.invoke({\"generated_content\": x[\"generated_content\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_evaluation(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"evaluation\": try_or_error(\n",
    "            lambda: evaluation_chain.invoke(x),\n",
    "            x[\"errors\"]\n",
    "        ) if not x[\"errors\"] else None\n",
    "    }\n",
    "\n",
    "def finalize_output(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"completion_time\": time.time() - x[\"start_time\"],\n",
    "        \"status\": \"success\" if not x[\"errors\"] else \"error\"\n",
    "    }\n",
    "\n",
    "# Step 6: Create integrated pipeline\n",
    "def create_evaluation_pipeline():\n",
    "    return RunnableSequence(\n",
    "        first=RunnableLambda(init_state),\n",
    "        middle=[\n",
    "            RunnableLambda(generate_content),\n",
    "            RunnableLambda(perform_analysis),\n",
    "            RunnableLambda(perform_evaluation)\n",
    "        ],\n",
    "        last=RunnableLambda(finalize_output)\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "def demonstrate_evaluation():\n",
    "    pipeline = create_evaluation_pipeline()\n",
    "    result = pipeline.invoke({\"topic\": \"Transformer attention mechanisms\"})\n",
    "\n",
    "    print(\"Pipeline Status:\", result[\"status\"])\n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(\"\\nEvaluation Results:\", json.dumps(result[\"evaluation\"], indent=2))\n",
    "    else:\n",
    "        print(\"\\nErrors Encountered:\", result[\"errors\"])\n",
    "\n",
    "    print(f\"\\nProcessing Time: {result['completion_time']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_evaluation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Status: success\n",
      "\n",
      "Evaluation Results: {\n",
      "  \"technical_evaluation\": {\n",
      "    \"core_technical_concepts\": [\n",
      "      \"Transformer model\",\n",
      "      \"Attention mechanisms\",\n",
      "      \"Self-attention\",\n",
      "      \"Multi-headed attention\",\n",
      "      \"Long-range dependencies\",\n",
      "      \"Sequence-to-sequence models\",\n",
      "      \"Recurrent neural networks\"\n",
      "    ],\n",
      "    \"implementation_details\": \"The content explains how attention mechanisms work in Transformers, including the computation of attention weights and the types of attention mechanisms used. It also highlights the advantages of Transformer attention mechanisms over previous models in capturing long-range dependencies.\",\n",
      "    \"quality_metrics\": 9\n",
      "  }\n",
      "}\n",
      "\n",
      "Processing Time: 7.48 seconds\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Integration and Monitoring\n",
    "\n",
    "The final section showcases how to integrate all components with comprehensive monitoring and quality assurance. We'll add performance tracking and quality metrics to our pipeline."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:53:00.774872Z",
     "start_time": "2025-01-12T08:52:53.292673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Integrated Pipeline with Monitoring and Quality Assurance\n",
    "\n",
    "This demonstrates:\n",
    "1. Full pipeline integration with monitoring\n",
    "2. Quality metrics collection\n",
    "3. Performance tracking\n",
    "4. Comprehensive logging\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter/IPython environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Handles monitoring and metrics collection for the pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.metrics = {\n",
    "            \"timestamps\": {},\n",
    "            \"performance\": {},\n",
    "            \"quality\": {}\n",
    "        }\n",
    "\n",
    "    def add_timestamp(self, stage: str):\n",
    "        self.metrics[\"timestamps\"][stage] = time.time() - self.start_time\n",
    "\n",
    "    def add_quality_metric(self, name: str, value: float):\n",
    "        self.metrics[\"quality\"][name] = value\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            **self.metrics,\n",
    "            \"total_time\": time.time() - self.start_time\n",
    "        }\n",
    "\n",
    "def extract_score(result: str) -> float:\n",
    "    \"\"\"Extracts numerical score from quality assessment result\"\"\"\n",
    "    try:\n",
    "        return float(next(num for num in result.split() if num.replace('.', '').isdigit()))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Step 1: Initialize pipeline state\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"monitor\": PipelineMonitor(),\n",
    "        \"start_time\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Step 2: Content generation\n",
    "async def generate_content(x):\n",
    "    content_prompt = f\"Generate technical content about: {x['topic']}\"\n",
    "    content = await ChatOpenAI(temperature=0.7).ainvoke(content_prompt)\n",
    "    x[\"monitor\"].add_timestamp(\"generation\")\n",
    "    return {**x, \"content\": content.content}\n",
    "\n",
    "# Step 3: Content analysis\n",
    "async def analyze_content(x):\n",
    "    analysis_prompt = f\"Analyze the following content: {x['content']}\"\n",
    "    analysis = await ChatOpenAI(temperature=0).ainvoke(analysis_prompt)\n",
    "    x[\"monitor\"].add_timestamp(\"analysis\")\n",
    "    return {**x, \"analysis\": analysis.content}\n",
    "\n",
    "# Step 4: Quality assessment\n",
    "quality_templates = {\n",
    "    \"technical_quality\": \"\"\"Rate the technical quality of this content (1-10):\n",
    "        {content}\n",
    "        \n",
    "        Provide a number and brief justification.\"\"\",\n",
    "    \"clarity\": \"\"\"Rate the clarity and understandability (1-10):\n",
    "        {content}\n",
    "        \n",
    "        Provide a number and brief justification.\"\"\"\n",
    "}\n",
    "\n",
    "def create_quality_chain():\n",
    "    return RunnableParallel({\n",
    "        metric: RunnableSequence(\n",
    "            first=PromptTemplate.from_template(template),\n",
    "            middle=[ChatOpenAI(temperature=0)],\n",
    "            last=RunnableLambda(lambda x: x.content)\n",
    "        )\n",
    "        for metric, template in quality_templates.items()\n",
    "    })\n",
    "\n",
    "async def assess_quality(x):\n",
    "    quality_chain = create_quality_chain()\n",
    "    quality_results = await quality_chain.ainvoke({\"content\": x[\"content\"]})\n",
    "\n",
    "    quality_metrics = {\n",
    "        k: v for k, v in quality_results.items()\n",
    "    }\n",
    "\n",
    "    for metric, result in quality_metrics.items():\n",
    "        x[\"monitor\"].add_quality_metric(metric, extract_score(result))\n",
    "\n",
    "    x[\"monitor\"].add_timestamp(\"quality_assessment\")\n",
    "    return {**x, \"quality_metrics\": quality_metrics}\n",
    "\n",
    "# Step 5: Create final output\n",
    "def format_output(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"content\": x[\"content\"],\n",
    "        \"analysis\": x[\"analysis\"],\n",
    "        \"quality_metrics\": x[\"quality_metrics\"],\n",
    "        \"metrics\": x[\"monitor\"].get_metrics(),\n",
    "        \"start_time\": x[\"start_time\"]\n",
    "    }\n",
    "\n",
    "# Create integrated pipeline\n",
    "def create_monitored_pipeline():\n",
    "    return RunnableSequence(\n",
    "        first=RunnableLambda(init_state),\n",
    "        middle=[\n",
    "            RunnableLambda(generate_content),\n",
    "            RunnableLambda(analyze_content),\n",
    "            RunnableLambda(assess_quality)\n",
    "        ],\n",
    "        last=RunnableLambda(format_output)\n",
    "    )\n",
    "\n",
    "# Example usage with full monitoring\n",
    "async def demonstrate_monitored_pipeline():\n",
    "    pipeline = create_monitored_pipeline()\n",
    "    result = await pipeline.ainvoke({\"topic\": \"Deep learning optimization techniques\"})\n",
    "\n",
    "    print(\"Generated Content Length:\", len(str(result[\"content\"])))\n",
    "    print(\"\\nContent:\", result[\"content\"])\n",
    "    print(\"\\nAnalysis:\", result[\"analysis\"])\n",
    "    print(\"\\nQuality Metrics:\", json.dumps(result[\"quality_metrics\"], indent=2))\n",
    "    print(\"\\nPerformance Metrics:\", json.dumps(result[\"metrics\"], indent=2))\n",
    "    return result\n",
    "\n",
    "# Jupyter/IPython 환경을 위한 실행 함수\n",
    "def run_pipeline():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    task = demonstrate_monitored_pipeline()\n",
    "    return loop.run_until_complete(task)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content Length: 2254\n",
      "\n",
      "Content: Deep learning optimization techniques refer to the methods and algorithms used to improve the performance and efficiency of deep learning models. These techniques are crucial for training deep neural networks effectively and achieving better results in various applications such as image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "One of the most commonly used optimization techniques in deep learning is gradient descent, which is a first-order optimization algorithm that updates the parameters of the neural network in the direction of the steepest descent of the loss function. However, gradient descent can sometimes suffer from slow convergence and getting stuck in local minima.\n",
      "\n",
      "To address these issues, various advanced optimization techniques have been developed, such as stochastic gradient descent (SGD), mini-batch gradient descent, momentum, RMSprop, Adam, and AdaGrad. These techniques incorporate additional strategies to speed up convergence, prevent oscillations, and adapt the learning rate during training.\n",
      "\n",
      "SGD is a popular optimization technique that randomly samples a subset of training data (mini-batch) to update the model parameters. This helps to accelerate the training process and reduce computational overhead compared to batch gradient descent, which processes the entire training dataset in each iteration.\n",
      "\n",
      "Momentum is another technique that introduces a momentum term to the gradient update, which helps to accelerate convergence by accumulating past gradients and smoothing out the update trajectory. RMSprop and Adam are adaptive learning rate optimization algorithms that adjust the learning rate for each parameter based on the magnitude of the gradients and the second moments of the gradients, respectively. This helps to prevent the learning rate from becoming too large or too small, improving the overall performance of the model.\n",
      "\n",
      "Overall, deep learning optimization techniques play a crucial role in training deep neural networks effectively and efficiently. By using advanced optimization algorithms, researchers and practitioners can improve the convergence speed, accuracy, and generalization of their deep learning models, leading to better performance in real-world applications.\n",
      "\n",
      "Analysis: The content provides an overview of deep learning optimization techniques and their importance in improving the performance and efficiency of deep learning models. It explains that while gradient descent is a commonly used optimization technique, it can have limitations such as slow convergence and getting stuck in local minima. To address these issues, advanced optimization techniques like stochastic gradient descent, momentum, RMSprop, Adam, and AdaGrad have been developed.\n",
      "\n",
      "The content highlights the benefits of these advanced optimization techniques, such as speeding up convergence, preventing oscillations, and adapting the learning rate during training. It explains how techniques like mini-batch gradient descent and momentum can help accelerate the training process and improve convergence by incorporating additional strategies.\n",
      "\n",
      "Furthermore, the content discusses adaptive learning rate optimization algorithms like RMSprop and Adam, which adjust the learning rate for each parameter based on the magnitude of the gradients and second moments of the gradients. This helps prevent the learning rate from becoming too large or too small, ultimately improving the overall performance of the model.\n",
      "\n",
      "Overall, the content emphasizes the importance of deep learning optimization techniques in training deep neural networks effectively and efficiently. By utilizing advanced optimization algorithms, researchers and practitioners can enhance the convergence speed, accuracy, and generalization of their deep learning models, leading to better performance in real-world applications.\n",
      "\n",
      "Quality Metrics: {\n",
      "  \"technical_quality\": \"I would rate this content a 8. \\n\\nThe content provides a clear and concise explanation of various deep learning optimization techniques, including gradient descent, SGD, momentum, RMSprop, and Adam. It effectively explains the purpose and benefits of each technique, as well as how they address common issues in training deep neural networks. The content also highlights the importance of optimization techniques in improving the performance and efficiency of deep learning models. Overall, the technical quality of the content is high, but could be improved with more specific examples or case studies to further illustrate the concepts.\",\n",
      "  \"clarity\": \"I would rate the clarity and understandability of this passage as an 8. The passage provides a clear explanation of deep learning optimization techniques, including common algorithms such as gradient descent, SGD, momentum, RMSprop, and Adam. The use of examples and comparisons helps to enhance understanding, but some technical terms may require prior knowledge of deep learning concepts. Overall, the passage effectively conveys the importance and benefits of optimization techniques in deep learning.\"\n",
      "}\n",
      "\n",
      "Performance Metrics: {\n",
      "  \"timestamps\": {\n",
      "    \"generation\": 3.3589181900024414,\n",
      "    \"analysis\": 5.718494415283203,\n",
      "    \"quality_assessment\": 7.253857135772705\n",
      "  },\n",
      "  \"performance\": {},\n",
      "  \"quality\": {\n",
      "    \"technical_quality\": 8.0,\n",
      "    \"clarity\": 8.0\n",
      "  },\n",
      "  \"total_time\": 7.2549262046813965\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
