{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableSequence\n",
    "\n",
    "- Author: [Jinu Cho](https://github.com/jinucho), [Lee Jungbin](https://github.com/leebeanbin)\n",
    "- Peer Review: [Teddy Lee](https://github.com/teddylee777), [김무상](https://github.com/musangk), [전창원](https://github.com/changwonjeon)\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces three key tools in LangChain: `RunnableSequence`, `RunnableBranch`, and `RunnableLambda`, essential for building efficient and powerful AI applications.\n",
    "\n",
    "`RunnableSequence` is a fundamental component that enables sequential processing pipelines, allowing structured and efficient handling of AI-related tasks. It provides automatic data flow management, error handling, and seamless integration with other LangChain components.\n",
    "\n",
    "`RunnableBranch` enables structured decision-making by routing input through predefined conditions, simplifying complex branching scenarios.\n",
    "\n",
    "`RunnableLambda` offers a flexible, function-based approach, ideal for lightweight transformations and inline processing.\n",
    "\n",
    "**Key Features of these components:**\n",
    "\n",
    "- **`RunnableSequence`:**\n",
    "  - Sequential processing pipeline creation\n",
    "  - Automatic data flow management\n",
    "  - Error handling and monitoring\n",
    "  - Support for async operations  \n",
    "\n",
    "- **`RunnableBranch`:**\n",
    "  - Dynamic routing based on conditions\n",
    "  - Structured decision trees\n",
    "  - Complex branching logic\n",
    "\n",
    "- **`RunnableLambda`:**\n",
    "  - Lightweight transformations\n",
    "  - Function-based processing\n",
    "  - Inline data manipulation\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [What is the RunnableSequence](#what-is-the-runnablesequence)\n",
    "- [What is the RunnableBranch](#what-is-the-runnablebranch)\n",
    "- [RunnableLambda](#runnablelambda)\n",
    "- [RunnableBranch](#runnablebranch)\n",
    "- [Comparison of RunnableBranch and RunnableLambda](#comparison-of-runnablebranch-and-runnablelambda)\n",
    "\n",
    "### References\n",
    "- [RunnableSequence API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html)\n",
    "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/interface)\n",
    "- [RunnableBranch API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html)  \n",
    "- [RunnableLambda API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:09:34.791241Z",
     "start_time": "2025-01-12T06:09:34.179621Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:24:44.108029Z",
     "start_time": "2025-01-12T06:24:42.365164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_openai\",\n",
    "        \"pydantic\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:37.994065Z",
     "start_time": "2025-01-08T14:32:37.976718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"04-Routing\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:19:18.866769Z",
     "start_time": "2025-01-12T06:19:18.848533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "# Reload any variables that need to be overwritten from the previous cell\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the RunnableSequence\n",
    "\n",
    "`RunnableSequence` is a fundamental component in LangChain that enables the creation of sequential processing pipelines. It allows developers to chain multiple operations together where the output of one step becomes the input of the next step.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Sequential Processing**\n",
    "   - Ordered execution of operations\n",
    "   - Automatic data flow between steps\n",
    "   - Clear pipeline structure\n",
    "\n",
    "2. **Data Transformation**\n",
    "   - Input preprocessing\n",
    "   - State management\n",
    "   - Output formatting\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Pipeline-level error management\n",
    "   - Step-specific error recovery\n",
    "   - Fallback mechanisms\n",
    "\n",
    "Let's explore these concepts with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:15:08.386754Z",
     "start_time": "2025-01-12T06:15:05.979156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is a sample for processing purposes. It is likely being used to test a system or program. The content is brief and straightforward.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Basic Example: Text Processing Pipeline\n",
    "basic_chain = (\n",
    "    # Step 1: Input handling and prompt creation\n",
    "    PromptTemplate.from_template(\"Summarize this text in three sentences: {text}\")\n",
    "    # Step 2: LLM processing\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    # Step 3: Output parsing\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = basic_chain.invoke({\"text\": \"This is a sample text to process.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pipeline Creation\n",
    "\n",
    "In this section, we'll explore how to create fundamental pipelines using RunnableSequence. We'll start with a simple text generation pipeline and gradually build more complex functionality.\n",
    "\n",
    "**Understanding Basic Pipeline Structure**  \n",
    "- Sequential Processing: How data flows through the pipeline\n",
    "- Component Integration: Combining different LangChain components\n",
    "- Data Transformation: Managing input/output between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:24:09.219811Z",
     "start_time": "2025-01-12T08:24:05.509848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content: This text is a sample for processing purposes. It is likely being used to test a system or program. The content is brief and straightforward.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\"\"\"\n",
    "Basic Text Generation Pipeline\n",
    "This demonstrates the fundamental way to chain components in RunnableSequence.\n",
    "\n",
    "Flow:\n",
    "1. PromptTemplate -> Creates the prompt with specific instructions\n",
    "2. ChatOpenAI -> Processes the prompt and generates content\n",
    "3. StrOutputParser -> Cleans and formats the output\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Define the basic text generation chain\n",
    "basic_generation_chain = (\n",
    "    # Create prompt template for AI content generation\n",
    "        PromptTemplate.from_template(\n",
    "            \"\"\"Generate a detailed technical explanation about {topic} in AI/ML field.\n",
    "            Include:\n",
    "            - Core technical concepts\n",
    "            - Implementation details\n",
    "            - Real-world applications\n",
    "            - Technical challenges\n",
    "            \"\"\"\n",
    "        )\n",
    "        # Process with LLM\n",
    "        | ChatOpenAI(temperature=0.7)\n",
    "        # Convert output to clean string\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "basic_result = basic_generation_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"Generated Content:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Analysis Pipeline\n",
    "\n",
    "\n",
    "Building upon our basic pipeline, we'll now create a more sophisticated analysis system that processes and evaluates the generated content.\n",
    "\n",
    "**Key Features**\n",
    "- State Management: Maintaining context throughout the pipeline\n",
    "- Structured Analysis: Organizing output in a clear format\n",
    "- Error Handling: Basic error management implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:37:28.149893Z",
     "start_time": "2025-01-12T08:37:28.032173Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "# Step 1: Define the analysis prompt template\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Analyze this technical content and extract the most crucial insights:\n",
    "    \n",
    "    {generated_basic_content}\n",
    "    \n",
    "    Provide a concise analysis focusing only on the most important aspects:\n",
    "    (Importance : You should use Notion Syntax and try highliting with underlines, bold, emoji for title or something you describe context)\n",
    "    \n",
    "    Output format markdown outlet:\n",
    "    # Key Technical Analysis\n",
    "    \n",
    "    ## Core Concept Summary\n",
    "    [Extract and explain the 2-3 most fundamental concepts]\n",
    "    \n",
    "    ## Critical Implementation Insights\n",
    "    [Focus on crucial implementation details that make this technology work]\n",
    "    \n",
    "    ## Key Challenges & Solutions\n",
    "    [Identify the most significant challenges and their potential solutions]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Define the critical analysis chain\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=analysis_prompt,\n",
    "    middle=[ChatOpenAI(temperature=0)],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 3: Define the basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 4: Define the state initialization function\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"start_time\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "init_step = RunnableLambda(init_state)\n",
    "\n",
    "# Step 5: Define the content generation function\n",
    "def generated_basic_content(x):\n",
    "    content = basic_generation_chain.invoke({\"topic\": x[\"topic\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        # \"generated_basic_content\": content.content\n",
    "        # To create a comprehensive wrap-up, you can combine the previous basic result with new annotated analysis.\n",
    "        \"generated_basic_content\": basic_result\n",
    "    }\n",
    "\n",
    "generate_step = RunnableLambda(generated_basic_content)\n",
    "\n",
    "# Step 6: Define the analysis function\n",
    "def perform_analysis(x):\n",
    "    analysis = analysis_chain.invoke({\"generated_basic_content\": x[\"generated_basic_content\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        \"key_insights\": analysis\n",
    "    }\n",
    "\n",
    "analysis_step = RunnableLambda(perform_analysis)\n",
    "\n",
    "# Step 7: Define the output formatting function\n",
    "def format_output(x):\n",
    "    return {\n",
    "        \"timestamp\": x[\"start_time\"],\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"content\": x[\"generated_basic_content\"],\n",
    "        \"analysis\": x[\"key_insights\"],\n",
    "        \"formatted_output\": f\"\"\"\n",
    "# Technical Analysis Summary\n",
    "Generated: {x['start_time']}\n",
    "\n",
    "## Original Technical Content\n",
    "{x['generated_basic_content']}\n",
    "\n",
    "---\n",
    "\n",
    "{x['key_insights']}\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "format_step = RunnableLambda(format_output)\n",
    "\n",
    "# Step 8: Create the complete analysis pipeline\n",
    "analysis_pipeline = RunnableSequence(\n",
    "    first=init_step,\n",
    "    middle=[\n",
    "        generate_step,\n",
    "        analysis_step\n",
    "    ],\n",
    "    last=format_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    " <img src = \"/Users/leejungbin/Downloads/LangChain-OpenTutorial/13-LangChain-Expression-Language/img/Runnable_Pipeline.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:37:35.459087Z",
     "start_time": "2025-01-12T08:37:30.911877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Timestamp: 2025-01-15 23:38:46\n",
      "\n",
      "Topic: Transformer attention mechanisms\n",
      "\n",
      "Formatted Output: \n",
      "# Technical Analysis Summary\n",
      "Generated: 2025-01-15 23:38:46\n",
      "\n",
      "## Original Technical Content\n",
      "Transformer architecture in Large Language Models (LLMs) is a key advancement in the field of Artificial Intelligence (AI) and Machine Learning (ML). The core technical concept of the Transformer architecture lies in its ability to process sequential data, such as text, with parallelization and attention mechanisms. This architecture eliminates the need for recurrent neural networks (RNNs) and allows for more efficient training of large-scale language models.\n",
      "\n",
      "At the heart of the Transformer architecture are self-attention mechanisms, which allow the model to weigh the importance of different input tokens when making predictions. This attention mechanism enables the model to capture long-range dependencies in the input sequence, making it well-suited for language modeling tasks. The Transformer architecture consists of multiple layers, each containing self-attention and feedforward neural network modules. The outputs of each layer are then passed to the next layer, allowing the model to learn complex patterns in the data.\n",
      "\n",
      "Implementation details of the Transformer architecture involve training the model on large amounts of text data using techniques such as stochastic gradient descent and backpropagation. The model is typically pre-trained on a large dataset using unsupervised learning methods, such as language modeling or masked language modeling. Fine-tuning the model on a specific task, such as text classification or language translation, can further improve its performance.\n",
      "\n",
      "Real-world applications of Transformer-based LLMs include natural language processing tasks such as language translation, text generation, sentiment analysis, and question-answering. These models have achieved state-of-the-art performance on various benchmark datasets and have been deployed in production systems by companies like Google, Facebook, and OpenAI.\n",
      "\n",
      "However, there are several technical challenges associated with Transformer-based LLMs. One major challenge is the large computational resources required to train and deploy these models, which can limit their scalability and accessibility. Additionally, fine-tuning these models on specific tasks may require large amounts of labeled data, which can be expensive and time-consuming to acquire.\n",
      "\n",
      "Overall, the Transformer architecture in LLMs represents a significant advancement in AI/ML research, enabling the development of more powerful and versatile language models that can tackle a wide range of natural language processing tasks. Despite the technical challenges, the potential of Transformer-based LLMs to revolutionize the field of AI is immense, and ongoing research efforts continue to push the boundaries of what is possible with these models.\n",
      "\n",
      "---\n",
      "\n",
      "# Key Technical Analysis\n",
      "\n",
      "## Core Concept Summary\n",
      "- **Transformer Architecture**: Key advancement in AI/ML for processing sequential data with parallelization and attention mechanisms.\n",
      "- **Self-Attention Mechanisms**: Allows the model to weigh the importance of input tokens for capturing long-range dependencies.\n",
      "\n",
      "## Critical Implementation Insights\n",
      "- **Training**: Utilizes techniques like stochastic gradient descent and backpropagation on large text datasets.\n",
      "- **Pre-training and Fine-tuning**: Pre-trained on unsupervised methods and fine-tuned for specific tasks like text classification or translation.\n",
      "\n",
      "## Key Challenges & Solutions\n",
      "- **Computational Resources**: Large resources required for training and deployment, limiting scalability. Solution: Optimize algorithms for efficiency.\n",
      "- **Data Requirements**: Fine-tuning may need large labeled data, expensive and time-consuming. Solution: Explore semi-supervised or transfer learning approaches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def run_analysis(topic: str):\n",
    "    result = analysis_pipeline.invoke({\"topic\": topic})\n",
    "\n",
    "    print(\"Analysis Timestamp:\", result[\"timestamp\"])\n",
    "    print(\"\\nTopic:\", result[\"topic\"])\n",
    "    print(\"\\nFormatted Output:\", result[\"formatted_output\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis(\"Transformer attention mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Evaluation Pipeline\n",
    "\n",
    "In this section, we'll add structured evaluation capabilities to our pipeline, including proper error handling and validation.\n",
    "\n",
    "**Features**\n",
    "- Structured Output: Using schema-based parsing\n",
    "- Validation: Input and output validation\n",
    "- Error Management: Comprehensive error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T08:00:45.859918Z",
     "start_time": "2025-01-12T08:00:38.280834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Status: success\n",
      "\n",
      "Evaluation Results: {\n",
      "  \"technical_evaluation\": {\n",
      "    \"core_technical_concepts\": [\n",
      "      \"Transformer model architecture\",\n",
      "      \"Attention mechanisms\",\n",
      "      \"Self-attention mechanisms\",\n",
      "      \"Long-range dependencies\",\n",
      "      \"Variable-length input sequences\"\n",
      "    ],\n",
      "    \"implementation_details\": \"The content provides a clear explanation of how attention mechanisms work in the Transformer model, specifically focusing on self-attention mechanisms and their advantages over traditional recurrent neural networks. It also highlights the ability of the Transformer model to handle variable-length input sequences efficiently.\",\n",
      "    \"quality_metrics\": {\n",
      "      \"clarity\": 9,\n",
      "      \"depth\": 8,\n",
      "      \"relevance\": 10,\n",
      "      \"accuracy\": 9\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Processing Time: 8.35 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Structured Evaluation Pipeline\n",
    "\n",
    "This demonstrates:\n",
    "1. Custom output parsing with schema validation\n",
    "2. Error handling at each pipeline stage\n",
    "3. Comprehensive validation system\n",
    "\"\"\"\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Step 1: Define structured output schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"technical_evaluation\",\n",
    "        description=\"Technical evaluation of the content\",\n",
    "        type=\"object\",\n",
    "        properties={\n",
    "            \"core_concepts\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Key technical concepts identified\"\n",
    "            },\n",
    "            \"implementation_details\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"complexity\": {\"type\": \"string\"},\n",
    "                    \"requirements\": {\"type\": \"array\"},\n",
    "                    \"challenges\": {\"type\": \"array\"}\n",
    "                }\n",
    "            },\n",
    "            \"quality_metrics\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"technical_accuracy\": {\"type\": \"number\"},\n",
    "                    \"completeness\": {\"type\": \"number\"},\n",
    "                    \"clarity\": {\"type\": \"number\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "evaluation_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Step 2: Create basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 3: Create analysis chain\n",
    "analysis_prompt = RunnableLambda(lambda x: f\"\"\"Analyze the following content: {x['generated_content']}\"\"\")\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[analysis_prompt],\n",
    "    last=ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "# Step 4: Create evaluation chain\n",
    "evaluation_prompt = RunnableLambda(\n",
    "    lambda x: f\"\"\"\n",
    "    Evaluate the following AI technical content:\n",
    "    {x['generated_content']}\n",
    "    \n",
    "    Provide a structured evaluation following these criteria:\n",
    "    1. Identify and list core technical concepts\n",
    "    2. Assess implementation details\n",
    "    3. Rate quality metrics (1-10)\n",
    "    \n",
    "    {evaluation_parser.get_format_instructions()}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "evaluation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[evaluation_prompt, ChatOpenAI(temperature=0)],\n",
    "    last=evaluation_parser\n",
    ")\n",
    "\n",
    "# Helper function for error handling\n",
    "def try_or_error(func, error_list):\n",
    "    try:\n",
    "        return func()\n",
    "    except Exception as e:\n",
    "        error_list.append(str(e))\n",
    "        return None\n",
    "\n",
    "# Step 5: Create pipeline components\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"errors\": [],\n",
    "        \"start_time\": time.time()\n",
    "    }\n",
    "\n",
    "def generate_content(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"generated_content\": try_or_error(\n",
    "            lambda: basic_generation_chain.invoke({\"topic\": x[\"topic\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_analysis(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"analysis\": try_or_error(\n",
    "            lambda: analysis_chain.invoke({\"generated_content\": x[\"generated_content\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_evaluation(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"evaluation\": try_or_error(\n",
    "            lambda: evaluation_chain.invoke(x),\n",
    "            x[\"errors\"]\n",
    "        ) if not x[\"errors\"] else None\n",
    "    }\n",
    "\n",
    "def finalize_output(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"completion_time\": time.time() - x[\"start_time\"],\n",
    "        \"status\": \"success\" if not x[\"errors\"] else \"error\"\n",
    "    }\n",
    "\n",
    "# Step 6: Create integrated pipeline\n",
    "def create_evaluation_pipeline():\n",
    "    return RunnableSequence(\n",
    "        first=RunnableLambda(init_state),\n",
    "        middle=[\n",
    "            RunnableLambda(generate_content),\n",
    "            RunnableLambda(perform_analysis),\n",
    "            RunnableLambda(perform_evaluation)\n",
    "        ],\n",
    "        last=RunnableLambda(finalize_output)\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "def demonstrate_evaluation():\n",
    "    pipeline = create_evaluation_pipeline()\n",
    "    result = pipeline.invoke({\"topic\": \"Transformer attention mechanisms\"})\n",
    "\n",
    "    print(\"Pipeline Status:\", result[\"status\"])\n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(\"\\nEvaluation Results:\", json.dumps(result[\"evaluation\"], indent=2))\n",
    "    else:\n",
    "        print(\"\\nErrors Encountered:\", result[\"errors\"])\n",
    "\n",
    "    print(f\"\\nProcessing Time: {result['completion_time']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the RunnableBranch\n",
    "\n",
    "`RunnableBranch` is a powerful tool that allows dynamic routing of logic based on input. It enables developers to flexibly define different processing paths depending on the characteristics of the input data.  \n",
    "\n",
    "`RunnableBranch` helps implement complex decision trees in a simple and intuitive way. This greatly improves code readability and maintainability while promoting logic modularization and reusability.  \n",
    "\n",
    "Additionally, `RunnableBranch` can dynamically evaluate branching conditions at runtime and select the appropriate processing routine, enhancing the system's adaptability and scalability.  \n",
    "\n",
    "Due to these features, `RunnableBranch` can be applied across various domains and is particularly useful for developing applications with high input data variability and volatility.\n",
    "\n",
    "By effectively utilizing `RunnableBranch`, developers can reduce code complexity and improve system flexibility and performance.\n",
    "\n",
    "### Dynamic Logic Routing Based on Input\n",
    "\n",
    "This section covers how to perform routing in LangChain Expression Language.\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. This helps bring structure and consistency to interactions with LLMs.\n",
    "\n",
    "There are two primary methods for performing routing:\n",
    "\n",
    "1. Returning a Conditionally Executable Object from `RunnableLambda` (*Recommended*)  \n",
    "2. Using `RunnableBranch`\n",
    "\n",
    "Both methods can be explained using a two-step sequence, where the first step classifies the input question as related to math, science, or other, and the second step routes it to the corresponding prompt chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the given user question into one of `math`, `science`, or `other`. Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain.\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()  # Use a string output parser.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the created chain to classify the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'math'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is 2+2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'science'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is the law of action and reaction?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'other'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda  \n",
    "\n",
    "`RunnableLambda` is a type of `Runnable` designed to simplify the execution of a single transformation or operation using a lambda (anonymous) function. \n",
    "\n",
    "It is primarily used for lightweight, stateless operations where defining an entire custom `Runnable` class would be overkill.  \n",
    "\n",
    "Unlike `RunnableBranch`, which focuses on conditional branching logic, `RunnableLambda` excels in straightforward data transformations or function applications.\n",
    "\n",
    "Syntax  \n",
    "- `RunnableLambda` is initialized with a single lambda function or callable object.  \n",
    "- When invoked, the input value is passed directly to the lambda function.  \n",
    "- The lambda function processes the input and returns the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create three sub-chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in math. \\\n",
    "Always answer questions starting with \"Pythagoras once said...\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "science_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in science. \\\n",
    "Always answer questions starting with \"Isaac Newton once said...\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question concisely:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Functions  \n",
    "\n",
    "This is the recommended approach in the official LangChain documentation. You can wrap custom functions with `RunnableLambda` to handle routing between different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return each chain based on the contents included in the topic.\n",
    "\n",
    "\n",
    "def route(info):\n",
    "    if \"math\" in info[\"topic\"].lower():\n",
    "        return math_chain\n",
    "    elif \"science\" in info[\"topic\"].lower():\n",
    "        return science_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")}\n",
    "    | RunnableLambda(\n",
    "        # Pass the routing function as an argument.\n",
    "        route\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythagoras once said, \"Mathematics is the language in which God has written the universe.\" In the realm of mathematics, calculus is a branch that allows us to understand change and motion. It consists primarily of two fundamental concepts: differentiation and integration.\\n\\nDifferentiation is concerned with how a function changes at any given point, essentially measuring the rate of change or the slope of a curve. This helps in understanding how quantities vary with respect to one another.\\n\\nOn the other hand, integration focuses on accumulating quantities, such as finding the area under a curve. It can be thought of as the reverse process of differentiation.\\n\\nTogether, these concepts enable us to solve complex problems related to motion, area, and other dynamic systems. Calculus serves as a foundational tool in fields ranging from physics to economics, allowing us to model and analyze real-world phenomena.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a math-related question.\n",
    "full_chain.invoke({\"question\": \"Please explain the concept of calculus.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Isaac Newton once said that gravity is a force that attracts two bodies toward each other, and he formulated the law of universal gravitation. Gravitational acceleration, specifically near the surface of the Earth, can be calculated using the equation \\\\( g = \\\\frac{GM}{r^2} \\\\), where \\\\( g \\\\) is the acceleration due to gravity, \\\\( G \\\\) is the universal gravitational constant, \\\\( M \\\\) is the mass of the Earth, and \\\\( r \\\\) is the distance from the center of the Earth to the point of measurement. Near the Earth's surface, this value is approximately \\\\( 9.81 \\\\, \\\\text{m/s}^2 \\\\).\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a science-related question.\n",
    "full_chain.invoke({\"question\": \"How is gravitational acceleration calculated?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval-Augmented Generation) is a framework that combines retrieval and generation techniques in natural language processing. It retrieves relevant information from a knowledge base and uses it to enhance the generation of responses, thereby improving the quality and relevance of the output. This approach is particularly useful in tasks like question answering and dialogue generation, where context and accurate information are critical.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a general question.\n",
    "full_chain.invoke({\"question\": \"What is RAG (Retrieval Augmented Generation)?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch\n",
    "\n",
    "`RunnableBranch` is a special type of `Runnable` that allows you to define conditions and corresponding Runnable objects based on input values.\n",
    "\n",
    "However, it does not provide functionality that cannot be achieved with custom functions, so using custom functions is generally recommended.\n",
    "\n",
    "Syntax\n",
    "\n",
    "- `RunnableBranch` is initialized with a list of (condition, Runnable) pairs and a default Runnable.\n",
    "- When invoked, the input value is passed to each condition sequentially.\n",
    "- The first condition that evaluates to True is selected, and the corresponding Runnable is executed with the input value.\n",
    "- If no condition matches, the `default Runnable` is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    # Check if the topic contains \"math\" and execute math_chain if true.\n",
    "    (lambda x: \"math\" in x[\"topic\"].lower(), math_chain),\n",
    "    # Check if the topic contains \"science\" and execute science_chain if true.\n",
    "    (lambda x: \"science\" in x[\"topic\"].lower(), science_chain),\n",
    "    # If none of the above conditions match, execute general_chain.\n",
    "    general_chain,\n",
    ")\n",
    "\n",
    "# Define the full chain that takes a topic and question, routes it, and parses the output.\n",
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")} | branch | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the full chain with each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pythagoras once said that the pursuit of knowledge is essential for understanding the world around us. Calculus is a branch of mathematics that focuses on how things change. It provides tools for modeling and analyzing dynamic systems and is primarily divided into two main concepts: differentiation and integration.\\n\\nDifferentiation is about finding the rate at which a quantity changes. For instance, if you have a curve representing the position of an object over time, the derivative of that curve gives you the object's velocity at any instant.\\n\\nIntegration, on the other hand, deals with accumulation. It allows us to calculate areas under curves, total quantities, or the sum of infinitely many infinitesimal parts. In essence, while differentiation breaks down a function to understand its behavior at specific points, integration builds up from those points to find total values.\\n\\nTogether, these concepts offer profound insights into not just mathematics, but also physics, engineering, economics, and other fields where change and accumulation are fundamental.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"Please explain the concept of calculus.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac Newton once said, \"What goes up must come down.\" To calculate gravitational acceleration, we use the formula \\\\( g = \\\\frac{G \\\\cdot M}{r^2} \\\\), where \\\\( g \\\\) is the acceleration due to gravity, \\\\( G \\\\) is the universal gravitational constant approximately equal to \\\\( 6.674 \\\\times 10^{-11} \\\\, \\\\text{N(m/kg)}^2 \\\\), \\\\( M \\\\) is the mass of the object attracting another object (like the Earth), and \\\\( r \\\\) is the distance from the center of mass of the attracting object to the center of the object being attracted (like an object on the surface of the Earth). On the surface of the Earth, this results in an approximate gravitational acceleration of \\\\( 9.81 \\\\, \\\\text{m/s}^2 \\\\).'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"How is gravitational acceleration calculated?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval Augmented Generation) is a machine learning approach that enhances natural language generation by integrating external information retrieval. It combines a generative model with a retrieval mechanism, allowing the system to fetch relevant documents or data from a knowledge base to improve the accuracy and relevance of the generated text. This method is particularly useful in generating responses that require factual knowledge or specific information.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"What is RAG (Retrieval Augmented Generation)?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an AI Learning Assistant\n",
    "\n",
    "Let's apply what we've learned about Runnable components to build a practical AI Learning Assistant. This system will help students by providing tailored responses based on their questions.\n",
    "\n",
    "First, let's set up our core components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnableBranch, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "\n",
    "# Question Classification Component\n",
    "question_classifier = RunnableSequence(\n",
    "    first=PromptTemplate.from_template(\n",
    "        \"\"\"Classify this question into one of: beginner, intermediate, advanced\n",
    "        Consider:\n",
    "        - Complexity of concepts\n",
    "        - Prior knowledge required\n",
    "        - Technical depth needed\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Return only the classification word in lowercase.\"\"\"\n",
    "    ),\n",
    "    middle=[ChatOpenAI(temperature=0)],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example Generator Component\n",
    "example_generator = RunnableSequence(\n",
    "    first=PromptTemplate.from_template(\n",
    "        \"\"\"Generate a practical example for this concept.\n",
    "        Level: {level}\n",
    "        Question: {question}\n",
    "        \n",
    "        If code is needed, provide it in appropriate markdown format.\"\"\"\n",
    "    ),\n",
    "    middle=[ChatOpenAI(temperature=0.7)],\n",
    "    last=StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create our response generation strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Generation Strategy\n",
    "response_strategy = RunnableBranch(\n",
    "    (\n",
    "        lambda x: x[\"level\"] == \"beginner\",\n",
    "        RunnableSequence(\n",
    "            first=PromptTemplate.from_template(\n",
    "                \"\"\"Explain in simple terms for a beginner:\n",
    "                Question: {question}\n",
    "                \n",
    "                Use simple analogies and avoid technical jargon.\"\"\"\n",
    "            ),\n",
    "            middle=[ChatOpenAI(temperature=0.3)],\n",
    "            last=StrOutputParser()\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        lambda x: x[\"level\"] == \"intermediate\",\n",
    "        RunnableSequence(\n",
    "            first=PromptTemplate.from_template(\n",
    "                \"\"\"Provide a detailed explanation with practical examples:\n",
    "                Question: {question}\n",
    "                \n",
    "                Include relevant technical concepts and use cases.\"\"\"\n",
    "            ),\n",
    "            middle=[ChatOpenAI(temperature=0.3)],\n",
    "            last=StrOutputParser()\n",
    "        )\n",
    "    ),\n",
    "    # Default case (advanced)\n",
    "    RunnableSequence(\n",
    "        first=PromptTemplate.from_template(\n",
    "            \"\"\"Give an in-depth technical explanation:\n",
    "            Question: {question}\n",
    "            \n",
    "            Include advanced concepts and detailed technical information.\"\"\"\n",
    "        ),\n",
    "        middle=[ChatOpenAI(temperature=0.3)],\n",
    "        last=StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our main pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(x):\n",
    "    return {\n",
    "        \"question\": x[\"question\"],\n",
    "        \"level\": x[\"level\"],\n",
    "        \"explanation\": x[\"response\"],\n",
    "        \"example\": x[\"example\"],\n",
    "        \"metadata\": {\n",
    "            \"difficulty\": x[\"level\"],\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Learning Assistant Pipeline\n",
    "learning_assistant = RunnableSequence(\n",
    "    first=RunnableLambda(lambda x: {\"question\": x[\"question\"]}),\n",
    "    middle=[\n",
    "        RunnableLambda(lambda x: {\n",
    "            **x,\n",
    "            \"level\": question_classifier.invoke({\"question\": x[\"question\"]})\n",
    "        }),\n",
    "        RunnableLambda(lambda x: {\n",
    "            **x,\n",
    "            \"response\": response_strategy.invoke(x),\n",
    "            \"example\": example_generator.invoke(x)\n",
    "        })\n",
    "    ],\n",
    "    last=RunnableLambda(format_response)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is a variable in Python?\n",
      "Difficulty Level: beginner\n",
      "\n",
      "Explanation: In Python, a variable is like a container that holds information. Just like a box can hold different things like toys or clothes, a variable can hold different types of data like numbers or words. You can give a variable a name so you can easily refer to it later in your code. Think of it as a label on a box that helps you find what you're looking for.\n",
      "\n",
      "Example: A variable in Python is a way to store and manipulate data within a program. It is essentially a placeholder that can hold different values.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "# Define a variable called 'name' and assign it the value \"Alice\"\n",
      "name = \"Alice\"\n",
      "\n",
      "# Print out the value stored in the variable 'name'\n",
      "print(name)\n",
      "```\n",
      "\n",
      "In this example, the variable `name` is storing the string \"Alice\". The `print` statement then outputs the value of the variable, which is \"Alice\".\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: How does dependency injection work?\n",
      "Difficulty Level: intermediate\n",
      "\n",
      "Explanation: Dependency injection is a design pattern used in software development to achieve loose coupling between classes and their dependencies. It allows the dependencies of a class to be injected from the outside rather than being created within the class itself. This makes the code more modular, testable, and maintainable.\n",
      "\n",
      "There are three common types of dependency injection: constructor injection, setter injection, and interface injection.\n",
      "\n",
      "1. Constructor Injection:\n",
      "In constructor injection, dependencies are provided through a class constructor. This is the most common type of dependency injection and ensures that all required dependencies are provided when an object is created. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public class UserService {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    public UserService(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods using userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "2. Setter Injection:\n",
      "In setter injection, dependencies are provided through setter methods. This allows for more flexibility as dependencies can be changed at runtime. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public class UserService {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    public void setUserRepository(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods using userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "3. Interface Injection:\n",
      "In interface injection, the class implements an interface that defines methods to set dependencies. This allows for more control over how dependencies are injected. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public interface UserRepositorySetter {\n",
      "    void setUserRepository(UserRepository userRepository);\n",
      "}\n",
      "\n",
      "public class UserService implements UserRepositorySetter {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    @Override\n",
      "    public void setUserRepository(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods using userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "Dependency injection is commonly used in frameworks like Spring and Angular to manage dependencies in a more organized and efficient way. It allows for easier testing by enabling the use of mock objects for dependencies, and it promotes the principle of inversion of control, where the control of object creation and lifecycle is delegated to an external entity.\n",
      "\n",
      "Overall, dependency injection is a powerful tool in software development that helps improve code quality, maintainability, and scalability by reducing tight coupling between classes and their dependencies.\n",
      "\n",
      "Example: Dependency injection is a design pattern in which the dependencies of a class are provided from the outside rather than created within the class itself. This allows for easier testing, reusability, and flexibility in the code.\n",
      "\n",
      "Here is a practical example in Python:\n",
      "\n",
      "```python\n",
      "# Define a class that has a dependency on another class\n",
      "class Logger:\n",
      "    def log(self, message):\n",
      "        print(message)\n",
      "\n",
      "class UserManager:\n",
      "    def __init__(self, logger):\n",
      "        self.logger = logger\n",
      "\n",
      "    def register_user(self, username):\n",
      "        self.logger.log(f\"User {username} registered successfully\")\n",
      "\n",
      "# Create an instance of the Logger class\n",
      "logger = Logger()\n",
      "\n",
      "# Create an instance of the UserManager class and pass in the Logger instance as a dependency\n",
      "user_manager = UserManager(logger)\n",
      "\n",
      "# Use the UserManager instance to register a user\n",
      "user_manager.register_user(\"John Doe\")\n",
      "```\n",
      "\n",
      "In this example, the `UserManager` class has a dependency on the `Logger` class. Instead of creating an instance of the `Logger` class within the `UserManager` class, we pass in an instance of the `Logger` class during the initialization of the `UserManager` class. This allows us to easily switch out the `Logger` implementation or mock it for testing purposes.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: Explain quantum computing qubits\n",
      "Difficulty Level: intermediate\n",
      "\n",
      "Explanation: Quantum computing qubits are the fundamental building blocks of quantum computers. Unlike classical computers that use bits to represent information as either a 0 or 1, quantum computers use qubits, which can represent both 0 and 1 simultaneously due to the principles of superposition and entanglement in quantum mechanics.\n",
      "\n",
      "Superposition allows qubits to exist in multiple states at the same time, enabling quantum computers to perform multiple calculations simultaneously. This parallel processing capability gives quantum computers the potential to solve complex problems much faster than classical computers.\n",
      "\n",
      "Entanglement is another key property of qubits, where the state of one qubit is dependent on the state of another, even when they are physically separated. This allows quantum computers to perform operations on multiple qubits simultaneously, leading to exponential speedup in certain computational tasks.\n",
      "\n",
      "One practical example of quantum computing qubits is in the field of cryptography. Quantum computers have the potential to break traditional encryption methods, such as RSA, due to their ability to quickly factor large numbers. Quantum-resistant encryption algorithms, such as lattice-based cryptography, are being developed to secure data in the age of quantum computing.\n",
      "\n",
      "Another use case for quantum computing qubits is in optimization problems, such as finding the most efficient route for delivery trucks or optimizing financial portfolios. Quantum computers can explore a vast number of possible solutions simultaneously, leading to faster and more accurate results compared to classical algorithms.\n",
      "\n",
      "In conclusion, quantum computing qubits are the key to unlocking the potential of quantum computers. Their unique properties of superposition and entanglement enable quantum computers to perform complex calculations at speeds that are unattainable with classical computers. As research and development in quantum computing continue to advance, we can expect to see more applications and breakthroughs in various fields.\n",
      "\n",
      "Example: A practical example of quantum computing qubits can be seen in the creation of a quantum encryption system. In this system, traditional binary bits are replaced with qubits, which can exist in multiple states simultaneously due to superposition.\n",
      "\n",
      "Using qubits, it is possible to create encryption keys that are exponentially more complex and secure compared to traditional encryption methods. This is because qubits can represent multiple values at once, allowing for the generation of truly random and unbreakable encryption keys.\n",
      "\n",
      "Here is a simplified example of how qubits can be used in a quantum encryption system:\n",
      "\n",
      "```python\n",
      "from qiskit import QuantumCircuit, Aer, execute\n",
      "\n",
      "# Create a quantum circuit with 2 qubits\n",
      "circuit = QuantumCircuit(2)\n",
      "\n",
      "# Apply a superposition to both qubits\n",
      "circuit.h(0)\n",
      "circuit.h(1)\n",
      "\n",
      "# Measure the qubits\n",
      "circuit.measure_all()\n",
      "\n",
      "# Simulate the circuit using a quantum simulator\n",
      "simulator = Aer.get_backend('qasm_simulator')\n",
      "result = execute(circuit, simulator).result()\n",
      "\n",
      "# Get the measurement results\n",
      "counts = result.get_counts(circuit)\n",
      "print(counts)\n",
      "```\n",
      "\n",
      "In this example, the qubits are put into a superposition state using the Hadamard gate (represented by `circuit.h()`), which allows them to exist in multiple states simultaneously. The qubits are then measured, and the measurement results are obtained using a quantum simulator.\n",
      "\n",
      "By leveraging the unique properties of qubits, quantum encryption systems can provide a higher level of security and privacy for sensitive data and communications.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "async def run_assistant():\n",
    "    # Example questions for different levels\n",
    "    questions = [\n",
    "        \"What is a variable in Python?\",\n",
    "        \"How does dependency injection work?\",\n",
    "        \"Explain quantum computing qubits\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = await learning_assistant.ainvoke({\"question\": question})\n",
    "        print(f\"\\nQuestion: {result['question']}\")\n",
    "        print(f\"Difficulty Level: {result['level']}\")\n",
    "        print(f\"\\nExplanation: {result['explanation']}\")\n",
    "        print(f\"\\nExample: {result['example']}\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# For Jupyter environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_assistant())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of RunnableSequence, RunnableBranch, and RunnableLambda\n",
    "\n",
    "| Criteria | RunnableSequence | RunnableBranch | RunnableLambda |\n",
    "|----------|------------------|----------------|----------------|\n",
    "| Primary Purpose | Sequential pipeline processing | Conditional routing and branching | Simple transformations and functions |\n",
    "| Condition Definition | No conditions, sequential flow | Each condition defined as `(condition, runnable)` pair | All conditions within single function (`route`) |\n",
    "| Structure | Linear chain of operations | Tree-like branching structure | Function-based transformation |\n",
    "| Readability | Very clear for sequential processes | Becomes clearer as conditions increase | Very clear for simple logic |\n",
    "| Maintainability | Easy to maintain step-by-step flow | Clear separation between conditions and runnables | Can become complex if function grows large |\n",
    "| Flexibility | Flexible for linear processes | Must follow `(condition, runnable)` pattern | Allows flexible condition writing |\n",
    "| Scalability | Add or modify pipeline steps | Requires adding new conditions and runnables | Expandable by modifying function |\n",
    "| Error Handling | Pipeline-level error management | Branch-specific error handling | Basic error handling |\n",
    "| State Management | Maintains state throughout pipeline | State managed per branch | Typically stateless |\n",
    "| Recommended Use Case | When you need ordered processing steps | When there are many conditions or maintainability is priority | When conditions are simple or function-based |\n",
    "| Complexity Level | Medium to High | Medium | Low |\n",
    "| Async Support | Full async support | Limited async support | Basic async support |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
