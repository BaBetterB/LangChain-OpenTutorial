{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Based Smart Web Search\n",
    "\n",
    "- Author: [sysin0116](https://github.com/syshin0116)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/07-Agent/15-CoT-basedSmartWebSearch.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/07-Agent/15-CoT-basedSmartWebSearch.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates a chain-of-thought (CoT) based Smart Web Search approach to build a plan-and-execute style QA chatbot. We will also explore the LLMCompiler method for speed optimization.\n",
    "\n",
    "### Key Features\n",
    "- Chain-of-Thought guided query expansion and reasoning\n",
    "- Dynamic plan-and-execute system for multi-step web search\n",
    "- Integration with LLMCompiler for accelerated pipeline\n",
    "\n",
    "\n",
    "\n",
    "## LLM Compiler\n",
    "\n",
    "- Proposed by Kim et al. as a high-speed, execution-oriented agent architecture\n",
    "- Core Components\n",
    "    - **Planner**\n",
    "        - Generates tasks in a DAG (Directed Acyclic Graph) format via streaming\n",
    "    - **Task Fetching Unit**\n",
    "        - Consumes the Planner’s list of tasks, executing each as soon as dependencies are met\n",
    "        - Supports parallel execution\n",
    "    - **Joiner**\n",
    "        - Collects results from all tasks and decides whether to produce a final answer or replan\n",
    "- Key Characteristics\n",
    "    - Allows streaming of tasks so certain tasks can be executed even before the Planner finishes building out the full plan\n",
    "    - Executes all dependency-free tasks in parallel to reduce overall runtime\n",
    "    - Provides fast responses with minimal large-model usage\n",
    "\n",
    "\n",
    "### What is a DAG?\n",
    "\n",
    "A **DAG (Directed Acyclic Graph)** is a common structure in computing and data processing\n",
    "\n",
    "- **Directed**\n",
    "    - Every edge in the graph has a direction\n",
    "    - For instance, A → B means you can only move from A to B\n",
    "- **Acyclic**\n",
    "    - The graph contains no cycles\n",
    "    - There is no path that starts at one node and eventually returns to it\n",
    "- **Graph**\n",
    "    - Composed of nodes and edges\n",
    "    - Nodes represent data or tasks, and edges define how they relate to or depend on each other\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [...](#...)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models](https://arxiv.org/abs/2310.04406)\n",
    "- [Building (and Breaking) WebLangChain](https://blog.langchain.dev/weblangchain/)\n",
    "- [Plan-and-Execute Agents](https://blog.langchain.dev/planning-agents/)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
    "\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
    "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰어님들을 위한 설명\n",
    "\n",
    "### 튜토리얼 주요 목적\n",
    "- Perplexity와 유사한 CoT 기반 검색 QA(Chatbot) 구현\n",
    "- Plan-Execute 방식을 바탕으로 LLMCompiler를 추가 적용하여 속도 최적화 시도\n",
    "\n",
    "### 현재 코드 상태\n",
    "- 코드가 두 부분으로 나뉘어 있음 (LLMCompiler 방식 / Plan & Schedule 방식)\n",
    "\n",
    "### 추가 예정 항목\n",
    "- Plan & Schedule 방식에 LLM Compiler 코드 통합\n",
    "- LangChain 공식 Config 설정 방식으로 변경\n",
    "- 설명, 주석 추가\n",
    "- Prompt 수정\n",
    "- Summarizer, Output Parser 고도화 및 최종 QA 결과 최적화\n",
    "- LangSmith 오류 해결\n",
    "\n",
    "    `Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)`\n",
    "\n",
    "### 추가 가능성 항목\n",
    "- VectorDB 검색, Knowledge Graph 검색, Web Search API 등 툴 추가\n",
    "- 이미지 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"COT-based-smart-websearch\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
    "ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
    "# $1 or ${1} -> 1\n",
    "ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "END_OF_PLAN = \"<END_OF_PLAN>\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "\n",
    "def _ast_parse(arg: str) -> Any:\n",
    "    try:\n",
    "        return ast.literal_eval(arg)\n",
    "    except:  # noqa\n",
    "        return arg\n",
    "\n",
    "\n",
    "def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> list[Any]:\n",
    "    \"\"\"Parse arguments from a string.\"\"\"\n",
    "    if args == \"\":\n",
    "        return ()\n",
    "    if isinstance(tool, str):\n",
    "        return ()\n",
    "    extracted_args = {}\n",
    "    tool_key = None\n",
    "    prev_idx = None\n",
    "    for key in tool.args.keys():\n",
    "        # Split if present\n",
    "        if f\"{key}=\" in args:\n",
    "            idx = args.index(f\"{key}=\")\n",
    "            if prev_idx is not None:\n",
    "                extracted_args[tool_key] = _ast_parse(\n",
    "                    args[prev_idx:idx].strip().rstrip(\",\")\n",
    "                )\n",
    "            args = args.split(f\"{key}=\", 1)[1]\n",
    "            tool_key = key\n",
    "            prev_idx = 0\n",
    "    if prev_idx is not None:\n",
    "        extracted_args[tool_key] = _ast_parse(\n",
    "            args[prev_idx:].strip().rstrip(\",\").rstrip(\")\")\n",
    "        )\n",
    "    return extracted_args\n",
    "\n",
    "\n",
    "def default_dependency_rule(idx, args: str):\n",
    "    matches = re.findall(ID_PATTERN, args)\n",
    "    numbers = [int(match) for match in matches]\n",
    "    return idx in numbers\n",
    "\n",
    "\n",
    "def _get_dependencies_from_graph(\n",
    "    idx: int, tool_name: str, args: Dict[str, Any]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Get dependencies from a graph.\"\"\"\n",
    "    if tool_name == \"join\":\n",
    "        return list(range(1, idx))\n",
    "    return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
    "\n",
    "\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "\n",
    "\n",
    "def instantiate_task(\n",
    "    tools: Sequence[BaseTool],\n",
    "    idx: int,\n",
    "    tool_name: str,\n",
    "    args: Union[str, Any],\n",
    "    thought: Optional[str] = None,\n",
    ") -> Task:\n",
    "    if tool_name == \"join\":\n",
    "        tool = \"join\"\n",
    "    else:\n",
    "        try:\n",
    "            tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
    "        except ValueError as e:\n",
    "            raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
    "    tool_args = _parse_llm_compiler_action_args(args, tool)\n",
    "    dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
    "\n",
    "    return Task(\n",
    "        idx=idx,\n",
    "        tool=tool,\n",
    "        args=tool_args,\n",
    "        dependencies=dependencies,\n",
    "        thought=thought,\n",
    "    )\n",
    "\n",
    "\n",
    "class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
    "    \"\"\"Planning output parser.\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
    "        texts = []\n",
    "        # TODO: Cleanup tuple state tracking here.\n",
    "        thought = None\n",
    "        for chunk in input:\n",
    "            # Assume input is str. TODO: support vision/other formats\n",
    "            text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
    "            for task, thought in self.ingest_token(text, texts, thought):\n",
    "                yield task\n",
    "        # Final possible task\n",
    "        if texts:\n",
    "            task, _ = self._parse_task(\"\".join(texts), thought)\n",
    "            if task:\n",
    "                yield task\n",
    "\n",
    "    def parse(self, text: str) -> List[Task]:\n",
    "        return list(self._transform([text]))\n",
    "\n",
    "    def stream(\n",
    "        self,\n",
    "        input: str | BaseMessage,\n",
    "        config: RunnableConfig | None = None,\n",
    "        **kwargs: Any | None,\n",
    "    ) -> Iterator[Task]:\n",
    "        yield from self.transform([input], config, **kwargs)\n",
    "\n",
    "    def ingest_token(\n",
    "        self, token: str, buffer: List[str], thought: Optional[str]\n",
    "    ) -> Iterator[Tuple[Optional[Task], str]]:\n",
    "        buffer.append(token)\n",
    "        if \"\\n\" in token:\n",
    "            buffer_ = \"\".join(buffer).split(\"\\n\")\n",
    "            suffix = buffer_[-1]\n",
    "            for line in buffer_[:-1]:\n",
    "                task, thought = self._parse_task(line, thought)\n",
    "                if task:\n",
    "                    yield task, thought\n",
    "            buffer.clear()\n",
    "            buffer.append(suffix)\n",
    "\n",
    "    def _parse_task(self, line: str, thought: Optional[str] = None):\n",
    "        task = None\n",
    "        if match := re.match(THOUGHT_PATTERN, line):\n",
    "            # Optionally, action can be preceded by a thought\n",
    "            thought = match.group(1)\n",
    "        elif match := re.match(ACTION_PATTERN, line):\n",
    "            # if action is parsed, return the task, and clear the buffer\n",
    "            idx, tool_name, args, _ = match.groups()\n",
    "            idx = int(idx)\n",
    "            task = instantiate_task(\n",
    "                tools=self.tools,\n",
    "                idx=idx,\n",
    "                tool_name=tool_name,\n",
    "                args=args,\n",
    "                thought=thought,\n",
    "            )\n",
    "            thought = None\n",
    "        # Else it is just dropped\n",
    "        return task, thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Fetching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps\n",
    "                and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    }
   ],
   "source": [
    "tool_messages = plan_and_schedule.invoke(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content=\"[{'url': 'https://www.wunderground.com/weather/us/ca/san-francisco', 'content': 'San Francisco, CA Weather Conditions | Weather Underground San Francisco, CA Weather Conditions_star_rate__home_ 56\\\\xa0°F South of Market Station|Report Report Station You are about to report this weather station for bad data. Personal Weather Station Nearby Weather Stations Nearby Weather Stations The time period when the sun is no more than 6 degrees below the horizon at either sunrise or sunset. The time period when the sun is between 6 and 12 degrees below the horizon at either sunrise or sunset. The time period when the sun is between 12 and 18 degrees below the horizon at either sunrise or sunset. The sun does not contribute to the illumination of the sky before this time in the morning, or after this time in the evening.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in San Francisco'}}, response_metadata={}, name='tavily_search_results_json', tool_call_id=1),\n",
       " FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', tool_call_id=2)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "runnable = joiner_prompt | llm.with_structured_output(\n",
    "    JoinOutputs, method=\"function_calling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content=\"Thought: The temperature in San Francisco is currently 56°F. To answer the user's question, we just need to calculate 56 raised to the 3rd power.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The temperature in San Francisco raised to the 3rd power is 56^3 = 175616.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joiner.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# # Visualize the compiled StateGraph as a Mermaid diagram\n",
    "# display(\n",
    "#     Image(\n",
    "#         chain.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'url\\': \\'https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/\\', \\'content\\': \"What is the gross domestic product (GDP) in New York? What is the gross domestic product (GDP) in New York? Gross domestic product (GDP) measures the value of goods and services a country or state produces — it’s the sum of consumer spending, business investment, government spending, and net exports. As of 2023, the real GDP was $1.8 trillion. Real GDP in New York, adjusted for inflation (chained 2017 dollars) GDP and the economic experience vary by location due to factors like cost of living, population density, workforce education, and the area’s main industries. In 2023, New York\\'s real (that is, inflation-adjusted) GDP per person was 1st out of all 50 states. In 2023, New York ranked 1st in state GDP per person.\"}]', additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York 2023'}}, response_metadata={}, name='tavily_search_results_json', id='bb64d2f5-7b96-4221-8bd3-63199991d488', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='23690888-044a-45ad-8577-56bca9213738', tool_call_id=2)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The search result provides the information needed to answer the user's question. The GDP of New York in 2023 is $1.8 trillion.\", additional_kwargs={}, response_metadata={}, id='6682ddbb-6a69-44dc-b7d9-fce28f7bf797'), AIMessage(content='The GDP of New York in 2023 is $1.8 trillion.', additional_kwargs={}, response_metadata={}, id='6f803c36-ffee-4ca1-952a-e81c92d056e8')]}}\n",
      "---\n",
      "The GDP of New York in 2023 is $1.8 trillion.\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")\n",
    "\n",
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-hop question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.birdful.org/is-wisdom-the-oldest-bird-still-alive/', 'content': '2002 – Aged 51 years, confirmed as oldest wild bird * Wisdom the Laysan albatross, age 67+ as of 2023 Wisdom’s 67+ years as a wild Laysan albatross have been well documented by scientists. Banded as oldest wild bird in world in 2002 at age 51 As the oldest known wild bird currently alive, Wisdom serves as a remarkable symbol of avian endurance and resistance to aging. After surviving over 60 years in a harsh ocean environment, raising chick after chick, and enduring storms, pollution, disruptions, and all the trials of life in the wild, Wisdom has more than earned her name. As the world’s oldest known wild bird, Wisdom defies scientists’ understanding of bird aging and reproductive lifespans.'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive 2023'}}, response_metadata={}, name='tavily_search_results_json', id='b47f45c2-3c9d-40a0-bd06-11a9b4884712', tool_call_id=1), FunctionMessage(content=\"[{'url': 'https://www.herebird.com/how-long-do-parrots-live/', 'content': 'According to the Guinness Book of Records, the world record for the oldest parrot of note is Cookie, a Major Mitchell’s Cockatoo who passed away at the grand old age of 83 years:\\\\nAnother contender to the throne is Duster, an Umbrella-Crested Cockatoo which is currently recorded at 89 year’s old and can trace back it’s original age back to a time when it was featured in a local news segment:\\\\nOther Long Living Parrots\\\\nAnother celebrated parrot is Tarbu, a privately owned African Grey that lived to an age of 55. Below are a number of things you can do to increase the health and longevity of your birds:\\\\nWild vs Captivity Lifespans\\\\nIn theory a parrot should be able to live longer in captivity than it does in the wild. If both of these avenues fail then you can do some simple observations that will help you to gauge the age of the bird:\\\\nHealth Issues for Older Birds\\\\nWhilst parrots do live for a long time, they still develop a number of health issues as they age. Image credit betcsbirds\\\\nSenegal Parrot Lifespan\\\\nOriginally from the west coast of Africa and from the Poicephalus family of birds, the Senegal Parrot is likely to live 40 years.\\\\n Click on the image below to see a full sized version\\\\nAverage Lifespan of a Parrot\\\\nLet’s look at some of the popular parrot species that you are likely to encounter.'}]\", additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of a parrot'}}, response_metadata={}, name='tavily_search_results_json', id='4c74866e-ae8e-4dff-92ef-a998bb859f8a', tool_call_id=2), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', id='041e5cc6-c337-40d8-95aa-6d5c6162a183', tool_call_id=3)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The first result erroneously refers to a Laysan albatross, which is not a parrot. The second result provides relevant information, identifying Duster, an Umbrella-Crested Cockatoo, as the current oldest parrot at 89 years old. However, the average lifespan of a parrot isn't directly provided, but it mentions that a Senegal Parrot, as an example, is likely to live 40 years, which isn't necessarily representative of all parrots' average lifespan.\", additional_kwargs={}, response_metadata={}, id='299a38bd-8e65-40bd-9744-5329be777d15'), SystemMessage(content='Context from last attempt: The task requires information on the oldest parrot alive and how much longer that is than the average lifespan of parrots. While we found the oldest parrot (Duster, 89 years old), the search did not provide a clear average lifespan for parrots in general to make the comparison. Information on the average lifespan of parrots is needed to accurately answer the question.', additional_kwargs={}, response_metadata={}, id='7727eb0c-18a5-4d31-b7d1-14fd9db34918')]}}\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'url\\': \\'https://pangovet.com/pet-health-wellness/birds/parrot-lifespan-how-long-do-they-live/\\', \\'content\\': \"A parrot\\'s lifespan is usually based on their species. That being said, most parrots can easily live for about 15-20 years; the larger parrots can more than double this estimate. Parrot owners need to be aware of the longevity of their birds so they can be prepared to provide the proper care throughout the animal\\'s lifetime.\"}]', additional_kwargs={'idx': 4, 'args': {'query': 'average lifespan of parrots'}}, response_metadata={}, name='tavily_search_results_json', id='e4d8e17e-de9f-4765-b4bf-cb0e596396da', tool_call_id=4), FunctionMessage(content='join', additional_kwargs={'idx': 5, 'args': ()}, response_metadata={}, name='join', id='6e4e2a47-ce33-4e04-aee2-3691dd801e98', tool_call_id=5)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The latest search result provides a general range for the lifespan of most parrots, stating they can live about 15-20 years, with larger parrots potentially living much longer. Using this average lifespan range and comparing it to Duster's age of 89 years, we can deduce the difference.\", additional_kwargs={}, response_metadata={}, id='71b1b319-bc18-4985-8425-aded5199eb71'), AIMessage(content=\"The oldest parrot alive is Duster, an Umbrella-Crested Cockatoo, at 89 years old. Comparing Duster's age to the average lifespan of parrots, which is about 15-20 years for most species, Duster has lived approximately 69-74 years longer than the average parrot lifespan.\", additional_kwargs={}, response_metadata={}, id='b244aea6-3341-4b9a-969e-3fc72bb7f5e4')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "steps = chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"recursion_limit\": 100,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The oldest parrot alive is Duster, an Umbrella-Crested Cockatoo, at 89 years old. Comparing Duster's age to the average lifespan of parrots, which is about 15-20 years for most species, Duster has lived approximately 69-74 years longer than the average parrot lifespan.\n"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.timeanddate.com/weather/south-korea/seoul/ext', 'content': 'Seoul, South Korea 14 day weather forecast Time Zone News Calendar & Holiday News Time Zones Time Zones Home Time Changes Worldwide Time Zone News Calendars Home Calendar & Holiday News Weather Sun & Moon Home Sun Calculator Moon Calculator Countdown for Your Site Calculators Calculators Home Home \\\\xa0 Weather \\\\xa0 South Korea \\\\xa0 Seoul \\\\xa0 Two-week forecast Weather Time Zone °F Last 2 weeks of weather Nov 19 |  | 51 / 28\\\\xa0°F | Overcast. Nov 20 |  | 51 / 30\\\\xa0°F | Cloudy. | * Updated Tuesday, November 19, 2024 12:24:28 am Seoul time - Weather by CustomWeather, © 2024 | Hour-by-hour weather for Seoul next 7 days Sun & Moon times precise to the second. Time Zones Weather Calculators'}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in Seoul'}}, response_metadata={}, name='tavily_search_results_json', id='26d2e995-4b51-42c3-b822-57a310544a53', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='e3b95081-e6a3-48e7-91cb-2e698fb2661f', tool_call_id=2)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: The provided search result contains the temperature for Seoul for November 19 and November 20, 2024, but it does not include the current temperature. This information is not sufficient to create a flashcard summarizing the current temperature in Seoul.', additional_kwargs={}, response_metadata={}, id='e3e24a1b-6067-49e9-b7b4-af87e06a85ea'), SystemMessage(content=\"Context from last attempt: The search result does not provide the current temperature in Seoul, which is necessary to answer the user's question. A direct source of current weather conditions or real-time weather data is needed to obtain this information. Consider searching a weather-specific source or API that offers current temperature readings.\", additional_kwargs={}, response_metadata={}, id='5145cf99-ab18-46e6-a545-078093bc3d4d')]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'url': 'https://www.timeanddate.com/weather/south-korea/seoul/ext', 'content': 'Seoul, South Korea 14 day weather forecast Time Zone News Calendar & Holiday News Time Zones Time Zones Home Time Changes Worldwide Time Zone News Calendars Home Calendar & Holiday News Weather Sun & Moon Home Sun Calculator Moon Calculator Countdown for Your Site Calculators Calculators Home Home \\\\xa0 Weather \\\\xa0 South Korea \\\\xa0 Seoul \\\\xa0 Two-week forecast Weather Time Zone °F Last 2 weeks of weather Nov 19 |  | 51 / 28\\\\xa0°F | Overcast. Nov 20 |  | 51 / 30\\\\xa0°F | Cloudy. | * Updated Tuesday, November 19, 2024 12:24:28 am Seoul time - Weather by CustomWeather, © 2024 | Hour-by-hour weather for Seoul next 7 days Sun & Moon times precise to the second. Time Zones Weather Calculators'}]\", additional_kwargs={'idx': 3, 'args': {'query': 'current temperature in Seoul'}}, response_metadata={}, name='tavily_search_results_json', id='bc9f013d-70f1-4c0d-8365-4c0fce4385a6', tool_call_id=3), FunctionMessage(content='join', additional_kwargs={'idx': 4, 'args': ()}, response_metadata={}, name='join', id='f5af94ac-00ce-4d62-91ce-6a207e39cc14', tool_call_id=4)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: The search result does not provide the current temperature in Seoul, which is necessary to complete the task. The information provided is regarding the weather forecast for future dates and not the current temperature. Therefore, we cannot create a flashcard summarizing the current temperature in Seoul.', additional_kwargs={}, response_metadata={}, id='15bbb19f-ec7c-4d98-a65d-d1772e3d78d7'), SystemMessage(content='Context from last attempt: The task requires finding the current temperature in Seoul to create a flashcard, but the search result provided is for future weather forecasts. A different approach or source is needed to obtain real-time weather information.', additional_kwargs={}, response_metadata={}, id='b97304d6-1208-4b3f-813d-5e2113f86b61')]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n",
      "Failed to use model_dump to serialize <class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'pydantic._internal._model_construction.ModelMetaclass'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': []}}\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The search result does not provide the current temperature in Seoul, which is required to create a flashcard summarizing this information. Multiple attempts have been made to find the current temperature without success. Therefore, it's necessary to admit the inability to provide the current temperature.\", additional_kwargs={}, response_metadata={}, id='f0d20ec0-3ede-4311-bb70-83dbd9a53622'), AIMessage(content=\"I'm unable to provide the current temperature in Seoul at this moment. The information retrieved does not include the current temperature, only future weather forecasts. I recommend checking a reliable weather website or app for the most up-to-date information.\", additional_kwargs={}, response_metadata={}, id='58ad3acb-a4dd-4f25-bc6f-a859944fad14')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Find the current temperature in Seoul, then, respond with a flashcard summarizing this information\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State\n",
    "\n",
    "from typing import List, TypedDict, Optional\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class Step(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents a step taken in the research process.\n",
    "    \"\"\"\n",
    "\n",
    "    id: str\n",
    "    description: str\n",
    "    status: str\n",
    "    type: str\n",
    "    description: str\n",
    "    search_result: Optional[str]\n",
    "    result: Optional[str]\n",
    "    updates: Optional[List[str]]\n",
    "\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"\n",
    "    This is the state of the agent.\n",
    "    It is a subclass of the MessagesState class from langgraph.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"openai\"\n",
    "    steps: List[Step]\n",
    "    answer: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from typing import List, Optional, Any, Union, Dict, Callable\n",
    "import asyncio\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    "    AIMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.callbacks.manager import adispatch_custom_event\n",
    "from typing import TypedDict\n",
    "from enum import Enum\n",
    "from typing_extensions import NotRequired\n",
    "\n",
    "\n",
    "class IntermediateStateConfig(TypedDict):\n",
    "    \"\"\"Intermediate state config\"\"\"\n",
    "\n",
    "    state_key: str\n",
    "    tool: str\n",
    "    tool_argument: NotRequired[str]\n",
    "\n",
    "\n",
    "def copilotkit_customize_config(\n",
    "    base_config: Optional[RunnableConfig] = None,\n",
    "    *,\n",
    "    emit_tool_calls: Optional[Union[bool, str, List[str]]] = None,\n",
    "    emit_messages: Optional[bool] = None,\n",
    "    emit_all: Optional[bool] = None,\n",
    "    emit_intermediate_state: Optional[List[IntermediateStateConfig]] = None\n",
    ") -> RunnableConfig:\n",
    "    \"\"\"\n",
    "    Configure for LangChain for use in CopilotKit\n",
    "    \"\"\"\n",
    "    metadata = base_config.get(\"metadata\", {}) if base_config else {}\n",
    "\n",
    "    if emit_all is True:\n",
    "        metadata[\"copilotkit:emit-tool-calls\"] = True\n",
    "        metadata[\"copilotkit:emit-messages\"] = True\n",
    "    else:\n",
    "        if emit_tool_calls is not None:\n",
    "            metadata[\"copilotkit:emit-tool-calls\"] = emit_tool_calls\n",
    "        if emit_messages is not None:\n",
    "            metadata[\"copilotkit:emit-messages\"] = emit_messages\n",
    "\n",
    "    if emit_intermediate_state:\n",
    "        metadata[\"copilotkit:emit-intermediate-state\"] = emit_intermediate_state\n",
    "\n",
    "    base_config = base_config or {}\n",
    "\n",
    "    return {**base_config, \"metadata\": metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "\n",
    "def get_model(state: AgentState) -> BaseChatModel:\n",
    "    \"\"\"\n",
    "    Get a model based on the environment variable.\n",
    "    \"\"\"\n",
    "    model = state.get(\"model\")\n",
    "\n",
    "    if model == \"openai\":\n",
    "        from langchain_openai import (\n",
    "            ChatOpenAI,\n",
    "        )  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "        return ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    if model == \"anthropic\":\n",
    "        from langchain_anthropic import (\n",
    "            ChatAnthropic,\n",
    "        )  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "        return ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-20240620\")\n",
    "    if model == \"google_genai\":\n",
    "        from langchain_google_genai import (\n",
    "            ChatGoogleGenerativeAI,\n",
    "        )  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "        return ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-pro\")\n",
    "\n",
    "    raise ValueError(\"Invalid model specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SearchStep(BaseModel):\n",
    "    \"\"\"Model for a search step\"\"\"\n",
    "\n",
    "    id: str = Field(\n",
    "        description=\"The id of the step. This is used to identify the step in the state. Just make sure it is unique.\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description='The description of the step, i.e. \"search for information about the latest AI news\"'\n",
    "    )\n",
    "    status: str = Field(\n",
    "        description='The status of the step. Always \"pending\".', enum=[\"pending\"]\n",
    "    )\n",
    "    type: str = Field(description=\"The type of step.\", enum=[\"search\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def SearchTool(steps: List[SearchStep]):  # pylint: disable=invalid-name,unused-argument\n",
    "    \"\"\"\n",
    "    Break the user's query into smaller steps.\n",
    "    Use step type \"search\" to search the web for information.\n",
    "    Make sure to add all the steps needed to answer the user's query.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "async def steps_node(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    The steps node is responsible for building the steps in the research process.\n",
    "    \"\"\"\n",
    "\n",
    "    config = copilotkit_customize_config(\n",
    "        config,\n",
    "        emit_intermediate_state=[\n",
    "            {\"state_key\": \"steps\", \"tool\": \"SearchTool\", \"tool_argument\": \"steps\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "You are a search assistant. Your task is to help the user with complex search queries by breaking the down into smaller steps.\n",
    "\n",
    "These steps are then executed serially. In the end, a final answer is produced in markdown format.\n",
    "\n",
    "The current date is {datetime.now().strftime(\"%Y-%m-%d\")}.\n",
    "\"\"\"\n",
    "\n",
    "    response = (\n",
    "        await get_model(state)\n",
    "        .bind_tools([SearchTool], tool_choice=\"SearchTool\")\n",
    "        .ainvoke(\n",
    "            [\n",
    "                state[\"messages\"][0],\n",
    "                HumanMessage(content=instructions),\n",
    "            ],\n",
    "            config,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if len(response.tool_calls) == 0:\n",
    "        steps = []\n",
    "    else:\n",
    "        steps = response.tool_calls[0][\"args\"][\"steps\"]\n",
    "\n",
    "    if len(steps) != 0:\n",
    "        steps[0][\"updates\"] = [\"Searching the web...\"]\n",
    "\n",
    "    return {\n",
    "        \"steps\": steps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "\n",
    "async def search_node(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    The search node is responsible for searching the internet for information.\n",
    "    \"\"\"\n",
    "    tavily_tool = TavilySearchResults(\n",
    "        max_results=3,\n",
    "        search_depth=\"advanced\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True,\n",
    "        include_images=True,\n",
    "    )\n",
    "\n",
    "    current_step = next(\n",
    "        (step for step in state[\"steps\"] if step[\"status\"] == \"pending\"), None\n",
    "    )\n",
    "\n",
    "    if current_step is None:\n",
    "        raise ValueError(\"No step to search for\")\n",
    "\n",
    "    if current_step[\"type\"] != \"search\":\n",
    "        raise ValueError(\"Current step is not a search step\")\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "This is a step in a series of steps that are being executed to answer the user's query.\n",
    "These are all of the steps: {json.dumps(state[\"steps\"])}\n",
    "\n",
    "You are responsible for carrying out the step: {json.dumps(current_step)}\n",
    "\n",
    "The current date is {datetime.now().strftime(\"%Y-%m-%d\")}.\n",
    "\n",
    "This is what you need to search for, please come up with a good search query: {current_step[\"description\"]}\n",
    "\"\"\"\n",
    "    model = get_model(state).bind_tools([tavily_tool], tool_choice=tavily_tool.name)\n",
    "\n",
    "    response = await model.ainvoke([HumanMessage(content=instructions)], config)\n",
    "\n",
    "    tool_msg = tavily_tool.invoke(response.tool_calls[0])\n",
    "\n",
    "    current_step[\"search_result\"] = json.loads(tool_msg.content)\n",
    "    current_step[\"updates\"] = [*current_step[\"updates\"], \"Extracting information...\"]\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "async def extract_node(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    The extract node is responsible for extracting information from a tavily search.\n",
    "    \"\"\"\n",
    "\n",
    "    current_step = next(\n",
    "        (step for step in state[\"steps\"] if step[\"status\"] == \"pending\"), None\n",
    "    )\n",
    "\n",
    "    if current_step is None:\n",
    "        raise ValueError(\"No current step\")\n",
    "\n",
    "    if current_step[\"type\"] != \"search\":\n",
    "        raise ValueError(\"Current step is not of type search\")\n",
    "\n",
    "    system_message = f\"\"\"\n",
    "This step was just executed: {json.dumps(current_step)}\n",
    "\n",
    "This is the result of the search:\n",
    "\n",
    "Please summarize ONLY the result of the search and include all relevant information from the search and reference links.\n",
    "DO NOT INCLUDE ANY EXTRA INFORMATION. ALL OF THE INFORMATION YOU ARE LOOKING FOR IS IN THE SEARCH RESULTS.\n",
    "\n",
    "DO NOT answer the user's query yet. Just summarize the search results.\n",
    "\n",
    "Use markdown formatting and put the references inline and the links at the end.\n",
    "Like this:\n",
    "This is a sentence with a reference to a source [source 1][1] and another reference [source 2][2].\n",
    "[1]: http://example.com/source1 \"Title of Source 1\"\n",
    "[2]: http://example.com/source2 \"Title of Source 2\"\n",
    "\"\"\"\n",
    "\n",
    "    response = await get_model(state).ainvoke(\n",
    "        [state[\"messages\"][0], HumanMessage(content=system_message)], config\n",
    "    )\n",
    "\n",
    "    current_step[\"result\"] = response.content\n",
    "    current_step[\"search_result\"] = None\n",
    "    current_step[\"status\"] = \"complete\"\n",
    "    current_step[\"updates\"] = [*current_step[\"updates\"], \"Done.\"]\n",
    "\n",
    "    next_step = next(\n",
    "        (step for step in state[\"steps\"] if step[\"status\"] == \"pending\"), None\n",
    "    )\n",
    "    if next_step:\n",
    "        next_step[\"updates\"] = [\"Searching the web...\"]\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    \"\"\"Model for a reference\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The title of the reference.\")\n",
    "    url: str = Field(description=\"The url of the reference.\")\n",
    "\n",
    "\n",
    "class SummarizeInput(BaseModel):\n",
    "    \"\"\"Input for the summarize tool\"\"\"\n",
    "\n",
    "    markdown: str = Field(\n",
    "        description=\"\"\"\n",
    "                          The markdown formatted summary of the final result.\n",
    "                          If you add any headings, make sure to start at the top level (#).\n",
    "                          \"\"\"\n",
    "    )\n",
    "    references: list[Reference] = Field(description=\"A list of references.\")\n",
    "\n",
    "\n",
    "@tool(args_schema=SummarizeInput)\n",
    "def SummarizeTool(\n",
    "    summary: str, references: list[Reference]\n",
    "):  # pylint: disable=invalid-name,unused-argument\n",
    "    \"\"\"\n",
    "    Summarize the final result. Make sure that the summary is complete and\n",
    "    includes all relevant information and reference links.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "async def summarize_node(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    The summarize node is responsible for summarizing the information.\n",
    "    \"\"\"\n",
    "\n",
    "    config = copilotkit_customize_config(\n",
    "        config,\n",
    "        emit_intermediate_state=[\n",
    "            {\n",
    "                \"state_key\": \"answer\",\n",
    "                \"tool\": \"SummarizeTool\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    system_message = f\"\"\"\n",
    "The system has performed a series of steps to answer the user's query.\n",
    "These are all of the steps: {json.dumps(state[\"steps\"])}\n",
    "\n",
    "Please summarize the final result and include all relevant information and reference links.\n",
    "\"\"\"\n",
    "\n",
    "    response = (\n",
    "        await get_model(state)\n",
    "        .bind_tools([SummarizeTool], tool_choice=\"SummarizeTool\")\n",
    "        .ainvoke(\n",
    "            [\n",
    "                HumanMessage(content=system_message),\n",
    "            ],\n",
    "            config,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.tool_calls[0][\"args\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def route(state):\n",
    "    \"\"\"Route to research nodes.\"\"\"\n",
    "    if not state.get(\"steps\", None):\n",
    "        return END\n",
    "\n",
    "    current_step = next(\n",
    "        (step for step in state[\"steps\"] if step[\"status\"] == \"pending\"), None\n",
    "    )\n",
    "\n",
    "    if not current_step:\n",
    "        return \"summarize_node\"\n",
    "\n",
    "    if current_step[\"type\"] == \"search\":\n",
    "        return \"search_node\"\n",
    "\n",
    "    raise ValueError(f\"Unknown step type: {current_step['type']}\")\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"steps_node\", steps_node)\n",
    "workflow.add_node(\"search_node\", search_node)\n",
    "workflow.add_node(\"summarize_node\", summarize_node)\n",
    "workflow.add_node(\"extract_node\", extract_node)\n",
    "# Chatbot\n",
    "workflow.set_entry_point(\"steps_node\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"steps_node\", route, [\"summarize_node\", \"search_node\", END]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"search_node\", \"extract_node\")\n",
    "\n",
    "workflow.add_conditional_edges(\"extract_node\", route, [\"summarize_node\", \"search_node\"])\n",
    "\n",
    "workflow.add_edge(\"summarize_node\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# # Visualize the compiled StateGraph as a Mermaid diagram\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps_node': {'steps': [{'id': 'step1', 'description': 'search for the current GDP of New York State', 'status': 'pending', 'type': 'search', 'updates': ['Searching the web...']}, {'id': 'step2', 'description': 'search for the GDP of New York City', 'status': 'pending', 'type': 'search'}]}}\n",
      "{'search_node': {'messages': [HumanMessage(content=\"What's the GDP of New York?\", additional_kwargs={}, response_metadata={}, id='eaf9e794-2ee2-42bd-92c5-bb0265395cbe')], 'model': 'openai', 'steps': [{'id': 'step1', 'description': 'search for the current GDP of New York State', 'status': 'pending', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...'], 'search_result': [{'url': 'https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf', 'content': \"All parties agree that the national economy, as measured by inflation-adjusted Gross Domestic Product, will expand at a slower pace in 2024, following growth of 2.5 percent in 2023. FORECAST CALENDAR YEAR Percent Change CY2024 CY2025 REAL GDP 2.1 1.8 PERSONAL INCOME 4.6 4.3 WAGES 4.9 4.3 CORP PROFITS 3.8 3.2 NONFARM EMPLOY.\\\\1E:'l:T 1.1 0.6 3-MOl'iTH T-BILL RATE 4.9 3.9 CPI 2.7 2.3 The parties agree that payroll employment in New York State will increase by 0.4 percent in FY 2025 following a 1.4 percent increase in FY 2024. The consensus forecast for personal income growth is 4.2 percent for FY 2025 following 3.8 percent for FY 2024. The consensus forecast calls for wage growth of 4.0 percent for FY 2025, following an increase of 3.8 percent for FY2024.\"}, {'url': 'https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/', 'content': \"What is the gross domestic product (GDP) in New York? What is the gross domestic product (GDP) in New York? Gross domestic product (GDP) measures the value of goods and services a country or state produces — it’s the sum of consumer spending, business investment, government spending, and net exports. As of 2023, the real GDP was $1.8 trillion. Real GDP in New York, adjusted for inflation (chained 2017 dollars) GDP and the economic experience vary by location due to factors like cost of living, population density, workforce education, and the area’s main industries. In 2023, New York's real (that is, inflation-adjusted) GDP per person was 1st out of all 50 states. In 2023, New York ranked 1st in state GDP per person.\"}, {'url': 'https://fiscalpolicy.org/the-state-of-new-yorks-fiscal-outlook', 'content': 'While the State’s enacted budget financial plan expected reserve levels to be flat between fiscal years 2024 and 2025, higher revenue and lower spending channeled an additional $3.2 billion into the State’s unrestricted general fund. Projections are from New York State Division of the Budget, Fiscal year 2025 enacted budget financial plan (June 2024), https://www.budget.ny.gov/pubs/archive/fy25/en/fy25fp-en.pdf. While the State’s enacted budget financial plan expected reserve levels to be flat between fiscal years 2024 and 2025, higher revenue and lower spending channeled an additional $3.2 billion into the State’s unrestricted general fund. Projections are from New York State Division of the Budget, Fiscal year 2025 enacted budget financial plan (June 2024), https://www.budget.ny.gov/pubs/archive/fy25/en/fy25fp-en.pdf.'}]}, {'id': 'step2', 'description': 'search for the GDP of New York City', 'status': 'pending', 'type': 'search'}], 'answer': None}}\n",
      "{'extract_node': {'messages': [HumanMessage(content=\"What's the GDP of New York?\", additional_kwargs={}, response_metadata={}, id='eaf9e794-2ee2-42bd-92c5-bb0265395cbe')], 'model': 'openai', 'steps': [{'id': 'step1', 'description': 'search for the current GDP of New York State', 'status': 'complete', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...', 'Done.'], 'search_result': None, 'result': 'As of 2023, the real GDP of New York was $1.8 trillion, adjusted for inflation (chained 2017 dollars). New York ranked 1st in state GDP per person in 2023, indicating a high economic output relative to its population [source 1][1][source 2][2].\\n\\n[1]: https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/ \"What is the gross domestic product (GDP) in New York?\"\\n[2]: https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf \"New York State Budget Forecast\"'}, {'id': 'step2', 'description': 'search for the GDP of New York City', 'status': 'pending', 'type': 'search', 'updates': ['Searching the web...']}], 'answer': None}}\n",
      "{'search_node': {'messages': [HumanMessage(content=\"What's the GDP of New York?\", additional_kwargs={}, response_metadata={}, id='eaf9e794-2ee2-42bd-92c5-bb0265395cbe')], 'model': 'openai', 'steps': [{'id': 'step1', 'description': 'search for the current GDP of New York State', 'status': 'complete', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...', 'Done.'], 'search_result': None, 'result': 'As of 2023, the real GDP of New York was $1.8 trillion, adjusted for inflation (chained 2017 dollars). New York ranked 1st in state GDP per person in 2023, indicating a high economic output relative to its population [source 1][1][source 2][2].\\n\\n[1]: https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/ \"What is the gross domestic product (GDP) in New York?\"\\n[2]: https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf \"New York State Budget Forecast\"'}, {'id': 'step2', 'description': 'search for the GDP of New York City', 'status': 'pending', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...'], 'search_result': [{'url': 'https://comptroller.nyc.gov/newsroom/newsletter/new-york-by-the-numbers-monthly-economic-and-fiscal-outlook/', 'content': 'New York City Economy Payroll Employment & Industry Trends. New York City private-sector employment was little changed in October, following a 9,000-job dip in September. ... By law, in the November update only the current fiscal year (FY 2025) budget must be balanced. The outyear budget gaps are $5.46 billion in FY 2026, $5.57 billion in FY'}, {'url': 'https://urbanland.uli.org/capital-markets-and-finance/2025-economic-forecast-new-york-city-and-the-u-s-poised-for-continued-growth-amid-uncertainties', 'content': '2025 Economic Forecast: New York City and the U.S. Poised for Continued Growth Amid Uncertainties - Urban Land Magazine The U.S. economy did very well in 2024, said Barbara Denham, lead economist for Oxford Economics, and the forecast for the coming year is more of the same—both in New York City and across North America. However, in presenting Oxford’s favorable economic forecast for 2025 at a ULI New York event last month, Denham also noted many caveats ahead of the incoming U.S. administration. The U.S. economy did very well in 2024, said Barbara Denham, lead economist for Oxford Economics, and the forecast for the coming year is more of the same—both in New York City and across North America.'}, {'url': 'https://www.osc.ny.gov/files/reports/osdc/pdf/report-5-2025.pdf', 'content': 'As shown in the Mayor’s Executive Budget for fiscal year 2025, New York City’s finances continue to benefit from better-than-projected revenues and savings generated through initiatives launched in response to its financial challenges. The Office of the State Comptroller (OSC) assumes cumulative upside tax revenue potential of $3.6 billion over the financial plan period in the City’s executive budget, a decline from an average of $4.9 billion projected in the executive budget over the last three fiscal years. The City’s April Plan includes a $379 million subsidy to NYCHA for FY 2024, an increase of $74.4 million from the January Plan, of which $47.1 million is for federally funded resilience expenses and $25.7 million for collective bargaining costs.'}]}], 'answer': None}}\n",
      "{'extract_node': {'messages': [HumanMessage(content=\"What's the GDP of New York?\", additional_kwargs={}, response_metadata={}, id='eaf9e794-2ee2-42bd-92c5-bb0265395cbe')], 'model': 'openai', 'steps': [{'id': 'step1', 'description': 'search for the current GDP of New York State', 'status': 'complete', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...', 'Done.'], 'search_result': None, 'result': 'As of 2023, the real GDP of New York was $1.8 trillion, adjusted for inflation (chained 2017 dollars). New York ranked 1st in state GDP per person in 2023, indicating a high economic output relative to its population [source 1][1][source 2][2].\\n\\n[1]: https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/ \"What is the gross domestic product (GDP) in New York?\"\\n[2]: https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf \"New York State Budget Forecast\"'}, {'id': 'step2', 'description': 'search for the GDP of New York City', 'status': 'complete', 'type': 'search', 'updates': ['Searching the web...', 'Extracting information...', 'Done.'], 'search_result': None, 'result': 'The search results did not provide a specific figure for the GDP of New York City. However, they discussed various aspects of the city\\'s economy, including employment trends and fiscal outlooks. The Mayor\\'s Executive Budget for fiscal year 2025 indicates that New York City\\'s finances are benefiting from better-than-projected revenues and savings from initiatives addressing financial challenges. Additionally, a forecast for 2025 suggests continued economic growth in New York City, despite uncertainties ahead [source 1][1][source 2][2][source 3][3].\\n\\n[1]: https://comptroller.nyc.gov/newsroom/newsletter/new-york-by-the-numbers-monthly-economic-and-fiscal-outlook/ \"New York City Economy Payroll Employment & Industry Trends\"\\n[2]: https://urbanland.uli.org/capital-markets-and-finance/2025-economic-forecast-new-york-city-and-the-u-s-poised-for-continued-growth-amid-uncertainties \"2025 Economic Forecast: New York City and the U.S.\"\\n[3]: https://www.osc.ny.gov/files/reports/osdc/pdf/report-5-2025.pdf \"Mayor’s Executive Budget for Fiscal Year 2025\"'}], 'answer': None}}\n",
      "{'summarize_node': {'answer': {'markdown': \"# Summary of GDP Information for New York State and New York City\\n\\n## New York State GDP\\n- As of 2023, the real GDP of New York State was **$1.8 trillion**, adjusted for inflation (chained 2017 dollars).\\n- New York ranked **1st** in state GDP per person in 2023, indicating a high economic output relative to its population.\\n\\n### References for New York State GDP:\\n1. [What is the gross domestic product (GDP) in New York?](https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/)  \\n2. [New York State Budget Forecast](https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf)  \\n\\n## New York City GDP\\n- The search did not yield a specific figure for the GDP of New York City. However, it highlighted various aspects of the city's economy, including employment trends and fiscal outlooks.\\n- The Mayor's Executive Budget for fiscal year 2025 indicates that New York City's finances are benefiting from better-than-projected revenues and savings from initiatives addressing financial challenges.\\n- A forecast for 2025 suggests continued economic growth in New York City, despite uncertainties ahead.\\n\\n### References for New York City GDP:\\n1. [New York City Economy Payroll Employment & Industry Trends](https://comptroller.nyc.gov/newsroom/newsletter/new-york-by-the-numbers-monthly-economic-and-fiscal-outlook/)  \\n2. [2025 Economic Forecast: New York City and the U.S.](https://urbanland.uli.org/capital-markets-and-finance/2025-economic-forecast-new-york-city-and-the-u-s-poised-for-continued-growth-amid-uncertainties)  \\n3. [Mayor’s Executive Budget for Fiscal Year 2025](https://www.osc.ny.gov/files/reports/osdc/pdf/report-5-2025.pdf)  \\n\", 'references': [{'title': 'What is the gross domestic product (GDP) in New York?', 'url': 'https://usafacts.org/answers/what-is-the-gross-domestic-product-gdp/state/new-york/'}, {'title': 'New York State Budget Forecast', 'url': 'https://www.budget.ny.gov/pubs/supporting/forecast/fy25/fy25-consensus-report-memo.pdf'}, {'title': 'New York City Economy Payroll Employment & Industry Trends', 'url': 'https://comptroller.nyc.gov/newsroom/newsletter/new-york-by-the-numbers-monthly-economic-and-fiscal-outlook/'}, {'title': '2025 Economic Forecast: New York City and the U.S.', 'url': 'https://urbanland.uli.org/capital-markets-and-finance/2025-economic-forecast-new-york-city-and-the-u-s-poised-for-continued-growth-amid-uncertainties'}, {'title': 'Mayor’s Executive Budget for Fiscal Year 2025', 'url': 'https://www.osc.ny.gov/files/reports/osdc/pdf/report-5-2025.pdf'}]}}}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": 1,  # temporary thread ID for testing\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = AgentState(\n",
    "    model=\"openai\",\n",
    "    messages=[HumanMessage(content=\"What's the GDP of New York?\")],\n",
    "    steps=[],\n",
    "    answer=None,\n",
    ")\n",
    "\n",
    "async for chunk in graph.astream(inputs, config=config):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-F0L5SJfm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
