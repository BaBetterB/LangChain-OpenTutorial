{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Scheduler System\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design: \n",
    "- Peer Review: [Mark()](https://github.com/obov), [Taylor(Jihyun Kim)](https://github.com/Taylor0819)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Multi-Agent Scheduler System represents an innovative approach to automating information retrieval and delivery through a coordinated network of specialized AI agents. At its core, this system transforms simple natural language requests into scheduled, automated search and delivery operations, making it particularly valuable for researchers, analysts, and anyone needing regular, scheduled information updates.\n",
    "\n",
    "Imagine asking \"Find me the latest RAG papers at 7 AM tomorrow.\" Instead of manually searching and compiling information early in the morning, the system automatically handles the entire process - from understanding your request to delivering a well-formatted email with relevant research papers at precisely 7 AM. This automation eliminates the need for manual intervention while ensuring timely delivery of crucial information.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "The system's architecture is built around five specialized agents, each handling a crucial aspect of the information retrieval and delivery process:\n",
    "\n",
    "1. `Query Analysis Agent`\n",
    "   This agent serves as the system's front door, interpreting natural language queries to extract critical information \n",
    "\n",
    "2. `Search Router`\n",
    "   Acting as the system's traffic controller, the Search Router directs queries to the most appropriate specialized search agent:\n",
    "   \n",
    "3. `Response Agent`\n",
    "   This agent transforms raw search results into well-structured, readable content by:\n",
    "\n",
    "4. `Scheduling System and Email Service`\n",
    "   The scheduling component manages the temporal aspects of the system:\n",
    "   This ensures that all operations occur at their specified times without conflicts.\n",
    "   The system implements a robust email delivery service using yagmail that provides:\n",
    "\n",
    "### System Flow\n",
    "\n",
    "The entire process follows this sequence:\n",
    "\n",
    "![Multi-Agent Scheduler System Flow](assets/21-Multi-AgentSchedulerSystem.png)\n",
    "\n",
    "This architecture ensures reliable, automated information retrieval and delivery, with each agent optimized for its specific role in the process.\n",
    "\n",
    "### Table of Contents\n",
    "- [Overview](#overview)\n",
    "- [System Architecture](#system-architecture)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Implementation Details](#implementation-details)\n",
    "   - [Query Analysis Agent](#query-analysis-agent)\n",
    "   - [Search Router and Specialized Agents](#Search-Router-and-Specialized-Agents)\n",
    "   - [Response Agent](#response-agent)\n",
    "   - [Scheduling System and Email Service](#Scheduling-System-and-Email-Service)  \n",
    "\n",
    "The system's modular design allows for easy expansion and customization, making it adaptable to various use cases while maintaining consistent performance and reliability. Whether you're tracking research papers, monitoring news, or gathering general information, the Multi-Agent Scheduler System automates the entire process from query to delivery, saving time and ensuring consistent, timely access to important information.\n",
    "\n",
    "### References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"chromadb\",\n",
    "        \"langchain_chroma\",\n",
    "        \"langchain_openai\",\n",
    "        \"pytz\",\n",
    "        \"google-search-results\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Multi-Agent Scheduler System\",\n",
    "        \"NEWS_API_KEY\": \"\",\n",
    "        \"SERPAPI_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Analysis Agent\n",
    "The QueryAnalysisAgent serves as the initial interpreter in our multi-agent scheduler system, transforming natural language queries into structured data that other agents can process. This agent employs advanced language understanding capabilities through GPT-4 to accurately parse user intentions and timing requirements.\n",
    "\n",
    "### Core Components\n",
    "The agent is built around three essential classes:\n",
    "- Time Extraction Processor: Handles temporal information\n",
    "- Task Analysis Engine: Understands search requirements\n",
    "- Query Coordinator: Combines and validates results\n",
    "\n",
    "\n",
    "### Core Functionality\n",
    "The agent performs two primary functions:\n",
    "\n",
    "1. Time Extraction\n",
    "```python\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from queries\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "    # Converts to standardized datetime format\n",
    "    return self._process_time(time_str)\n",
    "```\n",
    "\n",
    "2. Task Analysis\n",
    "```python\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Analyzes query content for search parameters\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "    return self._parse_response(response)\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize agent\n",
    "agent = QueryAnalysisAgent()\n",
    "\n",
    "# Process a sample query\n",
    "query = \"내일 오전 9시에 RAG 논문 찾아줘\"\n",
    "result = agent.analyze_query(query)\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```json\n",
    "{\n",
    "    \"target_time\": \"2025-02-06 09:00:00+0000\",\n",
    "    \"execution_time\": \"2025-02-06 08:55:00+0000\",\n",
    "    \"task_type\": \"search\",\n",
    "    \"search_type\": \"research_paper\",\n",
    "    \"keywords\": [\"RAG\", \"논문\"],\n",
    "    \"requirements\": \"minimum 5 results\",\n",
    "    \"time_sensitivity\": \"normal\",\n",
    "    \"original_query\": \"내일 오전 9시에 RAG 논문 찾아줘\",\n",
    "    \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "\n",
    "This structured approach ensures reliable query interpretation while maintaining flexibility for various query types and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing Required Libraries\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QueryAnalysisAgent class represents a specialized natural language query processor that utilizes OpenAI's language models. Let's break down its core components:\n",
    "The initialization method sets up the language model with temperature=0 to ensure consistent, deterministic responses:\n",
    "\n",
    "The setup_prompt_templates method defines two essential templates:\n",
    "\n",
    "1. Time Extraction Template\n",
    "This template focuses solely on extracting and standardizing time information from queries.\n",
    "\n",
    "2. Task Analysis Template\n",
    "This template structures the query analysis with specific rules:\n",
    "\n",
    "- Categorizes searches into three types: research_paper, news, or general\n",
    "- Distinguishes between normal and urgent time sensitivity\n",
    "- Separates search keywords from temporal terms\n",
    "- Maintains consistent task typing as \"search\"\n",
    "\n",
    "These templates work together to transform natural language queries into structured, actionable data that the rest of the system can process efficiently. The clear separation between time extraction and task analysis allows for precise handling of each aspect of the query.\n",
    "\n",
    "For example, a query like \"아침 7시에 RAG 논문 찾아줘\" would be processed to extract both the time (07:00) and the search parameters (research papers about RAG), while filtering out temporal terms from the actual search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Class Definition and __init__, setup_prompt_templates Methods\n",
    "class QueryAnalysisAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        self.time_extraction_prompt = PromptTemplate.from_template(\n",
    "            \"Extract time and convert to 24h format from: {query}\\nReturn HH:MM only\"\n",
    "        )\n",
    "\n",
    "        self.task_analysis_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Analyze the query and return only a JSON object. For the query: {query}\n",
    "\n",
    "        Return this exact format:\n",
    "        {{\n",
    "            \"task_type\": \"search\",\n",
    "            \"search_type\": \"research_paper\",\n",
    "            \"keywords\": [\"rag\", \"papers\"],\n",
    "            \"requirements\": \"minimum 5 results\",\n",
    "            \"time_sensitivity\": \"normal\",\n",
    "            \"search_terms\": [\"rag\", \"papers\"]  # Actual keywords to use for search\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        - search_type must be one of: \"research_paper\", \"news\", \"general\"\n",
    "        - time_sensitivity must be one of: \"normal\", \"urgent\"\n",
    "        - keywords should include all words from query including time-related terms\n",
    "        - search_terms should exclude time-related terms and only include actual search keywords\n",
    "        - task_type should always be \"search\"\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 1\n",
    "\n",
    "`extract_time()`\n",
    "- Functionality: Extracts and processes time information from natural language queries\n",
    "- Features:\n",
    "  - Converts various time formats (e.g., \"아침 7시\", \"오후 3:30\") to standardized datetime objects\n",
    "  - Maintains timezone awareness using pytz for accurate scheduling\n",
    "  - Automatically schedules for next day if requested time has already passed\n",
    "  - Strips unnecessary time components (seconds, microseconds) for cleaner scheduling\n",
    "- Error Handling: Raises ValueError with detailed error messages for invalid time formats\n",
    "- Returns: UTC-aware datetime object representing the target execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Adding extract_time Method\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from the query and returns a datetime object.\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Extract the actual time string from the ChatCompletion response\n",
    "        time_str = time_str.content.strip()\n",
    "\n",
    "        # Calculate the next scheduled time based on the current time\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        hour, minute = map(int, time_str.split(\":\"))\n",
    "\n",
    "        target_time = current_time.replace(\n",
    "            hour=hour, minute=minute, second=0, microsecond=0\n",
    "        )\n",
    "\n",
    "        # If the extracted time has already passed, set it for the next day\n",
    "        if target_time <= current_time:\n",
    "            target_time += timedelta(days=1)\n",
    "\n",
    "        return target_time\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Time extraction failed: {e}\")\n",
    "\n",
    "\n",
    "# After executing this cell, the method should be added to the class\n",
    "QueryAnalysisAgent.extract_time = extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 2\n",
    "\n",
    "`analyze_task()`\n",
    "- Functionality: Breaks down queries into structured task components\n",
    "- Features:\n",
    "  - Identifies search type (research_paper, news, general)\n",
    "  - Extracts relevant keywords while filtering temporal terms\n",
    "  - Determines task urgency (normal vs urgent)\n",
    "  - Identifies specific requirements (e.g., minimum result count)\n",
    "- Error Handling: Handles JSON parsing errors and invalid query formats\n",
    "- Returns: Dictionary containing parsed task information and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Adding analyze_task Method\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Extracts task intent and keywords from the query.\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Clean response content to ensure valid JSON format\n",
    "        content = response.content.strip()\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Original response: {response.content}\")\n",
    "        raise ValueError(f\"Failed to parse task analysis result: {e}\")\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_task = analyze_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 3\n",
    "\n",
    "`analyze_query()`\n",
    "- Functionality: Combines time extraction and task analysis into a complete query interpretation\n",
    "- Features:\n",
    "  - Coordinates between time extraction and task analysis\n",
    "  - Sets execution time 5 minutes before target time\n",
    "  - Validates and combines all query components\n",
    "- Error Handling: Catches and reports errors from both time and task processing\n",
    "- Returns: Combined dictionary with timing and task information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(self, query: str) -> dict:\n",
    "    \"\"\"Performs a full query analysis and returns the results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract time information\n",
    "        target_time = self.extract_time(query)\n",
    "\n",
    "        # Analyze task information\n",
    "        task_info = self.analyze_task(query)\n",
    "\n",
    "        # Return the results including all necessary details\n",
    "        return {\n",
    "            \"target_time\": target_time,\n",
    "            \"execution_time\": target_time - timedelta(minutes=5),\n",
    "            \"task_type\": task_info[\"task_type\"],\n",
    "            \"search_type\": task_info[\"search_type\"],  # Newly added\n",
    "            \"keywords\": task_info[\"keywords\"],\n",
    "            \"requirements\": task_info[\"requirements\"],\n",
    "            \"time_sensitivity\": task_info[\"time_sensitivity\"],  # Newly added\n",
    "            \"original_query\": query,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"original_query\": query}\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_query = analyze_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 4\n",
    "\n",
    "`datetime_handler(obj)`\n",
    "* Functionality: Converts datetime objects to JSON-serializable string format\n",
    "* Features:\n",
    "   * Accepts any object and checks if it's a datetime instance\n",
    "   * Converts datetime to standardized string format (YYYY-MM-DD HH:MM:SS+ZZZZ)\n",
    "   * Maintains timezone information in the output string\n",
    "* Error Handling: Raises TypeError with descriptive message for non-datetime objects\n",
    "* Returns: String representation of datetime in consistent format\n",
    "* Use Cases:\n",
    "   * JSON serialization for API responses\n",
    "   * Database storage of temporal data\n",
    "   * Logging and debugging timestamp formatting\n",
    "* Examples:\n",
    "   * Input: `datetime(2024, 2, 6, 15, 30, tzinfo=pytz.UTC)`\n",
    "   * Output: `\"2024-02-06 15:30:00+0000\"`\n",
    "\n",
    "The function serves as a critical utility for converting Python's datetime objects into a standardized string format that can be easily stored, transmitted, and later reconstructed. This is particularly important in our scheduling system where accurate time representation and timezone awareness are essential for reliable task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Testing QueryAnalysisAgent\n",
    "def datetime_handler(obj):\n",
    "    \"\"\"Handler to convert datetime objects into JSON serializable strings.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted datetime string (Format: YYYY-MM-DD HH:MM:SS+ZZZZ).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Raised if the object is not a datetime instance.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QueryAnalysisAgent Test Result ===\n",
      "{\n",
      "  \"target_time\": \"2025-02-07 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-07 06:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"news\",\n",
      "  \"keywords\": [\n",
      "    \"rag\",\n",
      "    \"7 AM\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"Find and recommend news related to RAG at 7 AM.\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_query_test():\n",
    "    \"\"\"Runs a test for QueryAnalysisAgent and returns the results.\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis result in JSON format.\n",
    "    \"\"\"\n",
    "    # Create an instance of QueryAnalysisAgent\n",
    "    agent = QueryAnalysisAgent()\n",
    "\n",
    "    # Test query\n",
    "    test_query = \"Find and recommend news related to RAG at 7 AM.\"\n",
    "\n",
    "    # Execute query analysis\n",
    "    result = agent.analyze_query(test_query)\n",
    "\n",
    "    # Convert the result to JSON format\n",
    "    return json.dumps(result, indent=2, ensure_ascii=False, default=datetime_handler)\n",
    "\n",
    "\n",
    "# Execute test and print results\n",
    "if __name__ == \"__main__\":\n",
    "    test_result = run_query_test()\n",
    "    print(\"\\n=== QueryAnalysisAgent Test Result ===\")\n",
    "    print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Router and Specialized Agents\n",
    "\n",
    "The Search Router system acts as an intelligent traffic controller, directing queries to the most appropriate specialized search agent based on the query analysis. This architecture ensures that each type of search request is handled by an agent specifically optimized for that domain.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```python\n",
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Initialize specialized search agents\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "```\n",
    "Each specialized agent is designed to handle specific types of searches:\n",
    "\n",
    "1. `Paper Search Agent`\n",
    "This agent specializes in academic paper searches, interfacing with arXiv's API to retrieve scholarly articles and research papers.\n",
    "\n",
    "2. `News Search Agent`\n",
    "This agent handles news-related searches, connecting to NewsAPI to gather current events and news articles.\n",
    "\n",
    "3. `General Search Agent`\n",
    "This agent manages general web searches using SerpAPI, handling broader information gathering needs.\n",
    "\n",
    "This routing system ensures that each query is handled by the most appropriate agent while maintaining consistent error handling and result formatting across all search types. The modular design allows for easy addition of new specialized agents as needed, making the system highly extensible and maintainable.\n",
    "\n",
    "Each agent provides standardized outputs despite their different data sources and search methodologies, enabling seamless integration with the rest of the system components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PaperSearchAgent`\n",
    "\n",
    "This agent focuses on academic content retrieval. It interfaces with the arXiv API to fetch scholarly papers and research documents. Key features include filtering papers by relevance, date ranges, and processing XML responses into structured data. The agent is particularly useful for researchers and academics needing current papers in their field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class PaperSearchAgent:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def perform_search(self, query_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        try:\n",
    "            keywords = self._process_keywords(query_info[\"keywords\"])\n",
    "            max_results = self._extract_max_results(query_info.get(\"requirements\", \"\"))\n",
    "\n",
    "            url = f\"{self.base_url}?search_query=all:{keywords}&start=0&max_results={max_results}\"\n",
    "            response = urllib.request.urlopen(url)\n",
    "            data = response.read().decode(\"utf-8\")\n",
    "\n",
    "            results = self._parse_arxiv_results(data)\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": results,\n",
    "                \"total_found\": len(results),\n",
    "                \"returned_count\": len(results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _process_keywords(self, keywords: List[str]) -> str:\n",
    "        # Remove time-related keywords\n",
    "        filtered_keywords = [\n",
    "            k\n",
    "            for k in keywords\n",
    "            if not any(\n",
    "                time in k.lower()\n",
    "                for time in [\"hour\", \"morning\", \"afternoon\", \"evening\"]\n",
    "            )\n",
    "        ]\n",
    "        return \"+\".join(filtered_keywords)\n",
    "\n",
    "    def _extract_max_results(self, requirements: str) -> int:\n",
    "        import re\n",
    "\n",
    "        # extracting numbers\n",
    "        numbers = re.findall(r\"\\d+\", requirements)\n",
    "        return int(numbers[0]) if numbers else 5\n",
    "\n",
    "    def _parse_arxiv_results(self, data: str) -> List[Dict[str, Any]]:\n",
    "        root = ET.fromstring(data)\n",
    "        results = []\n",
    "\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            url = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "            published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"type\": \"research_paper\",\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"published_date\": published[:10],\n",
    "                    \"summary\": summary,\n",
    "                    \"source\": \"arxiv\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NewsSearchAgent`\n",
    "\n",
    "This agent handles current events and news article searches. It connects to NewsAPI to access a wide range of news sources. The agent supports features like language filtering, date range specification, and source selection. It's especially valuable for users needing real-time information or tracking specific topics in the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class NewsSearchAgent:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initializes a news search agent using NewsAPI.\n",
    "\n",
    "        NewsAPI follows a REST API structure with the base URL 'https://newsapi.org/v2'.\n",
    "        It provides two main endpoints:\n",
    "        - /everything: Searches the entire news archive.\n",
    "        - /top-headlines: Retrieves the latest top headlines.\n",
    "        \"\"\"\n",
    "\n",
    "        self.api_key = os.environ[\"NEWS_API_KEY\"]\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"NewsAPI key is required\")\n",
    "\n",
    "        self.base_url = \"https://newsapi.org/v2\"\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs a news search based on the given query information.\n",
    "\n",
    "        Args:\n",
    "            query_info (Dict[str, Any]): Dictionary containing search parameters.\n",
    "            max_results (int): Maximum number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing search results or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract actual search terms (excluding time-related keywords)\n",
    "            search_terms = query_info.get(\n",
    "                \"search_terms\", query_info.get(\"keywords\", [])\n",
    "            )\n",
    "\n",
    "            # Check if the search is for real-time news\n",
    "            is_realtime = query_info.get(\"time_sensitivity\") == \"urgent\"\n",
    "            from_date = datetime.now() - timedelta(hours=1 if is_realtime else 24)\n",
    "\n",
    "            # Configure parameters for the 'everything' endpoint\n",
    "            params = {\n",
    "                \"q\": \" \".join(search_terms),  # Exclude time-related keywords\n",
    "                \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"sortBy\": \"publishedAt\",\n",
    "                \"language\": \"en\",\n",
    "                \"apiKey\": self.api_key,\n",
    "            }\n",
    "\n",
    "            # Construct API request URL\n",
    "            url = f\"{self.base_url}/everything?{urlencode(params)}\"\n",
    "\n",
    "            # Send API request\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            # Check response status\n",
    "            if response.status_code != 200:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": data.get(\"message\", \"Unknown error\"),\n",
    "                    \"query_info\": query_info,\n",
    "                }\n",
    "\n",
    "            # Process and format results\n",
    "            articles = data.get(\"articles\", [])\n",
    "            formatted_results = []\n",
    "\n",
    "            for article in articles[:max_results]:\n",
    "                formatted_results.append(\n",
    "                    {\n",
    "                        \"title\": article.get(\"title\"),\n",
    "                        \"description\": article.get(\"description\"),\n",
    "                        \"url\": article.get(\"url\"),\n",
    "                        \"published_at\": article.get(\"publishedAt\"),\n",
    "                        \"source\": article.get(\"source\", {}).get(\"name\"),\n",
    "                        \"content\": article.get(\"content\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": formatted_results,\n",
    "                \"total_results\": data.get(\"totalResults\", 0),\n",
    "                \"returned_count\": len(formatted_results),\n",
    "                \"search_parameters\": {\n",
    "                    \"keywords\": query_info[\"keywords\"],\n",
    "                    \"from_date\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"language\": \"en\",\n",
    "                },\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def get_top_headlines(\n",
    "        self, country: str = \"us\", category: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Fetches top news headlines.\n",
    "\n",
    "        Args:\n",
    "            country (str): Country code (default: 'us' for the United States).\n",
    "            category (str, optional): News category (e.g., business, technology).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing top headlines.\n",
    "        \"\"\"\n",
    "        params = {\"country\": country, \"apiKey\": self.api_key}\n",
    "\n",
    "        if category:\n",
    "            params[\"category\"] = category\n",
    "\n",
    "        url = f\"{self.base_url}/top-headlines?{urlencode(params)}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GeneralSearchAgent`\n",
    "\n",
    "This agent manages broader web searches through SerpAPI. It handles diverse information needs that don't fit strictly into academic or news categories. The agent includes features like language-specific searches, result ranking, and content type filtering. It's particularly useful for general research, product information, or any broad information gathering needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "class GeneralSearchAgent:\n",
    "    def __init__(self, serpapi_key: str = None):\n",
    "        if serpapi_key:\n",
    "            os.environ[\"SERPAPI_API_KEY\"] = serpapi_key\n",
    "        self.search = SerpAPIWrapper()\n",
    "\n",
    "    def setup_search_parameters(self, query_info: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Constructs search queries for general search.\"\"\"\n",
    "        keywords = \" \".join(query_info[\"keywords\"])\n",
    "\n",
    "        # Set up base search queries\n",
    "        search_queries = [\n",
    "            f\"{keywords} lang:ko\",\n",
    "            keywords,\n",
    "        ]  # Korean results  # General search\n",
    "\n",
    "        return search_queries\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs general search and returns results.\"\"\"\n",
    "        try:\n",
    "            search_queries = self.setup_search_parameters(query_info)\n",
    "            all_results = []\n",
    "\n",
    "            for query in search_queries:\n",
    "                raw_results = self.search.run(query)\n",
    "                parsed_results = self._parse_general_results(raw_results)\n",
    "                all_results.extend(parsed_results)\n",
    "\n",
    "            # Sort by relevance score\n",
    "            sorted_results = sorted(\n",
    "                all_results, key=lambda x: x.get(\"relevance_score\", 0), reverse=True\n",
    "            )[:max_results]\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": sorted_results,\n",
    "                \"total_found\": len(all_results),\n",
    "                \"returned_count\": len(sorted_results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _parse_general_results(self, raw_results: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parses general search results.\"\"\"\n",
    "        parsed_results = []\n",
    "\n",
    "        for result in raw_results.split(\"\\n\"):\n",
    "            if not result.strip():\n",
    "                continue\n",
    "\n",
    "            parsed_results.append(\n",
    "                {\n",
    "                    \"type\": \"general\",\n",
    "                    \"title\": self._extract_title(result),\n",
    "                    \"content\": result,\n",
    "                    \"url\": self._extract_url(result),\n",
    "                    \"relevance_score\": self._calculate_relevance(result),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return parsed_results\n",
    "\n",
    "    def _extract_title(self, result: str) -> str:\n",
    "        \"\"\"Extracts title from the result.\"\"\"\n",
    "        return result.split(\".\")[0].strip()[:100]\n",
    "\n",
    "    def _extract_url(self, result: str) -> str:\n",
    "        \"\"\"Extracts URL from the result.\"\"\"\n",
    "        import re\n",
    "\n",
    "        urls = re.findall(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[^\\s]*\", result)\n",
    "        return urls[0] if urls else \"\"\n",
    "\n",
    "    def _calculate_relevance(self, result: str) -> float:\n",
    "        \"\"\"Calculates relevance score for the search result.\"\"\"\n",
    "        relevance_score = 0.5  # Base score\n",
    "\n",
    "        # Calculate score based on keyword matching\n",
    "        keywords = [\"official\", \"guide\", \"tutorial\", \"review\", \"recommendation\"]\n",
    "        lower_result = result.lower()\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if keyword in lower_result:\n",
    "                relevance_score += 0.1\n",
    "\n",
    "        return min(1.0, relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SearchRouter`: The System's Traffic Controller\n",
    "\n",
    "The `SearchRouter` acts as the central coordinator for our multi-agent search system, intelligently directing queries to specialized search agents based on the type of information needed. Think of it as an expert traffic controller at a busy airport, making sure each \"flight\" (query) goes to the right \"runway\" (search agent).\n",
    "\n",
    "The `SearchRouter`'s modular design allows for easy expansion - new specialized search agents can be added without modifying the existing code, making the system highly adaptable to evolving search needs.\n",
    "\n",
    "Through this central coordination, the `SearchRouter` ensures efficient and reliable information retrieval across different types of searches while maintaining a consistent interface for the rest of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Get API key directly or from environment variables\n",
    "        # Initialize agents for each search type\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "\n",
    "    def route_and_search(self, query_analysis: dict) -> dict:\n",
    "        \"\"\"Routes the search request to appropriate search agent based on query analysis\n",
    "\n",
    "        Args:\n",
    "            query_analysis (dict): Query analysis results from QueryAnalysisAgent\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing search results, including success/failure status and related info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check search type\n",
    "            search_type = query_analysis.get(\"search_type\")\n",
    "\n",
    "            # Record start time for logging\n",
    "            start_time = datetime.now(pytz.utc)\n",
    "\n",
    "            # Perform search\n",
    "            if search_type == \"research_paper\":\n",
    "                print(\"Performing research paper search...\")\n",
    "                result = self.paper_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"news\":\n",
    "                print(\"Performing news search...\")\n",
    "                result = self.news_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"general\":\n",
    "                print(\"Performing general search...\")\n",
    "                result = self.general_search_agent.perform_search(query_analysis)\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Unsupported search type: {search_type}\",\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "\n",
    "            # Calculate search duration\n",
    "            end_time = datetime.now(pytz.utc)\n",
    "            search_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Add metadata to results\n",
    "            result.update(\n",
    "                {\n",
    "                    \"search_type\": search_type,\n",
    "                    \"search_duration\": search_duration,\n",
    "                    \"search_timestamp\": end_time.isoformat(),\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                \"search_type\": query_analysis.get(\"search_type\"),\n",
    "            }\n",
    "\n",
    "    def get_agent_status(self) -> dict:\n",
    "        \"\"\"Check the status of each search agent.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing status information for each agent\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"paper_search_agent\": \"ready\",\n",
    "            \"news_search_agent\": \"ready\",\n",
    "            \"general_search_agent\": \"ready\",\n",
    "            \"last_checked\": datetime.now(pytz.utc).isoformat(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Agent\n",
    "Crafting User-Friendly Information Delivery\n",
    "\n",
    "The ResponseAgent serves as our system's expert communicator, transforming raw search results into well-structured, readable content that meets users' needs. This agent is particularly crucial as it represents the final step in our information delivery pipeline, ensuring that complex search results are presented in a clear, digestible format.\n",
    "\n",
    "The agent maintains three specialized prompt templates for different types of content:\n",
    "\n",
    "Key Features of the Response Agent:\n",
    "\n",
    "1. Content Customization\n",
    "   - Adapts formatting based on content type (papers, news, general)\n",
    "   - Maintains consistent structure while accommodating different information types\n",
    "   - Ensures appropriate context and explanations are included\n",
    "\n",
    "2. Email Optimization\n",
    "   - Creates clear, professional email subjects\n",
    "   - Structures content for easy scanning and reading\n",
    "   - Includes all necessary context and source information\n",
    "\n",
    "The ResponseAgent represents the crucial final step in our information delivery pipeline, ensuring that users receive not just raw data, but well-organized, contextually relevant information that directly addresses their queries. Through its careful formatting and organization, it helps users quickly understand and act upon the information they've requested.\n",
    "\n",
    "This agent demonstrates how automated systems can maintain a human touch in their communications, making complex information accessible and actionable for end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        self.setup_prompts()\n",
    "\n",
    "    def setup_prompts(self):\n",
    "        # Research paper search prompt\n",
    "        self.paper_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following research paper search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Research Paper Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized research paper results for your search.\"\n",
    "               - Number each paper and format as follows:\n",
    "                 1. Paper title: [title]\n",
    "                    - Summary: [core research content and key findings]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # News search prompt\n",
    "        self.news_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following news search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Latest News Updates for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the latest news articles related to your search topic.\"\n",
    "               - Number each news item and format as follows:\n",
    "                 1. [title] - [news source]\n",
    "                    - Main content: [key content summary]\n",
    "                    - Published date: [date]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # General search prompt\n",
    "        self.general_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized results for your search.\"\n",
    "               - Number each result and format as follows:\n",
    "                 1. [title]\n",
    "                    - Content: [main content summary]\n",
    "                    - Source: [website or platform name]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def format_results(self, search_results):\n",
    "        try:\n",
    "            # Handle cases with no results or errors\n",
    "            if search_results.get(\"status\") == \"error\":\n",
    "                return {\n",
    "                    \"subject\": \"Search Error Notification\",\n",
    "                    \"body\": f\"An error occurred during search: {search_results.get('error_message', 'Unknown error')}\",\n",
    "                }\n",
    "\n",
    "            # Handle cases with no results\n",
    "            if not search_results.get(\"results\"):\n",
    "                return {\n",
    "                    \"subject\": \"No Search Results\",\n",
    "                    \"body\": f\"No results found for search term '{search_results.get('original_query', '')}'.\",\n",
    "                }\n",
    "\n",
    "            # Select prompt based on search type\n",
    "            search_type = search_results.get(\"search_type\", \"general\")\n",
    "            if search_type == \"research_paper\":\n",
    "                prompt = self.paper_prompt\n",
    "            elif search_type == \"news\":\n",
    "                prompt = self.news_prompt\n",
    "            else:\n",
    "                prompt = self.general_prompt\n",
    "\n",
    "            # Prepare input for result formatting\n",
    "            formatted_input = {\n",
    "                \"query\": search_results.get(\"original_query\", \"\"),\n",
    "                \"results\": json.dumps(\n",
    "                    search_results.get(\"results\", []), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Generate response through LLM\n",
    "            response = prompt.format(**formatted_input)\n",
    "            response = self.llm.invoke(response)\n",
    "\n",
    "            try:\n",
    "                # Attempt JSON parsing\n",
    "                return json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                # Return default format if JSON parsing fails\n",
    "                return {\n",
    "                    \"subject\": f\"Search Results for [{formatted_input['query']}]\",\n",
    "                    \"body\": response.content,\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"subject\": \"Result Processing Error\",\n",
    "                \"body\": f\"An error occurred while processing results: {str(e)}\\n\\nOriginal query: {search_results.get('original_query', '')}\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query: recommend place to eat at seoul in 7 pm\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-06 19:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-06 18:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"general\",\n",
      "  \"keywords\": [\n",
      "    \"recommend\",\n",
      "    \"place\",\n",
      "    \"to\",\n",
      "    \"eat\",\n",
      "    \"at\",\n",
      "    \"seoul\",\n",
      "    \"in\",\n",
      "    \"7\",\n",
      "    \"pm\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"recommend place to eat at seoul in 7 pm\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing general search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"type\": \"general\",\n",
      "      \"title\": \"['Seoul Late Night Restaurants · 1\",\n",
      "      \"content\": \"['Seoul Late Night Restaurants · 1. Daol Charcoal Grill. (441). Closed now · 2. Han-gong-gan. (145). Closed now · 3. Bogwangjung Itaewon. (89).', \\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean style pastries and coffee. Also some cafes.\\\", \\\"Folks, let me tell you, when I stumbled upon Seoul's vibrant 24-hour food scene, I thought I'd died and gone to midnight snacking heaven.\\\", 'This place is a 4 minute walk from Hongik station and open 24hours. The short rib gimbap was my favorite food this entire trip and I probably ate there 7 times ...', \\\"Gwangjang Market – This old market is one of the greatest culinary destinations in all of Seoul, and if you love food, there's absolutely no way you're going to ...\\\", 'Ilpyeon siroin myeongdong · 4.7 (42) ; On 6.5 · 4.5 (1,280) ; Hanmiok Flagship Restaurant · 4.5 (479) ; Sukseongdo Euljiro · 4.6 (544) ; Daol Charcoal Grill Restaurants ...', 'Most hotels and hostels are within walking distance of restaurants. Here are my recommendations though: Itaewon - Foreign food, clubs, and bars.', \\\"Gwangjang Market is one of the best experiences you can have in Seoul. It's a place where tradition, culture, and delicious Korean street foods ...\\\", \\\"Tosokchon is one of the best best Korean restaurants in Seoul and THE place to go for samgyetang. Don't miss it, especially if you visit Seoul ...\\\", \\\"Try Star Chef, a fusion place. I think it's in Gangnam. Reviews here from Seoul Eats and ZenKimchi, which are good blogs to find other suggestions.\\\"]\",\n",
      "      \"url\": \"\",\n",
      "      \"relevance_score\": 0.7\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"general\",\n",
      "      \"title\": \"[\\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean\",\n",
      "      \"content\": \"[\\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean style pastries and coffee. Also some cafes.\\\", \\\"If you wanna eat like a king, I suggest the Bulgogi Whopper but the one with 3 patties. Many locations are open 24 hours. If you're feeling ...\\\", 'Seoul Late Night Restaurants · 1. Daol Charcoal Grill. (441). Closed now · 2. Han-gong-gan. (145). Closed now · 3. Bogwangjung Itaewon. (89).', \\\"Folks, let me tell you, when I stumbled upon Seoul's vibrant 24-hour food scene, I thought I'd died and gone to midnight snacking heaven.\\\", '1 Mingles · 2 Gwangjang Market · 3 MOSU Seoul · 4 Born and Bred · 5 Myeongdong Kyoja Main Restaurant · 6 Tosokchon Samgyetang · 7 Hadongkwan · 8 La Yeon.', 'With favorites like Gwangjang Market, Tosokchon Samgyetang, and Myeongdong Kyoja Main Restaurant and more, get ready to experience the best flavors around ...', \\\"Gwangjang Market – This old market is one of the greatest culinary destinations in all of Seoul, and if you love food, there's absolutely no way you're going to ...\\\", 'Traditional Korean soup restaurant specializing in sul lung tang, a beef bone broth soup, with options like beef brisket and a mixed selection excluding tongue.', \\\"Gwangjang Market is one of the best experiences you can have in Seoul. It's a place where tradition, culture, and delicious Korean street foods ...\\\"]\",\n",
      "      \"url\": \"\",\n",
      "      \"relevance_score\": 0.5\n",
      "    }\n",
      "  ],\n",
      "  \"total_found\": 2,\n",
      "  \"returned_count\": 2,\n",
      "  \"query_info\": {\n",
      "    \"target_time\": \"2025-02-06 19:00:00+0000\",\n",
      "    \"execution_time\": \"2025-02-06 18:55:00+0000\",\n",
      "    \"task_type\": \"search\",\n",
      "    \"search_type\": \"general\",\n",
      "    \"keywords\": [\n",
      "      \"recommend\",\n",
      "      \"place\",\n",
      "      \"to\",\n",
      "      \"eat\",\n",
      "      \"at\",\n",
      "      \"seoul\",\n",
      "      \"in\",\n",
      "      \"7\",\n",
      "      \"pm\"\n",
      "    ],\n",
      "    \"requirements\": \"minimum 5 results\",\n",
      "    \"time_sensitivity\": \"normal\",\n",
      "    \"original_query\": \"recommend place to eat at seoul in 7 pm\",\n",
      "    \"status\": \"success\"\n",
      "  },\n",
      "  \"search_type\": \"general\",\n",
      "  \"search_duration\": 0.077063,\n",
      "  \"search_timestamp\": \"2025-02-06T11:07:00.465283+00:00\",\n",
      "  \"original_query\": \"recommend place to eat at seoul in 7 pm\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [recommend place to eat at seoul in 7 pm]\",\n",
      "  \"body\": \"Subject: Search Results for \\\"recommend place to eat at Seoul in 7 pm\\\"\\n\\n---\\n\\nHello,\\n\\nHere are the organized results for your search:\\n\\n1. Seoul Late Night Restaurants\\n   - Content: This result highlights several dining options in Seoul, including Daol Charcoal Grill, Han-gong-gan, and Bogwangjung Itaewon. It also mentions popular places like Gwangjang Market and Tosokchon, known for its samgyetang. Late-night options like Paris Baguette and Tous Les Jours are noted for pastries and coffee.\\n   - Source: Various mentions including recommendations on blogs like Seoul Eats and ZenKimchi.\\n   - URL: Not provided\\n\\n2. The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean\\n   - Content: Besides cafes and bakeries, the result talks about 24-hour options including a Bulgogi Whopper with multiple patties. Mentions popular places in Seoul like Mingles, Gwangjang Market, and Tosokchon Samgyetang.\\n   - Source: Various mentions and a list of favorite places in Seoul.\\n   - URL: Not provided\\n\\nClosing remarks: I hope you find these search results helpful for your dining plans in Seoul. Enjoy your meal!\\n\\nBest regards,\\n\\n[Your Name]\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Test Query: find rag persona paper at 3 pm\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-06 15:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-06 14:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"research_paper\",\n",
      "  \"keywords\": [\n",
      "    \"rag\",\n",
      "    \"persona\",\n",
      "    \"paper\",\n",
      "    \"at\",\n",
      "    \"3\",\n",
      "    \"pm\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"find rag persona paper at 3 pm\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing research paper search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging\\n  Narratives\",\n",
      "      \"url\": \"http://arxiv.org/abs/2305.02364v2\",\n",
      "      \"published_date\": \"2023-05-03\",\n",
      "      \"summary\": \"  Sustaining coherent and engaging narratives requires dialogue or storytelling\\nagents to understand how the personas of speakers or listeners ground the\\nnarrative. Specifically, these agents must infer personas of their listeners to\\nproduce statements that cater to their interests. They must also learn to\\nmaintain consistent speaker personas for themselves throughout the narrative,\\nso that their counterparts feel involved in a realistic conversation or story.\\n  However, personas are diverse and complex: they entail large quantities of\\nrich interconnected world knowledge that is challenging to robustly represent\\nin general narrative systems (e.g., a singer is good at singing, and may have\\nattended conservatoire). In this work, we construct a new large-scale persona\\ncommonsense knowledge graph, PeaCoK, containing ~100K human-validated persona\\nfacts. Our knowledge graph schematizes five dimensions of persona knowledge\\nidentified in previous studies of human interactive behaviours, and distils\\nfacts in this schema from both existing commonsense knowledge graphs and\\nlarge-scale pretrained language models. Our analysis indicates that PeaCoK\\ncontains rich and precise world persona inferences that help downstream systems\\ngenerate more consistent and engaging narratives.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"Detecting Speaker Personas from Conversational Texts\",\n",
      "      \"url\": \"http://arxiv.org/abs/2109.01330v1\",\n",
      "      \"published_date\": \"2021-09-03\",\n",
      "      \"summary\": \"  Personas are useful for dialogue response prediction. However, the personas\\nused in current studies are pre-defined and hard to obtain before a\\nconversation. To tackle this issue, we study a new task, named Speaker Persona\\nDetection (SPD), which aims to detect speaker personas based on the plain\\nconversational text. In this task, a best-matched persona is searched out from\\ncandidates given the conversational text. This is a many-to-many semantic\\nmatching task because both contexts and personas in SPD are composed of\\nmultiple sentences. The long-term dependency and the dynamic redundancy among\\nthese sentences increase the difficulty of this task. We build a dataset for\\nSPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate\\nseveral baseline models and propose utterance-to-profile (U2P) matching\\nnetworks for this task. The U2P models operate at a fine granularity which\\ntreat both contexts and personas as sets of multiple sequences. Then, each\\nsequence pair is scored and an interpretable overall score is obtained for a\\ncontext-persona pair through aggregation. Evaluation results show that the U2P\\nmodels outperform their baseline counterparts significantly.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented\\n  Generation\",\n",
      "      \"url\": \"http://arxiv.org/abs/2409.11598v2\",\n",
      "      \"published_date\": \"2024-09-17\",\n",
      "      \"summary\": \"  Many language models now enhance their responses with retrieval capabilities,\\nleading to the widespread adoption of retrieval-augmented generation (RAG)\\nsystems. However, despite retrieval being a core component of RAG, much of the\\nresearch in this area overlooks the extensive body of work on fair ranking,\\nneglecting the importance of considering all stakeholders involved. This paper\\npresents the first systematic evaluation of RAG systems integrated with fair\\nrankings. We focus specifically on measuring the fair exposure of each relevant\\nitem across the rankings utilized by RAG systems (i.e., item-side fairness),\\naiming to promote equitable growth for relevant item providers. To gain a deep\\nunderstanding of the relationship between item-fairness, ranking quality, and\\ngeneration quality in the context of RAG, we analyze nine different RAG systems\\nthat incorporate fair rankings across seven distinct datasets. Our findings\\nindicate that RAG systems with fair rankings can maintain a high level of\\ngeneration quality and, in many cases, even outperform traditional RAG systems,\\ndespite the general trend of a tradeoff between ensuring fairness and\\nmaintaining system-effectiveness. We believe our insights lay the groundwork\\nfor responsible and equitable RAG systems and open new avenues for future\\nresearch. We publicly release our codebase and dataset at\\nhttps://github.com/kimdanny/Fair-RAG.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"A Stack-Propagation Framework for Low-Resource Personalized Dialogue\\n  Generation\",\n",
      "      \"url\": \"http://arxiv.org/abs/2410.20174v1\",\n",
      "      \"published_date\": \"2024-10-26\",\n",
      "      \"summary\": \"  With the resurgent interest in building open-domain dialogue systems, the\\ndialogue generation task has attracted increasing attention over the past few\\nyears. This task is usually formulated as a conditional generation problem,\\nwhich aims to generate a natural and meaningful response given dialogue\\ncontexts and specific constraints, such as persona. And maintaining a\\nconsistent persona is essential for the dialogue systems to gain trust from the\\nusers. Although tremendous advancements have been brought, traditional\\npersona-based dialogue models are typically trained by leveraging a large\\nnumber of persona-dense dialogue examples. Yet, such persona-dense training\\ndata are expensive to obtain, leading to a limited scale. This work presents a\\nnovel approach to learning from limited training examples by regarding\\nconsistency understanding as a regularization of response generation. To this\\nend, we propose a novel stack-propagation framework for learning a generation\\nand understanding pipeline.Specifically, the framework stacks a Transformer\\nencoder and two Transformer decoders, where the first decoder models response\\ngeneration and the second serves as a regularizer and jointly models response\\ngeneration and consistency understanding. The proposed framework can benefit\\nfrom the stacked encoder and decoders to learn from much smaller personalized\\ndialogue data while maintaining competitive performance. Under different\\nlow-resource settings, subjective and objective evaluations prove that the\\nstack-propagation framework outperforms strong baselines in response quality\\nand persona consistency and largely overcomes the shortcomings of traditional\\nmodels that rely heavily on the persona-dense dialogue data.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\",\n",
      "      \"url\": \"http://arxiv.org/abs/2406.00944v2\",\n",
      "      \"published_date\": \"2024-06-03\",\n",
      "      \"summary\": \"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\\nlarge language models (LLMs). Studies show that while RAG provides valuable\\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\\nor incorrect retrieved texts. Although many existing methods attempt to\\npreserve benefit and avoid detriment, they lack a theoretical explanation for\\nRAG. The benefit and detriment in the next token prediction of RAG remain a\\nblack box that cannot be quantified or compared in an explainable manner, so\\nexisting methods are data-driven, need additional utility evaluators or\\npost-hoc. This paper takes the first step towards providing a theory to explain\\nand trade off the benefit and detriment in RAG. First, we model RAG as the\\nfusion between distribution of LLMs knowledge and distribution of retrieved\\ntexts. Then, we formalize the trade-off between the value of external knowledge\\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\\nprediction of RAG by distribution difference in this fusion. Finally, we prove\\nthat the actual effect of RAG on the token, which is the comparison between\\nbenefit and detriment, can be predicted without any training or accessing the\\nutility of retrieval. Based on our theory, we propose a practical novel method,\\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\\nat token level to preserve benefit and avoid detriment. Experiments in\\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\\neffectiveness of our method and support our theoretical findings.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_found\": 5,\n",
      "  \"returned_count\": 5,\n",
      "  \"query_info\": {\n",
      "    \"target_time\": \"2025-02-06 15:00:00+0000\",\n",
      "    \"execution_time\": \"2025-02-06 14:55:00+0000\",\n",
      "    \"task_type\": \"search\",\n",
      "    \"search_type\": \"research_paper\",\n",
      "    \"keywords\": [\n",
      "      \"rag\",\n",
      "      \"persona\",\n",
      "      \"paper\",\n",
      "      \"at\",\n",
      "      \"3\",\n",
      "      \"pm\"\n",
      "    ],\n",
      "    \"requirements\": \"minimum 5 results\",\n",
      "    \"time_sensitivity\": \"normal\",\n",
      "    \"original_query\": \"find rag persona paper at 3 pm\",\n",
      "    \"status\": \"success\"\n",
      "  },\n",
      "  \"search_type\": \"research_paper\",\n",
      "  \"search_duration\": 1.004571,\n",
      "  \"search_timestamp\": \"2025-02-06T11:07:10.281568+00:00\",\n",
      "  \"original_query\": \"find rag persona paper at 3 pm\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [find rag persona paper at 3 pm]\",\n",
      "  \"body\": \"Subject: Research Paper Search Results for \\\"find rag persona paper at 3 pm\\\"\\n\\nBody:\\n\\nHello,\\n\\nHere are the organized research paper results for your search:\\n\\n1. Paper title: PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives\\n   - Summary: This paper discusses the importance of understanding personas in sustaining coherent and engaging narratives by dialogue or storytelling agents. The authors introduce PeaCoK, a large-scale persona commonsense knowledge graph containing approximately 100K human-validated persona facts, aimed at helping systems generate more consistent and engaging narratives by schematizing five dimensions of persona knowledge.\\n   - URL: [http://arxiv.org/abs/2305.02364v2](http://arxiv.org/abs/2305.02364v2)\\n\\n2. Paper title: Detecting Speaker Personas from Conversational Texts\\n   - Summary: This research addresses the challenge of detecting speaker personas from plain conversational texts through a task called Speaker Persona Detection (SPD). The study introduces U2P (utterance-to-profile) matching networks that outperform baseline models, offering improved semantic matching and precise detection of personas.\\n   - URL: [http://arxiv.org/abs/2109.01330v1](http://arxiv.org/abs/2109.01330v1)\\n\\n3. Paper title: Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation\\n   - Summary: This paper presents an evaluation of retrieval-augmented generation (RAG) systems integrated with fair rankings, aiming to ensure item-side fairness. By analyzing different RAG systems, the study finds that those with fair rankings can maintain high generation quality and may outperform traditional systems, proposing a framework for equitable RAG systems.\\n   - URL: [http://arxiv.org/abs/2409.11598v2](http://arxiv.org/abs/2409.11598v2)\\n\\n4. Paper title: A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation\\n   - Summary: The study introduces a stack-propagation framework aimed at generating personalized dialogues in low-resource settings. Utilizing Transformer encoders and decoders, this framework improves response quality and persona consistency without relying heavily on persona-dense data, overcoming limitations of traditional models.\\n   - URL: [http://arxiv.org/abs/2410.20174v1](http://arxiv.org/abs/2410.20174v1)\\n\\n5. Paper title: A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\\n   - Summary: The authors propose a theoretical framework to quantify and balance the benefit and detriment of retrieval-augmented generation (RAG) on token predictions. The study introduces Tok-RAG, a method for collaborative generation at the token level, demonstrating effectiveness in enhancing LLMs' performance while mitigating misleading information retrieval across various tasks.\\n   - URL: [http://arxiv.org/abs/2406.00944v2](http://arxiv.org/abs/2406.00944v2)\\n\\nBest regards,\\n\\n[Your Name]\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Test Query: find news us president speech in 7 am\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-07 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-07 06:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"news\",\n",
      "  \"keywords\": [\n",
      "    \"news\",\n",
      "    \"us\",\n",
      "    \"president\",\n",
      "    \"speech\",\n",
      "    \"in\",\n",
      "    \"7\",\n",
      "    \"am\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"find news us president speech in 7 am\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing news search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"title\": \"Trump Tariffs Live Updates: Beijing Criticizes ‘Politicization’ Of Chinese Companies After USPS Halts Incoming Packages From China\",\n",
      "      \"description\": \"Economists have broadly predicted Trump’s tariffs will raise prices and harm the economy.\",\n",
      "      \"url\": \"https://www.forbes.com/sites/alisondurkee/2025/02/05/trump-tariffs-live-updates-beijing-criticizes-politicization-of-chinese-companies-after-usps-halts-incoming-packages-from-china/\",\n",
      "      \"published_at\": \"2025-02-05T10:50:59Z\",\n",
      "      \"source\": \"Forbes\",\n",
      "      \"content\": \"President Donald Trumps sweeping tariffs on imports from China went into effect after midnight on Tuesdaytriggering retaliatory levies by Beijing that some fear could escalate into a trade wareven as… [+20608 chars]\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_results\": 1,\n",
      "  \"returned_count\": 1,\n",
      "  \"search_parameters\": {\n",
      "    \"keywords\": [\n",
      "      \"news\",\n",
      "      \"us\",\n",
      "      \"president\",\n",
      "      \"speech\",\n",
      "      \"in\",\n",
      "      \"7\",\n",
      "      \"am\"\n",
      "    ],\n",
      "    \"from_date\": \"2025-02-05\",\n",
      "    \"language\": \"en\"\n",
      "  },\n",
      "  \"search_type\": \"news\",\n",
      "  \"search_duration\": 0.303273,\n",
      "  \"search_timestamp\": \"2025-02-06T11:07:33.469348+00:00\",\n",
      "  \"original_query\": \"find news us president speech in 7 am\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [find news us president speech in 7 am]\",\n",
      "  \"body\": \"Subject: Latest News Updates for \\\"find news US President speech in 7 am\\\"\\n\\nBody:\\n\\nHello,\\n\\nHere are the latest news articles related to your search topic.\\n\\n1. Trump Tariffs Live Updates: Beijing Criticizes ‘Politicization’ Of Chinese Companies After USPS Halts Incoming Packages From China - Forbes\\n   - Main content: President Donald Trump's sweeping tariffs on imports from China have taken effect, prompting Beijing to implement retaliatory measures amid concerns that this could lead to a trade war. Economists are broadly predicting that these tariffs will result in higher prices and potential economic harm.\\n   - Published date: February 5, 2025\\n   - URL: [Trump Tariffs Article](https://www.forbes.com/sites/alisondurkee/2025/02/05/trump-tariffs-live-updates-beijing-criticizes-politicization-of-chinese-companies-after-usps-halts-incoming-packages-from-china/)\\n\\nThank you for choosing our news update service. If you have any further inquiries or need more information, feel free to reach out.\\n\\nBest regards,\\n\\n[Your Name]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def test_search_system():\n",
    "    query_analyzer = QueryAnalysisAgent()\n",
    "    search_router = SearchRouter()\n",
    "    response_agent = ResponseAgent()  # Added\n",
    "\n",
    "    test_queries = [\n",
    "        \"recommend place to eat at seoul in 7 pm\",\n",
    "        \"find rag persona paper at 3 pm\",\n",
    "        \"find news us president speech in 7 am\",\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Test Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            query_analysis = query_analyzer.analyze_query(query)\n",
    "            print(\"\\n1. Query Analysis Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    query_analysis,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if query_analysis[\"status\"] != \"success\":\n",
    "                print(\"Query analysis failed!\")\n",
    "                continue\n",
    "\n",
    "            search_results = search_router.route_and_search(query_analysis)\n",
    "            print(\"\\n2. Search Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    search_results,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Added: Result formatting\n",
    "            print(\"\\n3. Formatted Results:\")\n",
    "            formatted_results = response_agent.format_results(search_results)\n",
    "            print(json.dumps(formatted_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during test: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_search_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling System and Email Service\n",
    "\n",
    "The `ScheduledSearchSystem` manages the complete lifecycle of search tasks, from scheduling to result delivery. Here's its core structure and functionality:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. Collection Management\n",
    "2. Task Scheduling\n",
    "3. Search Execution\n",
    "4. Email Delivery\n",
    "\n",
    "\n",
    "The system uses threading for non-blocking operation and includes comprehensive logging for monitoring task progress and debugging issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "import yagmail\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "class ScheduledSearchSystem:\n",
    "    def __init__(self, email_config: Dict[str, Any]):\n",
    "        print(f\"\\n[{self._get_current_time()}] Initializing system...\")\n",
    "        self.query_analyzer = QueryAnalysisAgent()\n",
    "        self.search_router = SearchRouter()\n",
    "        self.response_agent = ResponseAgent()\n",
    "        self.client = chromadb.PersistentClient(path=\"./search_data\")\n",
    "\n",
    "        # Email configuration\n",
    "        self.email_config = email_config\n",
    "        self.yag = yagmail.SMTP(email_config[\"username\"], email_config[\"password\"])\n",
    "        print(f\"[{self._get_current_time()}] Email client configuration complete\")\n",
    "\n",
    "        self.scheduled_tasks = {}\n",
    "        self.setup_collections()\n",
    "\n",
    "        # Add completion flag\n",
    "        self.is_completed = False\n",
    "        self.completion_event = threading.Event()\n",
    "\n",
    "        # Start scheduler\n",
    "        self.scheduler_thread = threading.Thread(target=self._run_scheduler)\n",
    "        self.scheduler_thread.daemon = True\n",
    "        self.scheduler_thread.start()\n",
    "        print(f\"[{self._get_current_time()}] System initialization complete\\n\")\n",
    "\n",
    "    def _get_current_time(self):\n",
    "        \"\"\"Return current time as string\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def setup_collections(self):\n",
    "        \"\"\"Set up ChromaDB collections\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Starting ChromaDB collection setup...\")\n",
    "        self.results_collection = self.client.get_or_create_collection(\n",
    "            name=\"search_results\", metadata={\"description\": \"Raw search results\"}\n",
    "        )\n",
    "\n",
    "        self.formatted_collection = self.client.get_or_create_collection(\n",
    "            name=\"formatted_responses\",\n",
    "            metadata={\"description\": \"Formatted responses for email delivery\"},\n",
    "        )\n",
    "\n",
    "        self.schedule_collection = self.client.get_or_create_collection(\n",
    "            name=\"scheduled_tasks\",\n",
    "            metadata={\"description\": \"Scheduled search and email tasks\"},\n",
    "        )\n",
    "        print(f\"[{self._get_current_time()}] ChromaDB collection setup complete\")\n",
    "\n",
    "    def schedule_task(self, query: str, user_email: str) -> Dict[str, Any]:\n",
    "        \"\"\"Schedule a task\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n[{self._get_current_time()}] Starting new task scheduling...\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Email: {user_email}\")\n",
    "\n",
    "            # Query analysis\n",
    "            query_analysis = self.query_analyzer.analyze_query(query)\n",
    "            execution_time = query_analysis[\"execution_time\"]\n",
    "            target_time = query_analysis[\"target_time\"]\n",
    "\n",
    "            print(f\"Scheduled search execution time: {execution_time}\")\n",
    "            print(f\"Scheduled email delivery time: {target_time}\")\n",
    "\n",
    "            # Generate task ID\n",
    "            schedule_id = f\"task_{datetime.now(pytz.UTC).timestamp()}\"\n",
    "            print(f\"Generated task ID: {schedule_id}\")\n",
    "\n",
    "            # Save task information\n",
    "            task_info = {\n",
    "                \"query\": query,\n",
    "                \"email\": user_email,\n",
    "                \"execution_time\": execution_time.isoformat(),\n",
    "                \"target_time\": target_time.isoformat(),\n",
    "                \"search_type\": query_analysis[\"search_type\"],\n",
    "                \"status\": \"scheduled\",\n",
    "            }\n",
    "\n",
    "            self.scheduled_tasks[schedule_id] = task_info\n",
    "\n",
    "            # Save to ChromaDB\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Saving task information to ChromaDB...\"\n",
    "            )\n",
    "            self.schedule_collection.add(\n",
    "                documents=[json.dumps(task_info)],\n",
    "                metadatas=[{\"type\": \"schedule\", \"status\": \"pending\"}],\n",
    "                ids=[schedule_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Task information saved\")\n",
    "\n",
    "            # Schedule search execution\n",
    "            execution_time_str = execution_time.strftime(\"%H:%M\")\n",
    "            schedule.every().day.at(execution_time_str).do(\n",
    "                self.execute_search, schedule_id=schedule_id\n",
    "            ).tag(schedule_id)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Search task scheduling complete\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Task successfully scheduled\",\n",
    "                \"schedule_id\": schedule_id,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"target_time\": target_time,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Task scheduling failed: {str(e)}\")\n",
    "            return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "    def execute_search(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Execute search\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting search execution (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            task_info = self.scheduled_tasks.get(schedule_id)\n",
    "            if not task_info:\n",
    "                print(f\"[{self._get_current_time()}] Task information not found\")\n",
    "                return False\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Analyzing search query...\")\n",
    "            query_analysis = self.query_analyzer.analyze_query(task_info[\"query\"])\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Performing search...\")\n",
    "            search_results = self.search_router.route_and_search(query_analysis)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Formatting search results...\")\n",
    "            formatted_response = self.response_agent.format_results(search_results)\n",
    "\n",
    "            # Save results\n",
    "            print(f\"[{self._get_current_time()}] Saving search results to ChromaDB...\")\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            self.formatted_collection.add(\n",
    "                documents=[json.dumps(formatted_response)],\n",
    "                metadatas=[\n",
    "                    {\n",
    "                        \"schedule_id\": schedule_id,\n",
    "                        \"email\": task_info[\"email\"],\n",
    "                        \"target_time\": task_info[\"target_time\"],\n",
    "                    }\n",
    "                ],\n",
    "                ids=[response_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Search results saved\")\n",
    "\n",
    "            # Schedule email delivery\n",
    "            target_time = datetime.fromisoformat(task_info[\"target_time\"])\n",
    "            target_time_str = target_time.strftime(\"%H:%M\")\n",
    "\n",
    "            schedule.every().day.at(target_time_str).do(\n",
    "                self.send_email, schedule_id=schedule_id\n",
    "            ).tag(f\"email_{schedule_id}\")\n",
    "\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Email delivery scheduled (Time: {target_time_str})\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Search execution failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def send_email(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Send email\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting email delivery (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            print(f\"[{self._get_current_time()}] Retrieving saved search results...\")\n",
    "            response_results = self.formatted_collection.get(ids=[response_id])\n",
    "\n",
    "            if not response_results[\"documents\"]:\n",
    "                print(f\"[{self._get_current_time()}] Search results not found\")\n",
    "                return False\n",
    "\n",
    "            formatted_response = json.loads(response_results[\"documents\"][0])\n",
    "            metadata = response_results[\"metadatas\"][0]\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Sending email...\")\n",
    "            print(f\"Recipient: {metadata['email']}\")\n",
    "            print(f\"Subject: {formatted_response['subject']}\")\n",
    "\n",
    "            self.yag.send(\n",
    "                to=metadata[\"email\"],\n",
    "                subject=formatted_response[\"subject\"],\n",
    "                contents=formatted_response[\"body\"],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Email sent successfully\")\n",
    "\n",
    "            # Update task status\n",
    "            print(f\"[{self._get_current_time()}] Updating task status...\")\n",
    "            task_info = self.scheduled_tasks[schedule_id]\n",
    "            task_info[\"status\"] = \"completed\"\n",
    "            self.schedule_collection.update(\n",
    "                documents=[json.dumps(task_info)], ids=[schedule_id]\n",
    "            )\n",
    "\n",
    "            # Clear schedule\n",
    "            schedule.clear(f\"email_{schedule_id}\")\n",
    "            print(f\"[{self._get_current_time()}] Task completion processing complete\\n\")\n",
    "\n",
    "            # Set completion flag\n",
    "            self.is_completed = True\n",
    "            self.completion_event.set()\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] All tasks completed. Shutting down system.\\n\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Email delivery failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _run_scheduler(self):\n",
    "        \"\"\"Run scheduler\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Scheduler started...\")\n",
    "        while not self.is_completed:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)  # Check every second\n",
    "\n",
    "    def wait_for_completion(self, timeout=None):\n",
    "        \"\"\"Wait for task completion\"\"\"\n",
    "        try:\n",
    "            completed = self.completion_event.wait(timeout=timeout)\n",
    "            if not completed:\n",
    "                print(f\"[{self._get_current_time()}] Task completion timeout\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n[{self._get_current_time()}] Terminated by user.\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Scheduler System Usage Guide\n",
    "The work starts 5 minutes before the work request time.\n",
    "\n",
    "1. Email Configuration\n",
    "- Enable Gmail 2-Step Verification\n",
    "- Generate App Password: https://myaccount.google.com/security > 2-Step Verification > App passwords\n",
    "```python\n",
    "email_config = {\n",
    "    \"username\": \"your_email@gmail.com\",\n",
    "    \"password\": \"your_app_password\",  # Gmail app password\n",
    "    \"smtp_server\": \"smtp.gmail.com\",\n",
    "    \"smtp_port\": 587\n",
    "}\n",
    "```\n",
    "\n",
    "2. Initialize System and Schedule Task\n",
    "```python\n",
    "# Initialize system\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# Schedule task\n",
    "result = system.schedule_task(\n",
    "    query=\"find AI papers at 9 AM\",  # Search query to execute\n",
    "    user_email=\"your_email@gmail.com\"  # Email to receive results\n",
    ")\n",
    "```\n",
    "\n",
    "3. Wait for Completion\n",
    "```python\n",
    "# Wait for task completion (max 4 hours)\n",
    "system.wait_for_completion(timeout=14400)\n",
    "```\n",
    "\n",
    "Search results will be automatically emailed to the specified address upon completion.\n",
    "\n",
    "The system supports various query types:\n",
    "- Research papers: \"find RAG papers at 7 AM\"\n",
    "- News: \"find AI news at 9 AM\"\n",
    "- General search: \"find restaurants in Seoul at 6 PM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 스케줄링 시스템 테스트 시작 ===\n",
      "\n",
      "\n",
      "[2025-02-06 20:08:00] Initializing system...\n",
      "[2025-02-06 20:08:00] Email client configuration complete\n",
      "[2025-02-06 20:08:00] Starting ChromaDB collection setup...\n",
      "[2025-02-06 20:08:00] ChromaDB collection setup complete\n",
      "[2025-02-06 20:08:00] Scheduler started...\n",
      "[2025-02-06 20:08:00] System initialization complete\n",
      "\n",
      "\n",
      "[2025-02-06 20:08:00] Starting new task scheduling...\n",
      "Query: find Modular RAG paper at 20:15 PM\n",
      "Email: jik9210@gmail.com\n",
      "Scheduled search execution time: 2025-02-06 20:10:00+00:00\n",
      "Scheduled email delivery time: 2025-02-06 20:15:00+00:00\n",
      "Generated task ID: task_1738840083.450023\n",
      "[2025-02-06 20:08:03] Saving task information to ChromaDB...\n",
      "[2025-02-06 20:08:04] Task information saved\n",
      "[2025-02-06 20:08:04] Search task scheduling complete\n",
      "\n",
      "=== 예약 결과 ===\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"message\": \"Task successfully scheduled\",\n",
      "  \"schedule_id\": \"task_1738840083.450023\",\n",
      "  \"execution_time\": \"2025-02-06 20:10:00+00:00\",\n",
      "  \"target_time\": \"2025-02-06 20:15:00+00:00\"\n",
      "}\n",
      "\n",
      "=== 예약된 시간에 작업이 실행됩니다... ===\n",
      "\n",
      "\n",
      "[2025-02-06 20:10:00] Starting search execution (ID: task_1738840083.450023)\n",
      "[2025-02-06 20:10:00] Analyzing search query...\n",
      "[2025-02-06 20:10:03] Performing search...\n",
      "Performing research paper search...\n",
      "[2025-02-06 20:10:04] Formatting search results...\n",
      "[2025-02-06 20:10:58] Saving search results to ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: response_task_1738168330.927504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-06 20:10:58] Search results saved\n",
      "[2025-02-06 20:10:58] Email delivery scheduled (Time: 20:15)\n",
      "\n",
      "[2025-02-06 20:15:00] Starting email delivery (ID: task_1738840083.450023)\n",
      "[2025-02-06 20:15:00] Retrieving saved search results...\n",
      "[2025-02-06 20:15:00] Sending email...\n",
      "Recipient: jik9210@gmail.com\n",
      "Subject: Search Results for [find Modular RAG paper at 20:15 PM]\n",
      "[2025-02-06 20:15:03] Email sent successfully\n",
      "[2025-02-06 20:15:03] Updating task status...\n",
      "[2025-02-06 20:15:03] Task completion processing complete\n",
      "\n",
      "[2025-02-06 20:15:03] All tasks completed. Shutting down system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 시스템 설정\n",
    "email_config = {\n",
    "    \"username\": \"jik9210@gmail.com\",\n",
    "    \"password\": \"yvkw bdlc rytq cjaj\",  # Gmail app password\n",
    "    \"smtp_server\": \"smtp.gmail.com\",\n",
    "    \"smtp_port\": 587,\n",
    "}\n",
    "\n",
    "print(\"\\n=== 스케줄링 시스템 테스트 시작 ===\\n\")\n",
    "\n",
    "# 시스템 초기화\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# 작업 예약\n",
    "result = system.schedule_task(\n",
    "    query=\"find Modular RAG paper at 20:15 PM\",  # 예: 오후 7시 45분\n",
    "    user_email=\"jik9210@gmail.com\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== 예약 결과 ===\")\n",
    "print(json.dumps(result, indent=2, default=str))\n",
    "print(\"\\n=== 예약된 시간에 작업이 실행됩니다... ===\\n\")\n",
    "\n",
    "# 작업 완료까지 대기 (최대 4시간)\n",
    "system.wait_for_completion(timeout=14400)  # 4시간 = 14400초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
