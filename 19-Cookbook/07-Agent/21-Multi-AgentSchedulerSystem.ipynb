{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Scheduler System\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design: \n",
    "- Peer Review: [Mark()](https://github.com/obov), [Taylor(Jihyun Kim)](https://github.com/Taylor0819)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Multi-Agent Scheduler System represents an innovative approach to automating information retrieval and delivery through a coordinated network of specialized AI agents. At its core, this system transforms simple natural language requests into scheduled, automated search and delivery operations, making it particularly valuable for researchers, analysts, and anyone needing regular, scheduled information updates.\n",
    "\n",
    "Imagine asking \"Find me the latest RAG papers at 7 AM tomorrow.\" Instead of manually searching and compiling information early in the morning, the system automatically handles the entire process - from understanding your request to delivering a well-formatted email with relevant research papers at precisely 7 AM. This automation eliminates the need for manual intervention while ensuring timely delivery of crucial information.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "The system's architecture is built around five specialized agents, each handling a crucial aspect of the information retrieval and delivery process:\n",
    "\n",
    "1. `Query Analysis Agent`\n",
    "   This agent serves as the system's front door, interpreting natural language queries to extract critical information \n",
    "\n",
    "2. `Search Router`\n",
    "   Acting as the system's traffic controller, the Search Router directs queries to the most appropriate specialized search agent:\n",
    "   \n",
    "3. `Response Agent`\n",
    "   This agent transforms raw search results into well-structured, readable content by:\n",
    "\n",
    "4. `Scheduling System and Email Service`\n",
    "   The scheduling component manages the temporal aspects of the system:\n",
    "   This ensures that all operations occur at their specified times without conflicts.\n",
    "   The system implements a robust email delivery service using yagmail that provides:\n",
    "\n",
    "### System Flow\n",
    "\n",
    "The entire process follows this sequence:\n",
    "\n",
    "![Multi-Agent Scheduler System Flow](assets/21-Multi-AgentSchedulerSystem.png)\n",
    "\n",
    "This architecture ensures reliable, automated information retrieval and delivery, with each agent optimized for its specific role in the process.\n",
    "\n",
    "### Table of Contents\n",
    "- [Overview](#overview)\n",
    "- [System Architecture](#system-architecture)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Query Analysis Agent](#query-analysis-agent)\n",
    "- [Search Router and Specialized Agents](#Search-Router-and-Specialized-Agents)\n",
    "- [Response Agent](#response-agent)\n",
    "- [Scheduling System and Email Service](#Scheduling-System-and-Email-Service)  \n",
    "\n",
    "The system's modular design allows for easy expansion and customization, making it adaptable to various use cases while maintaining consistent performance and reliability. Whether you're tracking research papers, monitoring news, or gathering general information, the Multi-Agent Scheduler System automates the entire process from query to delivery, saving time and ensuring consistent, timely access to important information.\n",
    "\n",
    "### References\n",
    "- [How to get NewsAPI](https://newsapi.org)\n",
    "- [How to get SerpAPI](https://serpapi.com/)\n",
    "- [How to get Google password](https://support.google.com/accounts/answer/185833?visit_id=638745290390245053-2925662375&p=InvalidSecondFactor&rd=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"chromadb\",\n",
    "        \"langchain_chroma\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"pytz\",\n",
    "        \"google-search-results\",\n",
    "        \"yagmail\",\n",
    "        \"schedule\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"21-Multi-AgentSchedulerSystem\",\n",
    "        \"NEWS_API_KEY\": \"\",\n",
    "        \"SERPAPI_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Analysis Agent\n",
    "The QueryAnalysisAgent serves as the initial interpreter in our multi-agent scheduler system, transforming natural language queries into structured data that other agents can process. This agent employs advanced language understanding capabilities through GPT-4 to accurately parse user intentions and timing requirements.\n",
    "\n",
    "### Core Components\n",
    "The agent is built around three essential classes:\n",
    "- Time Extraction Processor: Handles temporal information\n",
    "- Task Analysis Engine: Understands search requirements\n",
    "- Query Coordinator: Combines and validates results\n",
    "\n",
    "\n",
    "### Core Functionality\n",
    "The agent performs two primary functions:\n",
    "\n",
    "1. Time Extraction\n",
    "```python\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from queries\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "    # Converts to standardized datetime format\n",
    "    return self._process_time(time_str)\n",
    "```\n",
    "\n",
    "2. Task Analysis\n",
    "```python\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Analyzes query content for search parameters\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "    return self._parse_response(response)\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize agent\n",
    "agent = QueryAnalysisAgent()\n",
    "\n",
    "# Process a sample query\n",
    "query = \"Find RAG papers tomorrow at 9 AM\"  # Changed from \"내일 오전 9시에 RAG 논문 찾아줘\"\n",
    "result = agent.analyze_query(query)\n",
    "\n",
    "Expected output:\n",
    "```json\n",
    "{\n",
    "    \"target_time\": \"2025-02-06 09:00:00+0000\",\n",
    "    \"execution_time\": \"2025-02-06 08:55:00+0000\",\n",
    "    \"task_type\": \"search\",\n",
    "    \"search_type\": \"research_paper\",\n",
    "    \"keywords\": [\"RAG\", \"papers\"],  # Changed from [\"RAG\", \"논문\"]\n",
    "    \"requirements\": \"minimum 5 results\",\n",
    "    \"time_sensitivity\": \"normal\",\n",
    "    \"original_query\": \"Find RAG papers tomorrow at 9 AM\",  # Changed from Korean query\n",
    "    \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "\n",
    "This structured approach ensures reliable query interpretation while maintaining flexibility for various query types and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing Required Libraries\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QueryAnalysisAgent class represents a specialized natural language query processor that utilizes OpenAI's language models. Let's break down its core components:\n",
    "The initialization method sets up the language model with temperature=0 to ensure consistent, deterministic responses:\n",
    "\n",
    "The setup_prompt_templates method defines two essential templates:\n",
    "\n",
    "1. Time Extraction Template\n",
    "This template focuses solely on extracting and standardizing time information from queries.\n",
    "\n",
    "2. Task Analysis Template\n",
    "This template structures the query analysis with specific rules:\n",
    "\n",
    "- Categorizes searches into three types: research_paper, news, or general\n",
    "- Distinguishes between normal and urgent time sensitivity\n",
    "- Separates search keywords from temporal terms\n",
    "- Maintains consistent task typing as \"search\"\n",
    "\n",
    "These templates work together to transform natural language queries into structured, actionable data that the rest of the system can process efficiently. The clear separation between time extraction and task analysis allows for precise handling of each aspect of the query.\n",
    "\n",
    "For example, a query like \"아침 7시에 RAG 논문 찾아줘\" would be processed to extract both the time (07:00) and the search parameters (research papers about RAG), while filtering out temporal terms from the actual search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Class Definition and __init__, setup_prompt_templates Methods\n",
    "class QueryAnalysisAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        self.time_extraction_prompt = PromptTemplate.from_template(\n",
    "            \"Extract time and convert to 24h format from: {query}\\nReturn HH:MM only\"\n",
    "        )\n",
    "\n",
    "        self.task_analysis_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Analyze the query and return only a JSON object. For the query: {query}\n",
    "\n",
    "        Return this exact format:\n",
    "        {{\n",
    "            \"task_type\": \"search\",\n",
    "            \"search_type\": \"research_paper\",\n",
    "            \"keywords\": [\"rag\", \"papers\"],\n",
    "            \"requirements\": \"minimum 5 results\",\n",
    "            \"time_sensitivity\": \"normal\",\n",
    "            \"search_terms\": [\"rag\", \"papers\"]  # Actual keywords to use for search\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        - search_type must be one of: \"research_paper\", \"news\", \"general\"\n",
    "        - time_sensitivity must be one of: \"normal\", \"urgent\"\n",
    "        - keywords should include all words from query including time-related terms\n",
    "        - search_terms should exclude time-related terms and only include actual search keywords\n",
    "        - task_type should always be \"search\"\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 1\n",
    "\n",
    "`extract_time()`\n",
    "- Functionality: Extracts and processes time information from natural language queries\n",
    "- Features:\n",
    "  - Converts various time formats (e.g., \"아침 7시\", \"오후 3:30\") to standardized datetime objects\n",
    "  - Maintains timezone awareness using pytz for accurate scheduling\n",
    "  - Automatically schedules for next day if requested time has already passed\n",
    "  - Strips unnecessary time components (seconds, microseconds) for cleaner scheduling\n",
    "- Error Handling: Raises ValueError with detailed error messages for invalid time formats\n",
    "- Returns: UTC-aware datetime object representing the target execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Adding extract_time Method\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from the query and returns a datetime object.\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Extract the actual time string from the ChatCompletion response\n",
    "        time_str = time_str.content.strip()\n",
    "\n",
    "        # Calculate the next scheduled time based on the current time\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        hour, minute = map(int, time_str.split(\":\"))\n",
    "\n",
    "        target_time = current_time.replace(\n",
    "            hour=hour, minute=minute, second=0, microsecond=0\n",
    "        )\n",
    "\n",
    "        # If the extracted time has already passed, set it for the next day\n",
    "        if target_time <= current_time:\n",
    "            target_time += timedelta(days=1)\n",
    "\n",
    "        return target_time\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Time extraction failed: {e}\")\n",
    "\n",
    "\n",
    "# After executing this cell, the method should be added to the class\n",
    "QueryAnalysisAgent.extract_time = extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 2\n",
    "\n",
    "`analyze_task()`\n",
    "- Functionality: Breaks down queries into structured task components\n",
    "- Features:\n",
    "  - Identifies search type (research_paper, news, general)\n",
    "  - Extracts relevant keywords while filtering temporal terms\n",
    "  - Determines task urgency (normal vs urgent)\n",
    "  - Identifies specific requirements (e.g., minimum result count)\n",
    "- Error Handling: Handles JSON parsing errors and invalid query formats\n",
    "- Returns: Dictionary containing parsed task information and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Adding analyze_task Method\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Extracts task intent and keywords from the query.\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Clean response content to ensure valid JSON format\n",
    "        content = response.content.strip()\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Original response: {response.content}\")\n",
    "        raise ValueError(f\"Failed to parse task analysis result: {e}\")\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_task = analyze_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 3\n",
    "\n",
    "`analyze_query()`\n",
    "- Functionality: Combines time extraction and task analysis into a complete query interpretation\n",
    "- Features:\n",
    "  - Coordinates between time extraction and task analysis\n",
    "  - Sets execution time 5 minutes before target time\n",
    "  - Validates and combines all query components\n",
    "- Error Handling: Catches and reports errors from both time and task processing\n",
    "- Returns: Combined dictionary with timing and task information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(self, query: str) -> dict:\n",
    "    \"\"\"Performs a full query analysis and returns the results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract time information\n",
    "        target_time = self.extract_time(query)\n",
    "\n",
    "        # Analyze task information\n",
    "        task_info = self.analyze_task(query)\n",
    "\n",
    "        # Return the results including all necessary details\n",
    "        return {\n",
    "            \"target_time\": target_time,\n",
    "            \"execution_time\": target_time - timedelta(minutes=5),\n",
    "            \"task_type\": task_info[\"task_type\"],\n",
    "            \"search_type\": task_info[\"search_type\"],  # Newly added\n",
    "            \"keywords\": task_info[\"keywords\"],\n",
    "            \"requirements\": task_info[\"requirements\"],\n",
    "            \"time_sensitivity\": task_info[\"time_sensitivity\"],  # Newly added\n",
    "            \"original_query\": query,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"original_query\": query}\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_query = analyze_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 4\n",
    "\n",
    "`datetime_handler(obj)`\n",
    "* Functionality: Converts datetime objects to JSON-serializable string format\n",
    "* Features:\n",
    "   * Accepts any object and checks if it's a datetime instance\n",
    "   * Converts datetime to standardized string format (YYYY-MM-DD HH:MM:SS+ZZZZ)\n",
    "   * Maintains timezone information in the output string\n",
    "* Error Handling: Raises TypeError with descriptive message for non-datetime objects\n",
    "* Returns: String representation of datetime in consistent format\n",
    "* Use Cases:\n",
    "   * JSON serialization for API responses\n",
    "   * Database storage of temporal data\n",
    "   * Logging and debugging timestamp formatting\n",
    "* Examples:\n",
    "   * Input: `datetime(2024, 2, 6, 15, 30, tzinfo=pytz.UTC)`\n",
    "   * Output: `\"2024-02-06 15:30:00+0000\"`\n",
    "\n",
    "The function serves as a critical utility for converting Python's datetime objects into a standardized string format that can be easily stored, transmitted, and later reconstructed. This is particularly important in our scheduling system where accurate time representation and timezone awareness are essential for reliable task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Testing QueryAnalysisAgent\n",
    "def datetime_handler(obj):\n",
    "    \"\"\"Handler to convert datetime objects into JSON serializable strings.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted datetime string (Format: YYYY-MM-DD HH:MM:SS+ZZZZ).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Raised if the object is not a datetime instance.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilgyun/.pyenv/versions/3.11.11/lib/python3.11/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=7ecfe457-b9e4-4653-819e-3adb9473d2c5,id=7ecfe457-b9e4-4653-819e-3adb9473d2c5; trace=7ecfe457-b9e4-4653-819e-3adb9473d2c5,id=e61dafec-908c-48f9-a7fb-c19a038d24e8; trace=7ecfe457-b9e4-4653-819e-3adb9473d2c5,id=d1500e6f-541a-4920-896c-203d4fe745c8\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=b73e5269-54b3-4702-b8cb-479d0bcac8a8,id=b73e5269-54b3-4702-b8cb-479d0bcac8a8; trace=b73e5269-54b3-4702-b8cb-479d0bcac8a8,id=ca5225c3-c44f-45f9-be48-fe208bbb1065; trace=b73e5269-54b3-4702-b8cb-479d0bcac8a8,id=cafbba0c-9d77-4fca-95da-020a0b16890f; trace=7ecfe457-b9e4-4653-819e-3adb9473d2c5,id=7ecfe457-b9e4-4653-819e-3adb9473d2c5; trace=7ecfe457-b9e4-4653-819e-3adb9473d2c5,id=d1500e6f-541a-4920-896c-203d4fe745c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QueryAnalysisAgent Test Result ===\n",
      "{\n",
      "  \"target_time\": \"2025-02-08 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-08 06:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"news\",\n",
      "  \"keywords\": [\n",
      "    \"rag\",\n",
      "    \"7 AM\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"Find and recommend news related to RAG at 7 AM.\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_query_test():\n",
    "    \"\"\"Runs a test for QueryAnalysisAgent and returns the results.\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis result in JSON format.\n",
    "    \"\"\"\n",
    "    # Create an instance of QueryAnalysisAgent\n",
    "    agent = QueryAnalysisAgent()\n",
    "\n",
    "    # Test query\n",
    "    test_query = \"Find and recommend news related to RAG at 7 AM.\"\n",
    "\n",
    "    # Execute query analysis\n",
    "    result = agent.analyze_query(test_query)\n",
    "\n",
    "    # Convert the result to JSON format\n",
    "    return json.dumps(result, indent=2, ensure_ascii=False, default=datetime_handler)\n",
    "\n",
    "\n",
    "# Execute test and print results\n",
    "if __name__ == \"__main__\":\n",
    "    test_result = run_query_test()\n",
    "    print(\"\\n=== QueryAnalysisAgent Test Result ===\")\n",
    "    print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Router and Specialized Agents\n",
    "\n",
    "The Search Router system acts as an intelligent traffic controller, directing queries to the most appropriate specialized search agent based on the query analysis. This architecture ensures that each type of search request is handled by an agent specifically optimized for that domain.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```python\n",
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Initialize specialized search agents\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "```\n",
    "Each specialized agent is designed to handle specific types of searches:\n",
    "\n",
    "1. `Paper Search Agent`\n",
    "This agent specializes in academic paper searches, interfacing with arXiv's API to retrieve scholarly articles and research papers.\n",
    "\n",
    "2. `News Search Agent`\n",
    "This agent handles news-related searches, connecting to NewsAPI to gather current events and news articles.\n",
    "\n",
    "3. `General Search Agent`\n",
    "This agent manages general web searches using SerpAPI, handling broader information gathering needs.\n",
    "\n",
    "This routing system ensures that each query is handled by the most appropriate agent while maintaining consistent error handling and result formatting across all search types. The modular design allows for easy addition of new specialized agents as needed, making the system highly extensible and maintainable.\n",
    "\n",
    "Each agent provides standardized outputs despite their different data sources and search methodologies, enabling seamless integration with the rest of the system components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PaperSearchAgent`\n",
    "\n",
    "This agent focuses on academic content retrieval. It interfaces with the arXiv API to fetch scholarly papers and research documents. Key features include filtering papers by relevance, date ranges, and processing XML responses into structured data. The agent is particularly useful for researchers and academics needing current papers in their field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class PaperSearchAgent:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def perform_search(self, query_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        try:\n",
    "            keywords = self._process_keywords(query_info[\"keywords\"])\n",
    "            max_results = self._extract_max_results(query_info.get(\"requirements\", \"\"))\n",
    "\n",
    "            url = f\"{self.base_url}?search_query=all:{keywords}&start=0&max_results={max_results}\"\n",
    "            response = urllib.request.urlopen(url)\n",
    "            data = response.read().decode(\"utf-8\")\n",
    "\n",
    "            results = self._parse_arxiv_results(data)\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": results,\n",
    "                \"total_found\": len(results),\n",
    "                \"returned_count\": len(results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _process_keywords(self, keywords: List[str]) -> str:\n",
    "        # Remove time-related keywords\n",
    "        filtered_keywords = [\n",
    "            k\n",
    "            for k in keywords\n",
    "            if not any(\n",
    "                time in k.lower()\n",
    "                for time in [\"hour\", \"morning\", \"afternoon\", \"evening\"]\n",
    "            )\n",
    "        ]\n",
    "        return \"+\".join(filtered_keywords)\n",
    "\n",
    "    def _extract_max_results(self, requirements: str) -> int:\n",
    "        import re\n",
    "\n",
    "        # extracting numbers\n",
    "        numbers = re.findall(r\"\\d+\", requirements)\n",
    "        return int(numbers[0]) if numbers else 5\n",
    "\n",
    "    def _parse_arxiv_results(self, data: str) -> List[Dict[str, Any]]:\n",
    "        root = ET.fromstring(data)\n",
    "        results = []\n",
    "\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            url = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "            published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"type\": \"research_paper\",\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"published_date\": published[:10],\n",
    "                    \"summary\": summary,\n",
    "                    \"source\": \"arxiv\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NewsSearchAgent`\n",
    "\n",
    "This agent handles current events and news article searches. It connects to NewsAPI to access a wide range of news sources. The agent supports features like language filtering, date range specification, and source selection. It's especially valuable for users needing real-time information or tracking specific topics in the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class NewsSearchAgent:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initializes a news search agent using NewsAPI.\n",
    "\n",
    "        NewsAPI follows a REST API structure with the base URL 'https://newsapi.org/v2'.\n",
    "        It provides two main endpoints:\n",
    "        - /everything: Searches the entire news archive.\n",
    "        - /top-headlines: Retrieves the latest top headlines.\n",
    "        \"\"\"\n",
    "\n",
    "        self.api_key = os.environ[\"NEWS_API_KEY\"]\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"NewsAPI key is required\")\n",
    "\n",
    "        self.base_url = \"https://newsapi.org/v2\"\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs a news search based on the given query information.\n",
    "\n",
    "        Args:\n",
    "            query_info (Dict[str, Any]): Dictionary containing search parameters.\n",
    "            max_results (int): Maximum number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing search results or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract actual search terms (excluding time-related keywords)\n",
    "            search_terms = query_info.get(\n",
    "                \"search_terms\", query_info.get(\"keywords\", [])\n",
    "            )\n",
    "\n",
    "            # Check if the search is for real-time news\n",
    "            is_realtime = query_info.get(\"time_sensitivity\") == \"urgent\"\n",
    "            from_date = datetime.now() - timedelta(hours=1 if is_realtime else 24)\n",
    "\n",
    "            # Configure parameters for the 'everything' endpoint\n",
    "            params = {\n",
    "                \"q\": \" \".join(search_terms),  # Exclude time-related keywords\n",
    "                \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"sortBy\": \"publishedAt\",\n",
    "                \"language\": \"en\",\n",
    "                \"apiKey\": self.api_key,\n",
    "            }\n",
    "\n",
    "            # Construct API request URL\n",
    "            url = f\"{self.base_url}/everything?{urlencode(params)}\"\n",
    "\n",
    "            # Send API request\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            # Check response status\n",
    "            if response.status_code != 200:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": data.get(\"message\", \"Unknown error\"),\n",
    "                    \"query_info\": query_info,\n",
    "                }\n",
    "\n",
    "            # Process and format results\n",
    "            articles = data.get(\"articles\", [])\n",
    "            formatted_results = []\n",
    "\n",
    "            for article in articles[:max_results]:\n",
    "                formatted_results.append(\n",
    "                    {\n",
    "                        \"title\": article.get(\"title\"),\n",
    "                        \"description\": article.get(\"description\"),\n",
    "                        \"url\": article.get(\"url\"),\n",
    "                        \"published_at\": article.get(\"publishedAt\"),\n",
    "                        \"source\": article.get(\"source\", {}).get(\"name\"),\n",
    "                        \"content\": article.get(\"content\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": formatted_results,\n",
    "                \"total_results\": data.get(\"totalResults\", 0),\n",
    "                \"returned_count\": len(formatted_results),\n",
    "                \"search_parameters\": {\n",
    "                    \"keywords\": query_info[\"keywords\"],\n",
    "                    \"from_date\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"language\": \"en\",\n",
    "                },\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def get_top_headlines(\n",
    "        self, country: str = \"us\", category: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Fetches top news headlines.\n",
    "\n",
    "        Args:\n",
    "            country (str): Country code (default: 'us' for the United States).\n",
    "            category (str, optional): News category (e.g., business, technology).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing top headlines.\n",
    "        \"\"\"\n",
    "        params = {\"country\": country, \"apiKey\": self.api_key}\n",
    "\n",
    "        if category:\n",
    "            params[\"category\"] = category\n",
    "\n",
    "        url = f\"{self.base_url}/top-headlines?{urlencode(params)}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GeneralSearchAgent`\n",
    "\n",
    "This agent manages broader web searches through SerpAPI. It handles diverse information needs that don't fit strictly into academic or news categories. The agent includes features like language-specific searches, result ranking, and content type filtering. It's particularly useful for general research, product information, or any broad information gathering needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "class GeneralSearchAgent:\n",
    "    def __init__(self, serpapi_key: str = None):\n",
    "        if serpapi_key:\n",
    "            os.environ[\"SERPAPI_API_KEY\"] = serpapi_key\n",
    "        self.search = SerpAPIWrapper()\n",
    "\n",
    "    def setup_search_parameters(self, query_info: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Constructs search queries for general search.\"\"\"\n",
    "        keywords = \" \".join(query_info[\"keywords\"])\n",
    "\n",
    "        # Set up base search queries\n",
    "        search_queries = [\n",
    "            f\"{keywords} lang:ko\",\n",
    "            keywords,\n",
    "        ]  # Korean results  # General search\n",
    "\n",
    "        return search_queries\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs general search and returns results.\"\"\"\n",
    "        try:\n",
    "            search_queries = self.setup_search_parameters(query_info)\n",
    "            all_results = []\n",
    "\n",
    "            for query in search_queries:\n",
    "                raw_results = self.search.run(query)\n",
    "                parsed_results = self._parse_general_results(raw_results)\n",
    "                all_results.extend(parsed_results)\n",
    "\n",
    "            # Sort by relevance score\n",
    "            sorted_results = sorted(\n",
    "                all_results, key=lambda x: x.get(\"relevance_score\", 0), reverse=True\n",
    "            )[:max_results]\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": sorted_results,\n",
    "                \"total_found\": len(all_results),\n",
    "                \"returned_count\": len(sorted_results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _parse_general_results(self, raw_results: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parses general search results.\"\"\"\n",
    "        parsed_results = []\n",
    "\n",
    "        for result in raw_results.split(\"\\n\"):\n",
    "            if not result.strip():\n",
    "                continue\n",
    "\n",
    "            parsed_results.append(\n",
    "                {\n",
    "                    \"type\": \"general\",\n",
    "                    \"title\": self._extract_title(result),\n",
    "                    \"content\": result,\n",
    "                    \"url\": self._extract_url(result),\n",
    "                    \"relevance_score\": self._calculate_relevance(result),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return parsed_results\n",
    "\n",
    "    def _extract_title(self, result: str) -> str:\n",
    "        \"\"\"Extracts title from the result.\"\"\"\n",
    "        return result.split(\".\")[0].strip()[:100]\n",
    "\n",
    "    def _extract_url(self, result: str) -> str:\n",
    "        \"\"\"Extracts URL from the result.\"\"\"\n",
    "        import re\n",
    "\n",
    "        urls = re.findall(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[^\\s]*\", result)\n",
    "        return urls[0] if urls else \"\"\n",
    "\n",
    "    def _calculate_relevance(self, result: str) -> float:\n",
    "        \"\"\"Calculates relevance score for the search result.\"\"\"\n",
    "        relevance_score = 0.5  # Base score\n",
    "\n",
    "        # Calculate score based on keyword matching\n",
    "        keywords = [\"official\", \"guide\", \"tutorial\", \"review\", \"recommendation\"]\n",
    "        lower_result = result.lower()\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if keyword in lower_result:\n",
    "                relevance_score += 0.1\n",
    "\n",
    "        return min(1.0, relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SearchRouter`: The System's Traffic Controller\n",
    "\n",
    "The `SearchRouter` acts as the central coordinator for our multi-agent search system, intelligently directing queries to specialized search agents based on the type of information needed. Think of it as an expert traffic controller at a busy airport, making sure each \"flight\" (query) goes to the right \"runway\" (search agent).\n",
    "\n",
    "The `SearchRouter`'s modular design allows for easy expansion - new specialized search agents can be added without modifying the existing code, making the system highly adaptable to evolving search needs.\n",
    "\n",
    "Through this central coordination, the `SearchRouter` ensures efficient and reliable information retrieval across different types of searches while maintaining a consistent interface for the rest of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Get API key directly or from environment variables\n",
    "        # Initialize agents for each search type\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "\n",
    "    def route_and_search(self, query_analysis: dict) -> dict:\n",
    "        \"\"\"Routes the search request to appropriate search agent based on query analysis\n",
    "\n",
    "        Args:\n",
    "            query_analysis (dict): Query analysis results from QueryAnalysisAgent\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing search results, including success/failure status and related info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check search type\n",
    "            search_type = query_analysis.get(\"search_type\")\n",
    "\n",
    "            # Record start time for logging\n",
    "            start_time = datetime.now(pytz.utc)\n",
    "\n",
    "            # Perform search\n",
    "            if search_type == \"research_paper\":\n",
    "                print(\"Performing research paper search...\")\n",
    "                result = self.paper_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"news\":\n",
    "                print(\"Performing news search...\")\n",
    "                result = self.news_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"general\":\n",
    "                print(\"Performing general search...\")\n",
    "                result = self.general_search_agent.perform_search(query_analysis)\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Unsupported search type: {search_type}\",\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "\n",
    "            # Calculate search duration\n",
    "            end_time = datetime.now(pytz.utc)\n",
    "            search_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Add metadata to results\n",
    "            result.update(\n",
    "                {\n",
    "                    \"search_type\": search_type,\n",
    "                    \"search_duration\": search_duration,\n",
    "                    \"search_timestamp\": end_time.isoformat(),\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                \"search_type\": query_analysis.get(\"search_type\"),\n",
    "            }\n",
    "\n",
    "    # def get_agent_status(self) -> dict:\n",
    "    #     \"\"\"Check the status of each search agent.\n",
    "\n",
    "    #     Returns:\n",
    "    #         dict: Dictionary containing status information for each agent\n",
    "    #     \"\"\"\n",
    "    #     return {\n",
    "    #         \"paper_search_agent\": \"ready\",\n",
    "    #         \"news_search_agent\": \"ready\",\n",
    "    #         \"general_search_agent\": \"ready\",\n",
    "    #         \"last_checked\": datetime.now(pytz.utc).isoformat(),\n",
    "    #     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Agent\n",
    "Crafting User-Friendly Information Delivery\n",
    "\n",
    "The ResponseAgent serves as our system's expert communicator, transforming raw search results into well-structured, readable content that meets users' needs. This agent is particularly crucial as it represents the final step in our information delivery pipeline, ensuring that complex search results are presented in a clear, digestible format.\n",
    "\n",
    "The agent maintains three specialized prompt templates for different types of content:\n",
    "\n",
    "Key Features of the Response Agent:\n",
    "\n",
    "1. Content Customization\n",
    "   - Adapts formatting based on content type (papers, news, general)\n",
    "   - Maintains consistent structure while accommodating different information types\n",
    "   - Ensures appropriate context and explanations are included\n",
    "\n",
    "2. Email Optimization\n",
    "   - Creates clear, professional email subjects\n",
    "   - Structures content for easy scanning and reading\n",
    "   - Includes all necessary context and source information\n",
    "\n",
    "The ResponseAgent represents the crucial final step in our information delivery pipeline, ensuring that users receive not just raw data, but well-organized, contextually relevant information that directly addresses their queries. Through its careful formatting and organization, it helps users quickly understand and act upon the information they've requested.\n",
    "\n",
    "This agent demonstrates how automated systems can maintain a human touch in their communications, making complex information accessible and actionable for end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        self.setup_prompts()\n",
    "\n",
    "    def setup_prompts(self):\n",
    "        # Research paper search prompt\n",
    "        self.paper_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following research paper search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Research Paper Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized research paper results for your search.\"\n",
    "               - Number each paper and format as follows:\n",
    "                 1. Paper title: [title]\n",
    "                    - Summary: [core research content and key findings]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # News search prompt\n",
    "        self.news_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following news search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Latest News Updates for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the latest news articles related to your search topic.\"\n",
    "               - Number each news item and format as follows:\n",
    "                 1. [title] - [news source]\n",
    "                    - Main content: [key content summary]\n",
    "                    - Published date: [date]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # General search prompt\n",
    "        self.general_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized results for your search.\"\n",
    "               - Number each result and format as follows:\n",
    "                 1. [title]\n",
    "                    - Content: [main content summary]\n",
    "                    - Source: [website or platform name]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def format_results(self, search_results):\n",
    "        try:\n",
    "            # Handle cases with no results or errors\n",
    "            if search_results.get(\"status\") == \"error\":\n",
    "                return {\n",
    "                    \"subject\": \"Search Error Notification\",\n",
    "                    \"body\": f\"An error occurred during search: {search_results.get('error_message', 'Unknown error')}\",\n",
    "                }\n",
    "\n",
    "            # Handle cases with no results\n",
    "            if not search_results.get(\"results\"):\n",
    "                return {\n",
    "                    \"subject\": \"No Search Results\",\n",
    "                    \"body\": f\"No results found for search term '{search_results.get('original_query', '')}'.\",\n",
    "                }\n",
    "\n",
    "            # Select prompt based on search type\n",
    "            search_type = search_results.get(\"search_type\", \"general\")\n",
    "            if search_type == \"research_paper\":\n",
    "                prompt = self.paper_prompt\n",
    "            elif search_type == \"news\":\n",
    "                prompt = self.news_prompt\n",
    "            else:\n",
    "                prompt = self.general_prompt\n",
    "\n",
    "            # Prepare input for result formatting\n",
    "            formatted_input = {\n",
    "                \"query\": search_results.get(\"original_query\", \"\"),\n",
    "                \"results\": json.dumps(\n",
    "                    search_results.get(\"results\", []), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Generate response through LLM\n",
    "            response = prompt.format(**formatted_input)\n",
    "            response = self.llm.invoke(response)\n",
    "\n",
    "            try:\n",
    "                # Attempt JSON parsing\n",
    "                return json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                # Return default format if JSON parsing fails\n",
    "                return {\n",
    "                    \"subject\": f\"Search Results for [{formatted_input['query']}]\",\n",
    "                    \"body\": response.content,\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"subject\": \"Result Processing Error\",\n",
    "                \"body\": f\"An error occurred while processing results: {str(e)}\\n\\nOriginal query: {search_results.get('original_query', '')}\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_system():\n",
    "    query_analyzer = QueryAnalysisAgent()\n",
    "    search_router = SearchRouter()\n",
    "    response_agent = ResponseAgent()  # Added\n",
    "\n",
    "    test_queries = [\n",
    "        \"recommend place to eat at seoul in 7 pm\",\n",
    "        \"find rag persona paper at 3 pm\",\n",
    "        \"find news us president speech in 7 am\",\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Test Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            query_analysis = query_analyzer.analyze_query(query)\n",
    "            print(\"\\n1. Query Analysis Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    query_analysis,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if query_analysis[\"status\"] != \"success\":\n",
    "                print(\"Query analysis failed!\")\n",
    "                continue\n",
    "\n",
    "            search_results = search_router.route_and_search(query_analysis)\n",
    "            print(\"\\n2. Search Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    search_results,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Added: Result formatting\n",
    "            print(\"\\n3. Formatted Results:\")\n",
    "            formatted_results = response_agent.format_results(search_results)\n",
    "            print(json.dumps(formatted_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during test: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_search_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling System and Email Service\n",
    "\n",
    "The `ScheduledSearchSystem` manages the complete lifecycle of search tasks, from scheduling to result delivery. Here's its core structure and functionality:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. Collection Management\n",
    "2. Task Scheduling\n",
    "3. Search Execution\n",
    "4. Email Delivery\n",
    "\n",
    "\n",
    "The system uses threading for non-blocking operation and includes comprehensive logging for monitoring task progress and debugging issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "import yagmail\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "class ScheduledSearchSystem:\n",
    "    def __init__(self, email_config: Dict[str, Any]):\n",
    "        print(f\"\\n[{self._get_current_time()}] Initializing system...\")\n",
    "        self.query_analyzer = QueryAnalysisAgent()\n",
    "        self.search_router = SearchRouter()\n",
    "        self.response_agent = ResponseAgent()\n",
    "        self.client = chromadb.PersistentClient(path=\"./search_data\")\n",
    "\n",
    "        # Email configuration\n",
    "        self.email_config = email_config\n",
    "        self.yag = yagmail.SMTP(email_config[\"username\"], email_config[\"password\"])\n",
    "        print(f\"[{self._get_current_time()}] Email client configuration complete\")\n",
    "\n",
    "        self.scheduled_tasks = {}\n",
    "        self.setup_collections()\n",
    "\n",
    "        # Add completion flag\n",
    "        self.is_completed = False\n",
    "        self.completion_event = threading.Event()\n",
    "\n",
    "        # Start scheduler\n",
    "        self.scheduler_thread = threading.Thread(target=self._run_scheduler)\n",
    "        self.scheduler_thread.daemon = True\n",
    "        self.scheduler_thread.start()\n",
    "        print(f\"[{self._get_current_time()}] System initialization complete\\n\")\n",
    "\n",
    "    def _get_current_time(self):\n",
    "        \"\"\"Return current time as string\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def setup_collections(self):\n",
    "        \"\"\"Set up ChromaDB collections\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Starting ChromaDB collection setup...\")\n",
    "        self.results_collection = self.client.get_or_create_collection(\n",
    "            name=\"search_results\", metadata={\"description\": \"Raw search results\"}\n",
    "        )\n",
    "\n",
    "        self.formatted_collection = self.client.get_or_create_collection(\n",
    "            name=\"formatted_responses\",\n",
    "            metadata={\"description\": \"Formatted responses for email delivery\"},\n",
    "        )\n",
    "\n",
    "        self.schedule_collection = self.client.get_or_create_collection(\n",
    "            name=\"scheduled_tasks\",\n",
    "            metadata={\"description\": \"Scheduled search and email tasks\"},\n",
    "        )\n",
    "        print(f\"[{self._get_current_time()}] ChromaDB collection setup complete\")\n",
    "\n",
    "    def schedule_task(self, query: str, user_email: str) -> Dict[str, Any]:\n",
    "        \"\"\"Schedule a task\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n[{self._get_current_time()}] Starting new task scheduling...\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Email: {user_email}\")\n",
    "\n",
    "            # Query analysis\n",
    "            query_analysis = self.query_analyzer.analyze_query(query)\n",
    "            execution_time = query_analysis[\"execution_time\"]\n",
    "            target_time = query_analysis[\"target_time\"]\n",
    "\n",
    "            print(f\"Scheduled search execution time: {execution_time}\")\n",
    "            print(f\"Scheduled email delivery time: {target_time}\")\n",
    "\n",
    "            # Generate task ID\n",
    "            schedule_id = f\"task_{datetime.now(pytz.UTC).timestamp()}\"\n",
    "            print(f\"Generated task ID: {schedule_id}\")\n",
    "\n",
    "            # Save task information\n",
    "            task_info = {\n",
    "                \"query\": query,\n",
    "                \"email\": user_email,\n",
    "                \"execution_time\": execution_time.isoformat(),\n",
    "                \"target_time\": target_time.isoformat(),\n",
    "                \"search_type\": query_analysis[\"search_type\"],\n",
    "                \"status\": \"scheduled\",\n",
    "            }\n",
    "\n",
    "            self.scheduled_tasks[schedule_id] = task_info\n",
    "\n",
    "            # Save to ChromaDB\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Saving task information to ChromaDB...\"\n",
    "            )\n",
    "            self.schedule_collection.add(\n",
    "                documents=[json.dumps(task_info)],\n",
    "                metadatas=[{\"type\": \"schedule\", \"status\": \"pending\"}],\n",
    "                ids=[schedule_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Task information saved\")\n",
    "\n",
    "            # Schedule search execution\n",
    "            execution_time_str = execution_time.strftime(\"%H:%M\")\n",
    "            schedule.every().day.at(execution_time_str).do(\n",
    "                self.execute_search, schedule_id=schedule_id\n",
    "            ).tag(schedule_id)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Search task scheduling complete\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Task successfully scheduled\",\n",
    "                \"schedule_id\": schedule_id,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"target_time\": target_time,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Task scheduling failed: {str(e)}\")\n",
    "            return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "    def execute_search(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Execute search\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting search execution (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            task_info = self.scheduled_tasks.get(schedule_id)\n",
    "            if not task_info:\n",
    "                print(f\"[{self._get_current_time()}] Task information not found\")\n",
    "                return False\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Analyzing search query...\")\n",
    "            query_analysis = self.query_analyzer.analyze_query(task_info[\"query\"])\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Performing search...\")\n",
    "            search_results = self.search_router.route_and_search(query_analysis)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Formatting search results...\")\n",
    "            formatted_response = self.response_agent.format_results(search_results)\n",
    "\n",
    "            # Save results\n",
    "            print(f\"[{self._get_current_time()}] Saving search results to ChromaDB...\")\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            self.formatted_collection.add(\n",
    "                documents=[json.dumps(formatted_response)],\n",
    "                metadatas=[\n",
    "                    {\n",
    "                        \"schedule_id\": schedule_id,\n",
    "                        \"email\": task_info[\"email\"],\n",
    "                        \"target_time\": task_info[\"target_time\"],\n",
    "                    }\n",
    "                ],\n",
    "                ids=[response_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Search results saved\")\n",
    "\n",
    "            # Schedule email delivery\n",
    "            target_time = datetime.fromisoformat(task_info[\"target_time\"])\n",
    "            target_time_str = target_time.strftime(\"%H:%M\")\n",
    "\n",
    "            schedule.every().day.at(target_time_str).do(\n",
    "                self.send_email, schedule_id=schedule_id\n",
    "            ).tag(f\"email_{schedule_id}\")\n",
    "\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Email delivery scheduled (Time: {target_time_str})\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Search execution failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def send_email(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Send email\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting email delivery (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            print(f\"[{self._get_current_time()}] Retrieving saved search results...\")\n",
    "            response_results = self.formatted_collection.get(ids=[response_id])\n",
    "\n",
    "            if not response_results[\"documents\"]:\n",
    "                print(f\"[{self._get_current_time()}] Search results not found\")\n",
    "                return False\n",
    "\n",
    "            formatted_response = json.loads(response_results[\"documents\"][0])\n",
    "            metadata = response_results[\"metadatas\"][0]\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Sending email...\")\n",
    "            print(f\"Recipient: {metadata['email']}\")\n",
    "            print(f\"Subject: {formatted_response['subject']}\")\n",
    "\n",
    "            self.yag.send(\n",
    "                to=metadata[\"email\"],\n",
    "                subject=formatted_response[\"subject\"],\n",
    "                contents=formatted_response[\"body\"],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Email sent successfully\")\n",
    "\n",
    "            # Update task status\n",
    "            print(f\"[{self._get_current_time()}] Updating task status...\")\n",
    "            task_info = self.scheduled_tasks[schedule_id]\n",
    "            task_info[\"status\"] = \"completed\"\n",
    "            self.schedule_collection.update(\n",
    "                documents=[json.dumps(task_info)], ids=[schedule_id]\n",
    "            )\n",
    "\n",
    "            # Clear schedule\n",
    "            schedule.clear(f\"email_{schedule_id}\")\n",
    "            print(f\"[{self._get_current_time()}] Task completion processing complete\\n\")\n",
    "\n",
    "            # Set completion flag\n",
    "            self.is_completed = True\n",
    "            self.completion_event.set()\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] All tasks completed. Shutting down system.\\n\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Email delivery failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _run_scheduler(self):\n",
    "        \"\"\"Run scheduler\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Scheduler started...\")\n",
    "        while not self.is_completed:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)  # Check every second\n",
    "\n",
    "    def wait_for_completion(self, timeout=None):\n",
    "        \"\"\"Wait for task completion\"\"\"\n",
    "        try:\n",
    "            completed = self.completion_event.wait(timeout=timeout)\n",
    "            if not completed:\n",
    "                print(f\"[{self._get_current_time()}] Task completion timeout\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n[{self._get_current_time()}] Terminated by user.\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Scheduler System Usage Guide\n",
    "The work starts 5 minutes before the work request time.\n",
    "\n",
    "1. Email Configuration\n",
    "- Enable Gmail 2-Step Verification\n",
    "- Generate App Password: https://myaccount.google.com/security > 2-Step Verification > App passwords\n",
    "```python\n",
    "email_config = {\n",
    "    \"username\": \"your_email@gmail.com\",\n",
    "    \"password\": \"your_app_password\",  # Gmail app password\n",
    "    \"smtp_server\": \"smtp.gmail.com\",\n",
    "    \"smtp_port\": 587\n",
    "}\n",
    "```\n",
    "\n",
    "2. Initialize System and Schedule Task\n",
    "```python\n",
    "# Initialize system\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# Schedule task\n",
    "result = system.schedule_task(\n",
    "    query=\"find AI papers at 9 AM\",  # Search query to execute\n",
    "    user_email=\"your_email@gmail.com\"  # Email to receive results\n",
    ")\n",
    "```\n",
    "\n",
    "3. Wait for Completion\n",
    "```python\n",
    "# Wait for task completion (max 4 hours)\n",
    "system.wait_for_completion(timeout=14400)\n",
    "```\n",
    "\n",
    "Search results will be automatically emailed to the specified address upon completion.\n",
    "\n",
    "The system supports various query types:\n",
    "- Research papers: \"find RAG papers at 7 AM\"\n",
    "- News: \"find AI news at 9 AM\"\n",
    "- General search: \"find restaurants in Seoul at 6 PM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Scheduling System Test ===\n",
      "\n",
      "\n",
      "[2025-02-08 11:24:54] Initializing system...\n",
      "[2025-02-08 11:24:54] Email client configuration complete\n",
      "[2025-02-08 11:24:54] Starting ChromaDB collection setup...\n",
      "[2025-02-08 11:24:54] ChromaDB collection setup complete\n",
      "[2025-02-08 11:24:54] Scheduler started...\n",
      "[2025-02-08 11:24:54] System initialization complete\n",
      "\n",
      "\n",
      "[2025-02-08 11:24:54] Starting new task scheduling...\n",
      "Query: find Modular RAG paper at 11:30 AM\n",
      "Email: jik9210@gmail.com\n",
      "Scheduled search execution time: 2025-02-08 11:25:00+00:00\n",
      "Scheduled email delivery time: 2025-02-08 11:30:00+00:00\n",
      "Generated task ID: task_1738981498.110638\n",
      "[2025-02-08 11:24:58] Saving task information to ChromaDB...\n",
      "[2025-02-08 11:24:59] Task information saved\n",
      "[2025-02-08 11:24:59] Search task scheduling complete\n",
      "\n",
      "=== Scheduling Result ===\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"message\": \"Task successfully scheduled\",\n",
      "  \"schedule_id\": \"task_1738981498.110638\",\n",
      "  \"execution_time\": \"2025-02-08 11:25:00+00:00\",\n",
      "  \"target_time\": \"2025-02-08 11:30:00+00:00\"\n",
      "}\n",
      "\n",
      "=== Task will execute at scheduled time... ===\n",
      "\n",
      "\n",
      "[2025-02-08 11:25:00] Starting search execution (ID: task_1738981498.110638)\n",
      "[2025-02-08 11:25:00] Analyzing search query...\n",
      "[2025-02-08 11:25:03] Performing search...\n",
      "Performing research paper search...\n",
      "[2025-02-08 11:25:04] Formatting search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: response_task_1738168330.927504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-08 11:25:10] Saving search results to ChromaDB...\n",
      "[2025-02-08 11:25:10] Search results saved\n",
      "[2025-02-08 11:25:10] Email delivery scheduled (Time: 11:30)\n",
      "\n",
      "[2025-02-08 11:30:00] Starting email delivery (ID: task_1738981498.110638)\n",
      "[2025-02-08 11:30:00] Retrieving saved search results...\n",
      "[2025-02-08 11:30:00] Sending email...\n",
      "Recipient: jik9210@gmail.com\n",
      "Subject: Search Results for [find Modular RAG paper at 11:30 AM]\n",
      "[2025-02-08 11:30:04] Email sent successfully\n",
      "[2025-02-08 11:30:04] Updating task status...\n",
      "[2025-02-08 11:30:04] Task completion processing complete\n",
      "\n",
      "[2025-02-08 11:30:04] All tasks completed. Shutting down system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# System configuration\n",
    "email_config = {\n",
    "    \"username\": \"jik9210@gmail.com\",\n",
    "    \"password\": \"yqfu wwpw otrl fgji\",\n",
    "}\n",
    "\n",
    "print(\"\\n=== Starting Scheduling System Test ===\\n\")\n",
    "\n",
    "# Initialize system\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# Schedule task\n",
    "result = system.schedule_task(\n",
    "    query=\"find Modular RAG paper at 11:30 AM\",  # Example: 7:45 PM\n",
    "    user_email=\"jik9210@gmail.com\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== Scheduling Result ===\")\n",
    "print(json.dumps(result, indent=2, default=str))\n",
    "print(\"\\n=== Task will execute at scheduled time... ===\\n\")\n",
    "\n",
    "# Wait for task completion (maximum 4 hours)\n",
    "system.wait_for_completion(timeout=14400)  # 4 hours = 14400 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
