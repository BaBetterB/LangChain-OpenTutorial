{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraphStudio - MultiAgent\n",
    "\n",
    "- Author: [Taylor(Jihyun Kim)](https://github.com/Taylor0819)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/07-Agent/20-Multi-AgentWorkflowUsingLangGraphStudio.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/07-Agent/20-Multi-AgentWorkflowUsingLangGraphStudio.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this tutorial, you will learn how to build a persona-based chatbot that simulates a real individual’s style of communication. First, you’ll gather factual data and insights using people-researcher along with actual `X` (formerly `Twitter`) posts, synthesizing both factual details.\n",
    " \n",
    "Then, you’ll embed the data using `LangChain` to enable context-aware retrieval and define a persona prompt so your chatbot chat just like the real person. Finally, you’ll integrate everything with reply_gAI to provide a conversational interface that responds to user queries as if the subject of your study were directly answering.\n",
    "\n",
    "`LangGraph Studio` offers a new way to develop LLM applications by providing a specialized agent IDE that enables visualization, interaction, and debugging of complex agentic applications.\n",
    "\n",
    "With visual graphs and the ability to edit state, you can better understand agent workflows and iterate faster. LangGraph Studio integrates with LangSmith allowing you to collaborate with teammates to debug failure modes.\n",
    "\n",
    "![Langgraph Studio](./assets/20-multi-agentsorkflowusinglanggraphstudio-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [What is LangGraph Studio](#what-is-langgraph-studio)\n",
    "- [Building a Persona Chatbot](#building-a-persona-chatbot)\n",
    "- [How to connect a local agent to LangGraph Studio](#how-to-connect-a-local-agent-to-langgraph-studio)\n",
    "\n",
    "\n",
    "### References\n",
    "- [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]** \n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\"langsmith\", \"langchain_anthropic\", \"langgraph\", \"tavily-python\"],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"ANTHROPIC_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"20-Multi-AgentWorkflowUsingLangGraphStudio\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=4,\n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-latest\", temperature=0, rate_limiter=rate_limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get tavil api key\n",
    "from tavily import AsyncTavilyClient\n",
    "# Search\n",
    "tavily_async_client = AsyncTavilyClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"A class representing a person to research.\"\"\"\n",
    "\n",
    "    name: Optional[str] = None\n",
    "    \"\"\"The name of the person.\"\"\"\n",
    "    company: Optional[str] = None\n",
    "    \"\"\"The current company of the person.\"\"\"\n",
    "    linkedin: Optional[str] = None\n",
    "    \"\"\"The Linkedin URL of the person.\"\"\"\n",
    "    email: str\n",
    "    \"\"\"The email of the person.\"\"\"\n",
    "    role: Optional[str] = None\n",
    "    \"\"\"The current title of the person.\"\"\"\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class InputState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    person: Person\n",
    "    \"Person to research.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: Optional[dict[str, Any]] = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class OverallState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    person: Person\n",
    "    \"Person to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: str = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "    search_queries: list[str] = field(default=None)\n",
    "    \"List of generated search queries to find relevant information\"\n",
    "\n",
    "    # Add default values for required fields\n",
    "    completed_notes: Annotated[list, operator.add] = field(default_factory=list)\n",
    "    \"Notes from completed research related to the schema\"\n",
    "\n",
    "    info: dict[str, Any] = field(default=None)\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "    is_satisfactory: bool = field(default=None)\n",
    "    \"True if all required fields are well populated, False otherwise\"\n",
    "\n",
    "    reflection_steps_taken: int = field(default=0)\n",
    "    \"Number of times the reflection node has been executed\"\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class OutputState:\n",
    "    \"\"\"The response object for the end user.\n",
    "\n",
    "    This class defines the structure of the output that will be provided\n",
    "    to the user after the graph's execution is complete.\n",
    "    \"\"\"\n",
    "\n",
    "    info: dict[str, Any]\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: list[str] = Field(\n",
    "        description=\"List of search queries.\",\n",
    "    )\n",
    "\n",
    "class ReflectionOutput(BaseModel):\n",
    "    is_satisfactory: bool = Field(\n",
    "        description=\"True if all required fields are well populated, False otherwise\"\n",
    "    )\n",
    "    missing_fields: list[str] = Field(\n",
    "        description=\"List of field names that are missing or incomplete\"\n",
    "    )\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"If is_satisfactory is False, provide 1-3 targeted search queries to find the missing information\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Brief explanation of the assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "\n",
    "    max_search_queries: int = 3  # Max search queries per person\n",
    "    max_search_results: int = 3  # Max search results per query\n",
    "    max_reflection_steps: int = 0  # Max reflection steps\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate queries\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "def generate_queries(state: OverallState, config: RunnableConfig) -> dict[str, Any]:\n",
    "    \"\"\"Generate search queries based on the user input and extraction schema.\"\"\"\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    max_search_queries = configurable.max_search_queries\n",
    "\n",
    "    # Generate search queries\n",
    "    structured_llm = llm.with_structured_output(Queries)\n",
    "\n",
    "    # Format system instructions\n",
    "    person_str = f\"Email: {state.person['email']}\"\n",
    "    if \"name\" in state.person:\n",
    "        person_str += f\" Name: {state.person['name']}\"\n",
    "    if \"linkedin\" in state.person:\n",
    "        person_str += f\" LinkedIn URL: {state.person['linkedin']}\"\n",
    "    if \"role\" in state.person:\n",
    "        person_str += f\" Role: {state.person['role']}\"\n",
    "    if \"company\" in state.person:\n",
    "        person_str += f\" Company: {state.person['company']}\"\n",
    "\n",
    "    query_instructions = QUERY_WRITER_PROMPT.format(\n",
    "        person=person_str,\n",
    "        info=json.dumps(state.extraction_schema, indent=2),\n",
    "        user_notes=state.user_notes,\n",
    "        max_search_queries=max_search_queries,\n",
    "    )\n",
    "\n",
    "    # Generate queries\n",
    "    results = cast(\n",
    "        Queries,\n",
    "        structured_llm.invoke(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": query_instructions},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Please generate a list of search queries related to the schema that you want to populate.\",\n",
    "                },\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Queries\n",
    "    query_list = [query for query in results.queries]\n",
    "    return {\"search_queries\": query_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def research_person(state: OverallState, config: RunnableConfig) -> dict[str, Any]:\n",
    "    \"\"\"Execute a multi-step web search and information extraction process.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Executes concurrent web searches using the Tavily API\n",
    "    2. Deduplicates and formats the search results\n",
    "    \"\"\"\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    max_search_results = configurable.max_search_results\n",
    "\n",
    "    # Web search\n",
    "    search_tasks = []\n",
    "    for query in state.search_queries:\n",
    "        search_tasks.append(\n",
    "            tavily_async_client.search(\n",
    "                query,\n",
    "                days=360,\n",
    "                max_results=max_search_results,\n",
    "                include_raw_content=True,\n",
    "                topic=\"general\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Execute all searches concurrently\n",
    "    search_docs = await asyncio.gather(*search_tasks)\n",
    "\n",
    "    # Deduplicate and format sources\n",
    "    source_str = deduplicate_and_format_sources(\n",
    "        search_docs, max_tokens_per_source=1000, include_raw_content=True\n",
    "    )\n",
    "\n",
    "    # Generate structured notes relevant to the extraction schema\n",
    "    p = INFO_PROMPT.format(\n",
    "        info=json.dumps(state.extraction_schema, indent=2),\n",
    "        content=source_str,\n",
    "        people=state.person,\n",
    "        user_notes=state.user_notes,\n",
    "    )\n",
    "    result = await claude_3_5_sonnet.ainvoke(p)\n",
    "    return {\"completed_notes\": [str(result.content)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_notes_extract_schema(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"Gather notes from the web search and extract the schema fields.\"\"\"\n",
    "\n",
    "    # Format all notes\n",
    "    notes = format_all_notes(state.completed_notes)\n",
    "\n",
    "    # Extract schema fields\n",
    "    system_prompt = EXTRACTION_PROMPT.format(\n",
    "        info=json.dumps(state.extraction_schema, indent=2), notes=notes\n",
    "    )\n",
    "    structured_llm = claude_3_5_sonnet.with_structured_output(state.extraction_schema)\n",
    "    result = structured_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Produce a structured output from these notes.\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return {\"info\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"Reflect on the extracted information and generate search queries to find missing information.\"\"\"\n",
    "    structured_llm = claude_3_5_sonnet.with_structured_output(ReflectionOutput)\n",
    "\n",
    "    # Format reflection prompt\n",
    "    system_prompt = REFLECTION_PROMPT.format(\n",
    "        schema=json.dumps(state.extraction_schema, indent=2),\n",
    "        info=state.info,\n",
    "    )\n",
    "\n",
    "    # Invoke\n",
    "    result = cast(\n",
    "        ReflectionOutput,\n",
    "        structured_llm.invoke(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Produce a structured reflection output.\"},\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if result.is_satisfactory:\n",
    "        return {\"is_satisfactory\": result.is_satisfactory}\n",
    "    else:\n",
    "        return {\n",
    "            \"is_satisfactory\": result.is_satisfactory,\n",
    "            \"search_queries\": result.search_queries,\n",
    "            \"reflection_steps_taken\": state.reflection_steps_taken + 1,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import END\n",
    "\n",
    "def route_from_reflection(\n",
    "    state: OverallState, config: RunnableConfig\n",
    ") -> Literal[END, \"research_person\"]:  # type: ignore\n",
    "    \"\"\"Route the graph based on the reflection output.\"\"\"\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # If we have satisfactory results, end the process\n",
    "    if state.is_satisfactory:\n",
    "        return END\n",
    "\n",
    "    # If results aren't satisfactory but we haven't hit max steps, continue research\n",
    "    if state.reflection_steps_taken <= configurable.max_reflection_steps:\n",
    "        return \"research_person\"\n",
    "\n",
    "    # If we've exceeded max steps, end even if not satisfactory\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Add nodes and edges\n",
    "builder = StateGraph(\n",
    "    OverallState,\n",
    "    input=InputState,\n",
    "    output=OutputState,\n",
    "    config_schema=Configuration,\n",
    ")\n",
    "builder.add_node(\"gather_notes_extract_schema\", gather_notes_extract_schema)\n",
    "builder.add_node(\"generate_queries\", generate_queries)\n",
    "builder.add_node(\"research_person\", research_person)\n",
    "builder.add_node(\"reflection\", reflection)\n",
    "\n",
    "builder.add_edge(START, \"generate_queries\")\n",
    "builder.add_edge(\"generate_queries\", \"research_person\")\n",
    "builder.add_edge(\"research_person\", \"gather_notes_extract_schema\")\n",
    "builder.add_edge(\"gather_notes_extract_schema\", \"reflection\")\n",
    "builder.add_conditional_edges(\"reflection\", route_from_reflection)\n",
    "\n",
    "# Compile\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangGraph Studio\n",
    "![Using LangGraph Studio](./assets/20-multi-agentsorkflowusinglanggraphstudio-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to connect a local agent to LangGraph Studio\n",
    "\n",
    "**Connection Options**\n",
    "There are two ways to connect your local agent to LangGraph Studio:\n",
    "\n",
    "- [Development Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#development-server-with-web-ui): Python package, all platforms, `no Docker`\n",
    "- [LangGraph Desktop](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#desktop-app): `Application`, `Mac only`, `requires Docker`\n",
    "\n",
    "In this guide we will cover how to use the development server as that is generally an easier and better experience.\n",
    "\n",
    "\n",
    "[LangGraph Studio Desktop (Beta)](https://github.com/langchain-ai/langgraph-studio)\n",
    "\n",
    "The desktop application only supports macOS. Other users can [run a local LangGraph server and use the web studio](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/#langgraph-studio-web-ui). We also depend on Docker Engine to be running, currently we only support the following runtimes:\n",
    "\n",
    "[LangGraph Studio Download for MacOS](https://studio.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your application\n",
    "\n",
    "First, you will need to setup your application in the proper format. This means defining a langgraph.json file which contains paths to your agent(s). See [this guide](https://langchain-ai.github.io/langgraph/concepts/application_structure/) for information on how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. langgraph 연결\n",
    "2. ipynb -> py 파일 변환 코드 추가\n",
    "3. 데모 추가 \n",
    "4. agent 설명추가~\n",
    "5. usage examples 추가 (advanced agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-jPP3Iqy--py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
