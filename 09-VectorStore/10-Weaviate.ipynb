{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weaviate\n",
    "\n",
    "- Author: [Haseom Shin](https://github.com/IHAGI-c)\n",
    "- Design: []()\n",
    "- Peer Review: []()\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/11-Fallbacks.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/11-Fallbacks.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers how to get started with the Weaviate vector store in LangChain, using the `langchain-weaviate` package.\n",
    "\n",
    "> [Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects.\n",
    "\n",
    "To use this integration, you need to have a running Weaviate database instance.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Credentials](#credentials)\n",
    "  - [Setting up Weaviate Cloud Services](#setting-up-weaviate-cloud-services)\n",
    "- [What is Weaviate?](#what-is-weaviate)\n",
    "- [Why Use Weaviate?](#why-use-weaviate)\n",
    "- [Initialization](#initialization)\n",
    "  - [List Indexs](#list-indexs)\n",
    "  - [Create Index](#create-index)\n",
    "  - [Delete Index](#delete-index)\n",
    "  - [Select Embeddings model](#select-embeddings-model)\n",
    "  - [Data Preprocessing](#data-preprocessing)\n",
    "- [Manage vector store](#manage-vector-store)\n",
    "  - [Add items to vector store](#add-items-to-vector-store)\n",
    "  - [Delete items from vector store](#delete-items-from-vector-store)\n",
    "- [Finding Objects by Similarity](#finding-objects-by-similarity)\n",
    "  - [Step 1: Preparing Your Data](#step-1-preparing-your-data)\n",
    "  - [Step 2: Perform the search](#step-2-perform-the-search)\n",
    "  - [Quantify Result Similarity](#quantify-result-similarity)\n",
    "- [Search mechanism](#search-mechanism)\n",
    "- [Persistence](#persistence)\n",
    "- [Multi-tenancy](#multi-tenancy)\n",
    "- [Retriever options](#retriever-options)\n",
    "- [Use with LangChain](#use-with-langchain)\n",
    "  - [Question Answering with Sources](#question-answering-with-sources)\n",
    "  - [Retrieval-Augmented Generation](#retrieval-augmented-generation)\n",
    "\n",
    "\n",
    "### References\n",
    "- [Langchain-Weaviate](https://python.langchain.com/docs/integrations/providers/weaviate/)\n",
    "- [Weaviate Documentation](https://weaviate.io/developers/weaviate)\n",
    "- [Weaviate Introduction](https://weaviate.io/developers/weaviate/introduction)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"openai\",\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"tiktoken\",\n",
    "        \"langchain-weaviate\",\n",
    "        \"langchain-openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"WEAVIATE_API_KEY\": \"\",\n",
    "        \"WEAVIATE_URL\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Weaviate\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "There are three main ways to connect to Weaviate:\n",
    "\n",
    "1. **Local Connection**: Connect to a Weaviate instance running locally through Docker\n",
    "2. **Weaviate Cloud Services (WCS)**: Use Weaviate's managed cloud service\n",
    "3. **Custom Deployment**: Deploy Weaviate on Kubernetes or other custom configurations\n",
    "\n",
    "For this notebook, we'll use Weaviate Cloud Services (WCS) as it provides the easiest way to get started without any local setup.\n",
    "\n",
    "### Setting up Weaviate Cloud Services\n",
    "\n",
    "1. First, sign up for a free account at [Weaviate Cloud Console](https://console.weaviate.cloud)\n",
    "2. Create a new cluster\n",
    "3. Get your API key\n",
    "4. Set API key\n",
    "5. Connect to your WCS cluster\n",
    "\n",
    "#### 1. Weaviate Signup\n",
    "![Weaviate Cloud Console](./assets/09-Weaviate-Credentials-01.png)\n",
    "\n",
    "#### 2. Create Cluster\n",
    "![Weaviate Cloud Console](./assets/09-Weaviate-Credentials-02.png)\n",
    "![Weaviate Cloud Console](./assets/09-Weaviate-Credentials-03.png)\n",
    "\n",
    "#### 3. Get API Key\n",
    "**If you using gRPC, please copy the gRPC URL**\n",
    "\n",
    "![Weaviate Cloud Console](./assets/09-Weaviate-Credentials-04-1.png)\n",
    "\n",
    "#### 4. Set API Key\n",
    "```\n",
    "WEAVIATE_API_KEY=\"YOUR_WEAVIATE_API_KEY\"\n",
    "WEAVIATE_URL=\"YOUR_WEAVIATE_CLUSTER_URL\"\n",
    "```\n",
    "\n",
    "#### 5. Connect to your WCS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "if not os.getenv(\"WEAVIATE_API_KEY\"):\n",
    "    os.environ[\"WEAVIATE_API_KEY\"] = getpass.getpass(\"Enter your Weaviate API key: \")\n",
    "\n",
    "if not os.getenv(\"WEAVIATE_URL\"):\n",
    "    os.environ[\"WEAVIATE_URL\"] = getpass.getpass(\"Enter your Weaviate URL: \")\n",
    "\n",
    "weaviate_url = os.environ.get(\"WEAVIATE_URL\")\n",
    "weaviate_api_key = os.environ.get(\"WEAVIATE_API_KEY\")\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## api key Lookup\n",
    "def get_api_key():\n",
    "    return weaviate_api_key\n",
    "\n",
    "print(get_api_key())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Weaviate?\n",
    "\n",
    "Weaviate is a powerful open-source vector database that revolutionizes how we store and search data. It combines traditional database capabilities with advanced machine learning features, allowing you to:\n",
    "\n",
    "- Weaviate is an open source [vector database](https://weaviate.io/blog/what-is-a-vector-database).\n",
    "- Weaviate allows you to store and retrieve data objects based on their semantic properties by indexing them with [vectors](./concepts/vector-index.md).\n",
    "- Weaviate can be used stand-alone (aka _bring your vectors_) or with a variety of [modules](./modules/index.md) that can do the vectorization for you and extend the core capabilities.\n",
    "- Weaviate has a [GraphQL-API](./api/graphql/index.md) to access your data easily.\n",
    "- Weaviate is fast (check our [open source benchmarks](./benchmarks/index.md)).\n",
    "\n",
    "> 💡 **Key Feature**: Weaviate achieves millisecond-level query performance, making it suitable for production environments.\n",
    "\n",
    "## Why Use Weaviate?\n",
    "\n",
    "Weaviate stands out for several reasons:\n",
    "\n",
    "1. **Versatility**: Supports multiple media types (text, images, etc.)\n",
    "2. **Advanced Features**:\n",
    "   - Semantic Search\n",
    "   - Question-Answer Extraction\n",
    "   - Classification\n",
    "   - Custom ML Model Integration\n",
    "3. **Production-Ready**: Built in Go for high performance and scalability\n",
    "4. **Developer-Friendly**: Multiple access methods through GraphQL, REST, and various client libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Before initializing our vector store, let's connect to a Weaviate collection. If one named index_name doesn't exist, it will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Collection\n",
    "\n",
    "Creates a new collection in Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'BookChunk' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, VectorDistances\n",
    "from typing import List\n",
    "\n",
    "def create_collection(\n",
    "    client: weaviate.Client, \n",
    "    collection_name: str, \n",
    "    description: str, \n",
    "    properties: List[Property], \n",
    "    vectorizer: Configure.Vectorizer,\n",
    "    metric: str = \"cosine\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a new index (collection) in Weaviate with the specified properties.\n",
    "\n",
    "    :param client: Weaviate client instance\n",
    "    :param collection_name: Name of the index (collection) (e.g., \"BookChunk\")\n",
    "    :param description: Description of the index (e.g., \"A collection for storing book chunks\")\n",
    "    :param properties: List of properties, where each property is a dictionary with keys:\n",
    "        - name (str): Name of the property\n",
    "        - dataType (list[str]): Data types for the property (e.g., [\"text\"], [\"int\"])\n",
    "        - description (str): Description of the property\n",
    "    :param vectorizer: Vectorizer configuration created using Configure.Vectorizer \n",
    "                       (e.g., Configure.Vectorizer.text2vec_openai())\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    distance_metric = getattr(VectorDistances, metric.upper(), None)\n",
    "\n",
    "    # Set vector_index_config to hnsw\n",
    "    vector_index_config = Configure.VectorIndex.hnsw(\n",
    "        distance_metric=distance_metric\n",
    "    )\n",
    "    \n",
    "    # Create the collection in Weaviate\n",
    "    try:\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            description=description,\n",
    "            properties=properties,\n",
    "            vectorizer_config=vectorizer,\n",
    "            vector_index_config=vector_index_config\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create collection '{collection_name}': {e}\")\n",
    "\n",
    "collection_name = \"BookChunk\"  # change if desired\n",
    "description = \"A chunk of a book's content\"\n",
    "vectorizer = Configure.Vectorizer.text2vec_openai()\n",
    "metric = \"dot\"\n",
    "properties = [\n",
    "    Property(\n",
    "        name=\"text\",\n",
    "        data_type=DataType.TEXT,\n",
    "        description=\"The content of the text\"\n",
    "    ),\n",
    "    Property(\n",
    "        name=\"order\",\n",
    "        data_type=DataType.INT,\n",
    "        description=\"The order of the chunk in the book\"\n",
    "    ),\n",
    "    Property(\n",
    "        name=\"title\",\n",
    "        data_type=DataType.TEXT,\n",
    "        description=\"The title of the book\"\n",
    "    )\n",
    "]\n",
    "\n",
    "create_collection(client, collection_name, description, properties, vectorizer, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Collection\n",
    "\n",
    "Deletes a collection in Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted index: BookChunk\n"
     ]
    }
   ],
   "source": [
    "def delete_collection(client, collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "    print(f\"Deleted index: {collection_name}\")\n",
    "\n",
    "def delete_all_collections():\n",
    "    client.collections.delete_all()\n",
    "    print(\"Deleted all collections\")\n",
    "\n",
    "# delete_all_collections()    # if you want to delete all collections, uncomment this line\n",
    "delete_collection(client, collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Collections\n",
    "\n",
    "Lists all collections in Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections (indexes) in the Weaviate schema:\n",
      "- Collection name: BookChunk\n",
      "  Description: A chunk of a book's content\n",
      "  Properties:\n",
      "    - Name: text, Type: DataType.TEXT\n",
      "    - Name: order, Type: DataType.INT\n",
      "    - Name: title, Type: DataType.TEXT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def list_collections():\n",
    "    \"\"\"\n",
    "    Lists all collections (indexes) in the Weaviate database, including their properties.\n",
    "    \"\"\"\n",
    "    # Retrieve all collection configurations\n",
    "    collections = client.collections.list_all()\n",
    "\n",
    "    # Check if there are any collections\n",
    "    if collections:\n",
    "        print(\"Collections (indexes) in the Weaviate schema:\")\n",
    "        for name, config in collections.items():\n",
    "            print(f\"- Collection name: {name}\")\n",
    "            print(f\"  Description: {config.description if config.description else 'No description available'}\")\n",
    "            print(f\"  Properties:\")\n",
    "            for prop in config.properties:\n",
    "                print(f\"    - Name: {prop.name}, Type: {prop.data_type}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No collections found in the schema.\")\n",
    "\n",
    "list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weaviate.Collection config={\n",
      "  \"name\": \"BookChunk\",\n",
      "  \"description\": \"A chunk of a book's content\",\n",
      "  \"generative_config\": null,\n",
      "  \"inverted_index_config\": {\n",
      "    \"bm25\": {\n",
      "      \"b\": 0.75,\n",
      "      \"k1\": 1.2\n",
      "    },\n",
      "    \"cleanup_interval_seconds\": 60,\n",
      "    \"index_null_state\": false,\n",
      "    \"index_property_length\": false,\n",
      "    \"index_timestamps\": false,\n",
      "    \"stopwords\": {\n",
      "      \"preset\": \"en\",\n",
      "      \"additions\": null,\n",
      "      \"removals\": null\n",
      "    }\n",
      "  },\n",
      "  \"multi_tenancy_config\": {\n",
      "    \"enabled\": false,\n",
      "    \"auto_tenant_creation\": false,\n",
      "    \"auto_tenant_activation\": false\n",
      "  },\n",
      "  \"properties\": [\n",
      "    {\n",
      "      \"name\": \"text\",\n",
      "      \"description\": \"The content of the text\",\n",
      "      \"data_type\": \"text\",\n",
      "      \"index_filterable\": true,\n",
      "      \"index_range_filters\": false,\n",
      "      \"index_searchable\": true,\n",
      "      \"nested_properties\": null,\n",
      "      \"tokenization\": \"word\",\n",
      "      \"vectorizer_config\": {\n",
      "        \"skip\": false,\n",
      "        \"vectorize_property_name\": true\n",
      "      },\n",
      "      \"vectorizer\": \"text2vec-openai\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"order\",\n",
      "      \"description\": \"The order of the chunk in the book\",\n",
      "      \"data_type\": \"int\",\n",
      "      \"index_filterable\": true,\n",
      "      \"index_range_filters\": false,\n",
      "      \"index_searchable\": false,\n",
      "      \"nested_properties\": null,\n",
      "      \"tokenization\": null,\n",
      "      \"vectorizer_config\": {\n",
      "        \"skip\": false,\n",
      "        \"vectorize_property_name\": true\n",
      "      },\n",
      "      \"vectorizer\": \"text2vec-openai\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"title\",\n",
      "      \"description\": \"The title of the book\",\n",
      "      \"data_type\": \"text\",\n",
      "      \"index_filterable\": true,\n",
      "      \"index_range_filters\": false,\n",
      "      \"index_searchable\": true,\n",
      "      \"nested_properties\": null,\n",
      "      \"tokenization\": \"word\",\n",
      "      \"vectorizer_config\": {\n",
      "        \"skip\": false,\n",
      "        \"vectorize_property_name\": true\n",
      "      },\n",
      "      \"vectorizer\": \"text2vec-openai\"\n",
      "    }\n",
      "  ],\n",
      "  \"references\": [],\n",
      "  \"replication_config\": {\n",
      "    \"factor\": 1,\n",
      "    \"async_enabled\": false,\n",
      "    \"deletion_strategy\": \"NoAutomatedResolution\"\n",
      "  },\n",
      "  \"reranker_config\": null,\n",
      "  \"sharding_config\": {\n",
      "    \"virtual_per_physical\": 128,\n",
      "    \"desired_count\": 1,\n",
      "    \"actual_count\": 1,\n",
      "    \"desired_virtual_count\": 128,\n",
      "    \"actual_virtual_count\": 128,\n",
      "    \"key\": \"_id\",\n",
      "    \"strategy\": \"hash\",\n",
      "    \"function\": \"murmur3\"\n",
      "  },\n",
      "  \"vector_index_config\": {\n",
      "    \"quantizer\": null,\n",
      "    \"cleanup_interval_seconds\": 300,\n",
      "    \"distance_metric\": \"dot\",\n",
      "    \"dynamic_ef_min\": 100,\n",
      "    \"dynamic_ef_max\": 500,\n",
      "    \"dynamic_ef_factor\": 8,\n",
      "    \"ef\": -1,\n",
      "    \"ef_construction\": 128,\n",
      "    \"filter_strategy\": \"sweeping\",\n",
      "    \"flat_search_cutoff\": 40000,\n",
      "    \"max_connections\": 32,\n",
      "    \"skip\": false,\n",
      "    \"vector_cache_max_objects\": 1000000000000\n",
      "  },\n",
      "  \"vector_index_type\": \"hnsw\",\n",
      "  \"vectorizer_config\": {\n",
      "    \"vectorizer\": \"text2vec-openai\",\n",
      "    \"model\": {\n",
      "      \"baseURL\": \"https://api.openai.com\",\n",
      "      \"model\": \"text-embedding-3-small\"\n",
      "    },\n",
      "    \"vectorize_collection_name\": true\n",
      "  },\n",
      "  \"vectorizer\": \"text2vec-openai\",\n",
      "  \"vector_config\": null\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "def lookup_collection(collection_name: str):\n",
    "    return client.collections.get(collection_name)\n",
    "\n",
    "print(lookup_collection(collection_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x117422a50>\n"
     ]
    }
   ],
   "source": [
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "\n",
    "vector_store = WeaviateVectorStore(client=client, index_name=collection_name, text_key=\"text\", embedding=embeddings)\n",
    "\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Below is the preprocessing process for general documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./data/the_little_prince.txt\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='The Little Prince\\nWritten By Antoine de Saiot-Exupery (1900〜1944)'), Document(metadata={}, page_content='[ Antoine de Saiot-Exupery ]'), Document(metadata={}, page_content='Over the past century, the thrill of flying has inspired some to perform remarkable feats of daring. For others, their desire to soar into the skies led to dramatic leaps in technology. For Antoine'), Document(metadata={}, page_content='in technology. For Antoine de Saint-Exupéry, his love of aviation inspired stories, which have touched the hearts of millions around the world.'), Document(metadata={}, page_content='Born in 1900 in Lyons, France, young Antoine was filled with a passion for adventure. When he failed an entrance exam for the Naval Academy, his interest in aviation took hold. He joined the French'), Document(metadata={}, page_content='hold. He joined the French Army Air Force in 1921 where he first learned to fly a plane. Five years later, he would leave the military in order to begin flying air mail between remote settlements in'), Document(metadata={}, page_content='between remote settlements in the Sahara desert.'), Document(metadata={}, page_content=\"For Saint-Exupéry, it was a grand adventure - one with dangers lurking at every corner. Flying his open cockpit biplane, Saint-Exupéry had to fight the desert's swirling sandstorms. Worse, still, he\"), Document(metadata={}, page_content=\"sandstorms. Worse, still, he ran the risk of being shot at by unfriendly tribesmen below. Saint-Exupéry couldn't have been more thrilled. Soaring across the Sahara inspired him to spend his nights\"), Document(metadata={}, page_content='him to spend his nights writing about his love affair with flying.'), Document(metadata={}, page_content='When World War II broke out, Saint-Exupéry rejoined the French Air Force. After Nazi troops overtook France in 1940, Saint-Exupéry fled to the United States. He had hoped to join the U. S. war effort'), Document(metadata={}, page_content='to join the U. S. war effort as a fighter pilot, but was dismissed because of his age. To console himself, he drew upon his experiences over the Saharan desert to write and illustrate what would'), Document(metadata={}, page_content='and illustrate what would become his most famous book, The Little Prince (1943). Mystical and enchanting, this small book has fascinated both children and adults for decades. In the book, a pilot is'), Document(metadata={}, page_content='In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little prince'), Document(metadata={}, page_content='the book, the little prince discovers the true meaning of life. At the end of his conversation with the Little Prince, the aviator manages to fix his plane and both he and the little prince continue'), Document(metadata={}, page_content='the little prince continue on their journeys'), Document(metadata={}, page_content='Shortly after completing the book, Saint-Exupéry finally got his wish. He returned to North Africa to fly a warplane for his country. On July 31, 1944, Saint-Exupéry took off on a mission. Sadly, he'), Document(metadata={}, page_content='off on a mission. Sadly, he was never heard from again.'), Document(metadata={}, page_content='[ TO LEON WERTH ]'), Document(metadata={}, page_content='I ask the indulgence of the children who may read this book for dedicating it to a grown-up. I have a serious reason: he is the best friend I have in the world. I have another reason: this grown-up')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.create_documents([raw_text])\n",
    "\n",
    "print(split_docs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The Little Prince\\nWritten By Antoine de Saiot-Exupery (1900〜1944)',\n",
       "  'order': 1,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': '[ Antoine de Saiot-Exupery ]',\n",
       "  'order': 2,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'Over the past century, the thrill of flying has inspired some to perform remarkable feats of daring. For others, their desire to soar into the skies led to dramatic leaps in technology. For Antoine',\n",
       "  'order': 3,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'in technology. For Antoine de Saint-Exupéry, his love of aviation inspired stories, which have touched the hearts of millions around the world.',\n",
       "  'order': 4,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'Born in 1900 in Lyons, France, young Antoine was filled with a passion for adventure. When he failed an entrance exam for the Naval Academy, his interest in aviation took hold. He joined the French',\n",
       "  'order': 5,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'hold. He joined the French Army Air Force in 1921 where he first learned to fly a plane. Five years later, he would leave the military in order to begin flying air mail between remote settlements in',\n",
       "  'order': 6,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'between remote settlements in the Sahara desert.',\n",
       "  'order': 7,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': \"For Saint-Exupéry, it was a grand adventure - one with dangers lurking at every corner. Flying his open cockpit biplane, Saint-Exupéry had to fight the desert's swirling sandstorms. Worse, still, he\",\n",
       "  'order': 8,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': \"sandstorms. Worse, still, he ran the risk of being shot at by unfriendly tribesmen below. Saint-Exupéry couldn't have been more thrilled. Soaring across the Sahara inspired him to spend his nights\",\n",
       "  'order': 9,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'},\n",
       " {'text': 'him to spend his nights writing about his love affair with flying.',\n",
       "  'order': 10,\n",
       "  'title': 'The Little Prince',\n",
       "  'source': 'Original Text'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def preprocess_documents(\n",
    "    split_docs: List[Document],\n",
    "    metadata: Dict[str, str] = None\n",
    ") -> List[Dict[str, Dict[str, object]]]:\n",
    "    \"\"\"\n",
    "    Processes a list of pre-split documents into a format suitable for storing in Weaviate.\n",
    "\n",
    "    :param split_docs: List of LangChain Document objects (each containing page_content and metadata).\n",
    "    :param metadata: Additional metadata to include in each chunk (e.g., title, source).\n",
    "    :return: A list of dictionaries, each representing a chunk in the format:\n",
    "             {'properties': {'text': ..., 'order': ..., ...metadata}}\n",
    "    \"\"\"\n",
    "    processed_chunks = []\n",
    "\n",
    "    # Iterate over Document objects\n",
    "    for idx, doc in enumerate(split_docs, start=1):\n",
    "        # Extract text from page_content and include metadata\n",
    "        chunk_data = {\n",
    "            \"text\": doc.page_content,\n",
    "            \"order\": idx\n",
    "        }\n",
    "        # Combine with metadata from Document and additional metadata if provided\n",
    "        if metadata:\n",
    "            chunk_data.update(metadata)\n",
    "        if doc.metadata:\n",
    "            chunk_data.update(doc.metadata)\n",
    "\n",
    "        # Format for Weaviate\n",
    "        processed_chunks.append(chunk_data)\n",
    "\n",
    "    return processed_chunks\n",
    "\n",
    "metadata = {\"title\": \"The Little Prince\", \"source\": \"Original Text\"}\n",
    "\n",
    "processed_chunks = preprocess_documents(split_docs, metadata=metadata)\n",
    "\n",
    "processed_chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage vector store\n",
    "Once you have created your vector store, we can interact with it by adding and deleting different items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add items to vector store\n",
    "\n",
    "Weaviate supports dynamic batch processing, which allows you to add documents in parallel. This is useful when you have a large number of documents to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/7\n",
      "Processed batch 2/7\n",
      "Processed batch 3/7\n",
      "Processed batch 4/7\n",
      "Processed batch 5/7\n",
      "Processed batch 6/7\n",
      "Processed batch 7/7\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "def upsert_documents(\n",
    "    vector_store: WeaviateVectorStore,\n",
    "    docs: List[Dict],\n",
    "    unique_key: str = \"order\",\n",
    "    batch_size: int = 100,\n",
    "    show_progress: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    WeaviateVectorStore에 문서를 upsert합니다.\n",
    "    \"\"\"\n",
    "    # Document 객체와 ID 준비\n",
    "    documents = []\n",
    "    ids = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        unique_value = str(doc[unique_key])\n",
    "        doc_id = generate_uuid5(vector_store._index_name, unique_value)\n",
    "        \n",
    "        documents.append(Document(\n",
    "            page_content=doc[\"text\"],\n",
    "            metadata={k: v for k, v in doc.items() if k != \"text\"}\n",
    "        ))\n",
    "        ids.append(doc_id)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "    embeddings = vector_store.embeddings.embed_documents(texts)\n",
    "    \n",
    "    # 배치 처리\n",
    "    collection = vector_store._client.collections.get(vector_store._index_name)\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = embeddings[i:i + batch_size]\n",
    "            batch_ids = ids[i:i + batch_size]\n",
    "            batch_metadatas = metadatas[i:i + batch_size] if metadatas else None\n",
    "            \n",
    "            for j, text in enumerate(batch_texts):\n",
    "                properties = {\"text\": text}  # text_key 대신 직접 \"text\" 사용\n",
    "                if batch_metadatas:\n",
    "                    properties.update(batch_metadatas[j])\n",
    "                \n",
    "                # collection의 data 메서드를 사용하여 객체 추가/업데이트\n",
    "                collection.data.insert(\n",
    "                    uuid=batch_ids[j],\n",
    "                    properties=properties,\n",
    "                    vector=batch_embeddings[j]\n",
    "                )\n",
    "            \n",
    "            if show_progress:\n",
    "                print(f\"Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch processing: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return ids\n",
    "\n",
    "# 사용 예시\n",
    "results = upsert_documents(\n",
    "    vector_store=vector_store,\n",
    "    docs=processed_chunks,\n",
    "    unique_key=\"order\",\n",
    "    batch_size=100,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "배치 처리 중:  57%|█████▋    | 4/7 [00:00<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id '5959984f-1e54-5fea-91ce-cef8cc9894bd' already exists\"}]}.\n",
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id 'a8ff68c1-db62-51f6-a03b-5e12aceda12f' already exists\"}]}.\n",
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id '7b9c08f9-6ff4-59af-8565-d1dda0053472' already exists\"}]}.\n",
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id '3872de1c-e293-54d4-9a63-727fa8d156db' already exists\"}]}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "배치 처리 중: 100%|██████████| 7/7 [00:00<00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id 'b1f9bc44-6ff5-52d4-85cb-5dcfc93ef1ce' already exists\"}]}.\n",
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id '093cd537-de38-5d4f-b9f1-09083b02083f' already exists\"}]}.\n",
      "배치 처리 중 오류 발생: Object was not added! Unexpected status code: 422, with response body: {'error': [{'message': \"id '7e8695be-8a19-5ef9-9cc1-b86e1c022290' already exists\"}]}.\n",
      "\n",
      "처리 완료\n",
      "성공적으로 처리된 문서 수: 0\n",
      "총 소요 시간: 4.61초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from langchain_core.documents import Document\n",
    "from weaviate.util import generate_uuid5\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def upsert_documents_parallel(\n",
    "    vector_store: WeaviateVectorStore,\n",
    "    docs: List[Dict],\n",
    "    unique_key: str = \"order\",\n",
    "    batch_size: int = 100,\n",
    "    max_workers: Optional[int] = 4,\n",
    "    show_progress: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    WeaviateVectorStore에 문서를 병렬로 upsert합니다.\n",
    "    \n",
    "    Args:\n",
    "        vector_store: WeaviateVectorStore 인스턴스\n",
    "        docs: 업서트할 문서 리스트\n",
    "        unique_key: 고유 식별자로 사용할 키\n",
    "        batch_size: 배치 크기\n",
    "        max_workers: 최대 작업자 수\n",
    "        show_progress: 진행 상황 표시 여부\n",
    "    Returns:\n",
    "        List[str]: 성공적으로 처리된 문서들의 ID 리스트\n",
    "    \"\"\"\n",
    "    # Document 객체와 ID 준비\n",
    "    documents = []\n",
    "    ids = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        unique_value = str(doc[unique_key])\n",
    "        doc_id = generate_uuid5(vector_store._index_name, unique_value)\n",
    "        \n",
    "        documents.append(Document(\n",
    "            page_content=doc[\"text\"],\n",
    "            metadata={k: v for k, v in doc.items() if k != \"text\"}\n",
    "        ))\n",
    "        ids.append(doc_id)\n",
    "    \n",
    "    # 임베딩 생성\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "    embeddings = vector_store.embeddings.embed_documents(texts)\n",
    "    \n",
    "    # 배치로 데이터 분할\n",
    "    def create_batches(data, size):\n",
    "        return [data[i:i + size] for i in range(0, len(data), size)]\n",
    "    \n",
    "    batched_texts = create_batches(texts, batch_size)\n",
    "    batched_embeddings = create_batches(embeddings, batch_size)\n",
    "    batched_ids = create_batches(ids, batch_size)\n",
    "    batched_metadatas = create_batches(metadatas, batch_size)\n",
    "    \n",
    "    # 컬렉션 가져오기\n",
    "    collection = vector_store._client.collections.get(vector_store._index_name)\n",
    "    \n",
    "    def process_batch(batch_data):\n",
    "        \"\"\"배치 단위로 upsert를 처리하는 함수\"\"\"\n",
    "        batch_texts, batch_embeddings, batch_ids, batch_metadatas = batch_data\n",
    "        successful_ids = []\n",
    "        \n",
    "        try:\n",
    "            for j, text in enumerate(batch_texts):\n",
    "                properties = {\"text\": text}\n",
    "                if batch_metadatas:\n",
    "                    properties.update(batch_metadatas[j])\n",
    "                \n",
    "                collection.data.insert(\n",
    "                    uuid=batch_ids[j],\n",
    "                    properties=properties,\n",
    "                    vector=batch_embeddings[j]\n",
    "                )\n",
    "                successful_ids.append(batch_ids[j])\n",
    "            \n",
    "            return successful_ids\n",
    "        except Exception as e:\n",
    "            print(f\"배치 처리 중 오류 발생: {e}\")\n",
    "            return successful_ids\n",
    "    \n",
    "    # 병렬 처리를 위한 작업 목록 생성\n",
    "    batch_data = list(zip(\n",
    "        batched_texts,\n",
    "        batched_embeddings,\n",
    "        batched_ids,\n",
    "        batched_metadatas\n",
    "    ))\n",
    "    \n",
    "    successful_ids = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_batch, batch): i \n",
    "            for i, batch in enumerate(batch_data)\n",
    "        }\n",
    "        \n",
    "        if show_progress:\n",
    "            with tqdm(total=len(batch_data), desc=\"배치 처리 중\") as pbar:\n",
    "                for future in as_completed(futures):\n",
    "                    batch_result = future.result()\n",
    "                    successful_ids.extend(batch_result)\n",
    "                    pbar.update(1)\n",
    "        else:\n",
    "            for future in as_completed(futures):\n",
    "                batch_result = future.result()\n",
    "                successful_ids.extend(batch_result)\n",
    "    \n",
    "    return successful_ids\n",
    "\n",
    "# 사용 예시\n",
    "start_time = time.time()\n",
    "\n",
    "results = upsert_documents_parallel(\n",
    "    vector_store=vector_store,\n",
    "    docs=processed_chunks,\n",
    "    unique_key=\"order\",\n",
    "    batch_size=100,  # 배치 크기 설정\n",
    "    max_workers=4,   # 동시 작업자 수 설정\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n처리 완료\")\n",
    "print(f\"성공적으로 처리된 문서 수: {len(results)}\")\n",
    "print(f\"총 소요 시간: {end_time - start_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n",
      "객체 처리 중 오류 발생: Object was not added! Unexpected status code: 500, with response body: {'error': [{'message': 'update vector: API Key: no api key found neither in request header: X-Openai-Api-Key nor in environment variable under OPENAI_APIKEY'}]}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m unique_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m processed_chunks:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mupsert_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 28\u001b[0m, in \u001b[0;36mupsert_object\u001b[0;34m(client, collection_name, data_object, unique_key)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m객체 업데이트: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_uuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# 객체가 존재하지 않으면 삽입\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_uuid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m객체 삽입: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_uuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-BXw0bE1H-py3.11/lib/python3.11/site-packages/weaviate/syncify.py:23\u001b[0m, in \u001b[0;36mconvert.<locals>.sync_method\u001b[0;34m(self, __new_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msync_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, __new_name\u001b[38;5;241m=\u001b[39mnew_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     async_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, __new_name)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_EventLoopSingleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-opentutorial-BXw0bE1H-py3.11/lib/python3.11/site-packages/weaviate/event_loop.py:42\u001b[0m, in \u001b[0;36m_EventLoop.run_until_complete\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateClosedClientError()\n\u001b[1;32m     41\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "def upsert_object(client: weaviate.WeaviateClient, collection_name: str, data_object: dict, unique_key: str):\n",
    "    \"\"\"\n",
    "    Weaviate에서 객체를 upsert(업데이트 또는 삽입)합니다.\n",
    "\n",
    "    :param client: Weaviate 클라이언트 인스턴스\n",
    "    :param collection_name: 대상 컬렉션 이름\n",
    "    :param data_object: 저장할 데이터 객체 (dict)\n",
    "    :param unique_key: 고유 식별자 키 (예: 'order')\n",
    "    \"\"\"\n",
    "    # 고유 키 값을 문자열로 변환하여 UUID 생성\n",
    "    unique_value = str(data_object[unique_key])\n",
    "    object_uuid = generate_uuid5(collection_name, unique_value)\n",
    "\n",
    "    # 컬렉션 객체 가져오기\n",
    "    collection = client.collections.get(collection_name)\n",
    "\n",
    "    # 객체 존재 여부 확인\n",
    "    try:\n",
    "        existing_object = collection.data.exists(object_uuid)\n",
    "        if existing_object:\n",
    "            # 객체가 존재하면 업데이트\n",
    "            collection.data.update(uuid=object_uuid, properties=data_object)\n",
    "            print(f\"객체 업데이트: {object_uuid}\")\n",
    "        else:\n",
    "            # 객체가 존재하지 않으면 삽입\n",
    "            collection.data.insert(uuid=object_uuid, properties=data_object)\n",
    "            print(f\"객체 삽입: {object_uuid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"객체 처리 중 오류 발생: {e}\")\n",
    "\n",
    "unique_key = \"order\"\n",
    "for chunk in processed_chunks:\n",
    "    upsert_object(client, collection_name, chunk, unique_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def upsert_documents(docs: List[Document]):\n",
    "  return vector_store.add_documents(docs)\n",
    "\n",
    "upsert_documents(docs=processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upsert_documents_parallel(\n",
    "    docs: List[Document],\n",
    "    batch_size: int = 100,\n",
    "    max_workers: Optional[int] = None,\n",
    "    show_progress: bool = True\n",
    ") -> List[str]:\n",
    "    batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"Total documents: {len(docs)}, Number of batches: {len(batches)}\")\n",
    "    \n",
    "    def process_batch(batch: List[Document]) -> Optional[List[str]]:\n",
    "        try:\n",
    "            return vector_store.add_documents(batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during batch processing: {e}\")\n",
    "            return None\n",
    "    \n",
    "    all_ids = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_batch, batch): i \n",
    "            for i, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        if show_progress:\n",
    "            for future in tqdm(as_completed(future_to_batch), total=len(batches), desc=\"Uploading documents\"):\n",
    "                batch_result = future.result()\n",
    "                if batch_result:\n",
    "                    all_ids.extend(batch_result)\n",
    "        else:\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_result = future.result()\n",
    "                if batch_result:\n",
    "                    all_ids.extend(batch_result)\n",
    "    \n",
    "    return all_ids\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = upsert_documents_parallel(\n",
    "    docs=processed_docs,\n",
    "    batch_size=100,\n",
    "    max_workers=4,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Upsert completed\")\n",
    "print(f\"Processed documents: {len(results)}\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.collections.classes.filters import Filter\n",
    "\n",
    "filter_query = Filter.by_property(\"chapter\").equal(\"Chapter 21\")\n",
    "\n",
    "vector_store.similarity_search(\n",
    "    query=\"Who is the narrator of the story?\",\n",
    "    k=3,\n",
    "    filters=filter_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete items from vector store\n",
    "\n",
    "You can delete items from vector store by filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.collections.classes.filters import Filter\n",
    "from typing import Optional\n",
    "\n",
    "def delete_by_filter(filter_query: Filter) -> int:\n",
    "    try:\n",
    "        # Retrieve the collection\n",
    "        collection = client.collections.get(index_name)\n",
    "        \n",
    "        # Check the number of documents that match the filter before deletion\n",
    "        query_result = collection.query.fetch_objects(\n",
    "            filters=filter_query,\n",
    "        )\n",
    "        initial_count = len(query_result.objects)\n",
    "        \n",
    "        # Delete documents that match the filter condition\n",
    "        result = collection.data.delete_many(\n",
    "            where=filter_query\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of documents deleted: {initial_count}\")\n",
    "        return initial_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during deletion: {e}\")\n",
    "        raise\n",
    "    \n",
    "delete_by_filter(filter_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Objects by Similarity\n",
    "\n",
    "Weaviate allows you to find objects that are semantically similar to your query. Let's walk through a complete example, from importing data to executing similarity searches.\n",
    "\n",
    "### Step 1: Preparing Your Data\n",
    "\n",
    "Before we can perform similarity searches, we need to populate our Weaviate instance with data. We'll start by loading and chunking a text file into manageable pieces.\n",
    "\n",
    "> 💡 **Tip**: Breaking down large texts into smaller chunks helps optimize vector search performance and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "\n",
    "# Create a document with metadata, including geo-information\n",
    "raw_texts = [\n",
    "    \"The Eiffel Tower in Paris stands 324 meters tall and was completed in 1889.\",\n",
    "    \"The Great Wall of China is over 21,000 kilometers long and was built over several centuries.\",\n",
    "    \"The Taj Mahal in India was built by Emperor Shah Jahan as a tomb for his beloved wife.\",\n",
    "    \"Machu Picchu in Peru was built by the Inca Empire in the 15th century at an altitude of 2,430 meters.\",\n",
    "    \"The Pyramids of Giza in Egypt were built over 4,500 years ago as tombs for pharaohs.\",\n",
    "    \"The Colosseum in Rome could hold up to 50,000 spectators for gladiatorial contests.\",\n",
    "    \"Petra in Jordan was carved into rose-colored rock faces and served as a trading center.\",\n",
    "    \"Angkor Wat in Cambodia is the world's largest religious monument, built in the 12th century.\"\n",
    "]\n",
    "\n",
    "# Regional information for each text\n",
    "regions = [\n",
    "    \"Europe\",    # Eiffel Tower\n",
    "    \"Asia\",      # Great Wall\n",
    "    \"Asia\",      # Taj Mahal\n",
    "    \"South America\",  # Machu Picchu\n",
    "    \"Africa\",    # Pyramids\n",
    "    \"Europe\",    # Colosseum\n",
    "    \"Asia\",      # Petra\n",
    "    \"Asia\"       # Angkor Wat\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=text, metadata={\"region\": region}) \n",
    "    for text, region in zip(raw_texts, regions)\n",
    "]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = WeaviateVectorStore.from_documents(docs, embeddings, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Perform the search\n",
    "\n",
    "We can now perform a similarity search. This will return the most similar documents to the query text, based on the embeddings stored in Weaviate and an equivalent embedding generated from the query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Petra?\"\n",
    "docs = db.similarity_search(query, k=1)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add filters, which will either include or exclude results based on the filter conditions. (See [more filter examples](https://weaviate.io/developers/weaviate/search/filters).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "for region in regions:\n",
    "    search_filter = Filter.by_property(\"region\").equal(region)\n",
    "    filtered_results = db.similarity_search(query, filters=search_filter, k=4)\n",
    "    \n",
    "    print(f\"\\n=== Monuments in {region} ===\")\n",
    "    print(f\"Found {len(filtered_results)} results:\")\n",
    "    for i, doc in enumerate(filtered_results, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Region: {doc.metadata['region']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide `k`, which is the upper limit of the number of results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the k parameter to limit the number of results\n",
    "search_filter = Filter.by_property(\"region\").equal(regions[0])  # Europe\n",
    "filtered_search_results = db.similarity_search(query, filters=search_filter, k=3)\n",
    "\n",
    "print(\"\\n=== Limiting Results with k parameter ===\")\n",
    "print(f\"\\nSearching for monuments in {regions[0]} with k=3:\")\n",
    "print(f\"Number of results: {len(filtered_search_results)}\")\n",
    "\n",
    "for i, doc in enumerate(filtered_search_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "# Check if the number of results is k or less\n",
    "assert len(filtered_search_results) <= 3, f\"Expected 3 or fewer results, but got {len(filtered_search_results)}\"\n",
    "print(\"\\nVerification: ✓ Number of results is correctly limited by k parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Result Similarity\n",
    "\n",
    "When performing similarity searches, you might want to know not just which documents are similar, but how similar they are. Weaviate provides this information through a relevance score.\n",
    "> 💡 Tip: The relevance score helps you understand the relative similarity between search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search_with_score(\"What monuments are in Asia?\", k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc[1]:.3f}\", \":\", doc[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`similarity_search` uses Weaviate's [hybrid search](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid).\n",
    "\n",
    "A hybrid search combines a vector and a keyword search, with `alpha` as the weight of the vector search. The `similarity_search` function allows you to pass additional arguments as kwargs. See this [reference doc](https://weaviate.io/developers/weaviate/api/graphql/search-operators#hybrid) for the available arguments.\n",
    "\n",
    "So, you can perform a pure keyword search by adding `alpha=0` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query, alpha=0)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data added through `langchain-weaviate` will persist in Weaviate according to its configuration. \n",
    "\n",
    "WCS instances, for example, are configured to persist data indefinitely, and Docker instances can be set up to persist data in a volume. Read more about [Weaviate's persistence](https://weaviate.io/developers/weaviate/configuration/persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tenancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-tenancy](https://weaviate.io/developers/weaviate/concepts/data#multi-tenancy) allows you to have a high number of isolated collections of data, with the same collection configuration, in a single Weaviate instance. This is great for multi-user environments such as building a SaaS app, where each end user will have their own isolated data collection.\n",
    "\n",
    "To use multi-tenancy, the vector store need to be aware of the `tenant` parameter. \n",
    "\n",
    "So when adding any data, provide the `tenant` parameter as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a vector store with a specific tenant\n",
    "db_with_tenant = WeaviateVectorStore.from_documents(\n",
    "    docs, \n",
    "    embeddings, \n",
    "    client=client,\n",
    "    tenant=\"tenant1\"  # specify the tenant name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db_with_tenant.similarity_search(\n",
    "    \"What is Petra?\",\n",
    "    tenant=\"tenant1\"  # use the same tenant name\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_with_mt = WeaviateVectorStore.from_documents(\n",
    "    docs, embeddings, client=client, tenant=\"tenant1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when performing queries, provide the `tenant` parameter also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_with_mt.similarity_search(query, tenant=\"tenant1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever options\n",
    "\n",
    "Weaviate can also be used as a retriever\n",
    "\n",
    "### Maximal marginal relevance search (MMR)\n",
    "\n",
    "In addition to using similaritysearch  in the retriever object, you can also use `mmr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"mmr\")\n",
    "retriever.invoke(query)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A known limitation of large language models (LLMs) is that their training data can be outdated, or not include the specific domain knowledge that you require.\n",
    "\n",
    "Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "result = llm.invoke(\"What is Eiffel Tower?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector stores complement LLMs by providing a way to store and retrieve relevant information. This allow you to combine the strengths of LLMs and vector stores, by using LLM's reasoning and linguistic capabilities with vector stores' ability to retrieve relevant information.\n",
    "\n",
    "Two well-known applications for combining LLMs and vector stores are:\n",
    "- Question answering\n",
    "- Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering with Sources\n",
    "\n",
    "Question answering in langchain can be enhanced by the use of vector stores. Let's see how this can be done.\n",
    "\n",
    "This section uses the `RetrievalQAWithSourcesChain`, which does the lookup of the documents from an Index. \n",
    "\n",
    "First, we will chunk the text again and import them into the Weaviate vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    raw_texts,\n",
    "    embeddings,\n",
    "    client=client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(raw_texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the chain, with the retriever specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    {\"question\": \"What is Eiffel Tower?\"},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "Another very popular application of combining LLMs and vector stores is retrieval-augmented generation (RAG). This is a technique that uses a retriever to find relevant information from a vector store, and then uses an LLM to provide an output based on the retrieved data and a prompt.\n",
    "\n",
    "We begin with a similar setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = WeaviateVectorStore.from_texts(\n",
    "    raw_texts,\n",
    "    embeddings,\n",
    "    client=client,\n",
    "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(raw_texts))],\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct a template for the RAG model so that the retrieved information will be populated in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Petra?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-BXw0bE1H-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
