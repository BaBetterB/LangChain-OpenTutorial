{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Chroma\n",
        "\n",
        "- Author: [Gwangwon Jung](https://github.com/pupba)\n",
        "- Design: []()\n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to use **Chroma Vector Store** with **LangChain** .\n",
        "\n",
        "`Chroma` is an **open-source AI application database** .\n",
        "\n",
        "In this tutorial, after learning how to use `langchain-chroma` , we will implement examples of a simple **Text Search** engine using `Chroma` .\n",
        "\n",
        "![search-example](./assets/02-chroma-with-langchain-flow-search-example.png)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [What is Chroma?](#what-is-chroma?)\n",
        "- [LangChain Chroma Basic](#langchain-chroma-basic)\n",
        "- [Manage Store](#manage-store)\n",
        "- [Query Vector Store](#query-vector-store)\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [Chroma Docs](https://docs.trychroma.com/docs/overview/introduction)\n",
        "- [Langchain-Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\n",
        "- [List of VectorStore supported by Langchain](https://python.langchain.com/docs/integrations/vectorstores/)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain-core\",\n",
        "        \"langchain-chroma\",\n",
        "        \"chromadb\",\n",
        "        \"langchain-text-splitters\",\n",
        "        \"langchain-huggingface\",\n",
        "        \"python-dotenv\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Chroma\",\n",
        "        \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77dbe5de",
      "metadata": {},
      "source": [
        "## What is Chroma?\n",
        "\n",
        "![logo](./assets/02-chroma-with-langchain-chroma-logo.png)\n",
        "\n",
        "`Chroma` is the **open-source vector database** designed for AI application. \n",
        "\n",
        "It specializes in storing high-dimensional vectors and performing fast similariy search, making it ideal for tasks like **semantic search** , **recommendation systems** and **multimodal search** .\n",
        "\n",
        "With its **developer-friendly APIs** and seamless integration with frameworks like **LangChain** , `Chroma` is powerful tool for building scalable, AI-driven solutions.\n",
        "\n",
        "The biggest feature of `Chroma` is that it internally **Indexing ([HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world))** and **Embedding ([all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))** are used when storing data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6d8430",
      "metadata": {},
      "source": [
        "## LangChain Chroma Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f20414",
      "metadata": {},
      "source": [
        "### Select Embedding Model\n",
        "\n",
        "We load the **Embedding Model** with `langchain_huggingface` .\n",
        "\n",
        "If you want to use a different model, use a different model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fead7517",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs={\"trust_remote_code\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13874284",
      "metadata": {},
      "source": [
        "### Create VectorDB\n",
        "\n",
        "The **library** supported by **LangChain** has no `upsert` function and lacks interface uniformity with other **Vector DBs**, so we have implemented a new **Python** class.\n",
        "\n",
        "First, Load a **Python** class from **utils/chroma/basic.py** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ec89183",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.chroma.basic import ChromaDB\n",
        "\n",
        "vector_store = ChromaDB(embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4e5e6d",
      "metadata": {},
      "source": [
        "Create `ChromaDB` object.\n",
        "\n",
        "- **Mode** : `persistent`\n",
        "\n",
        "- **Persistent Path** : `data/chroma.sqlite` (Used `SQLite` DB)\n",
        "\n",
        "- **collection** : `test`\n",
        "\n",
        "- **hnsw:space** : `cosine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "27ccfa77",
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = {\n",
        "    \"mode\": \"persistent\",\n",
        "    \"persistent_path\": \"data/chroma_text\",\n",
        "    \"collection\": \"test\",\n",
        "    \"hnsw:space\": \"cosine\",\n",
        "}\n",
        "\n",
        "vector_store.connect(**configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492fd4a1",
      "metadata": {},
      "source": [
        "### Load Text Documents Data\n",
        "\n",
        "In this tutorial, we will use the **A Little Prince** fairy tale document.\n",
        "\n",
        "To put this data in **Chroma** ,we will do data preprocessing first.\n",
        "\n",
        "First of all, we will load the `data/the_little_prince.txt` file that extracted only the text of the fairy tale document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dd04292",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If your \"OS\" is \"Windows\", add 'encoding=utf-8' to the open function\n",
        "with open(\"./data/the_little_prince.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1534b127",
      "metadata": {},
      "source": [
        "Second, chunking the text imported into the `RecursiveCharacterTextSplitter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3b4b30b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: The Little Prince\n",
            "Written By Antoine de Saiot-Exupery (1900〜1944)\n",
            "Metadata: {}\n",
            "\n",
            "Content: [ Antoine de Saiot-Exupery ]\n",
            "Metadata: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.create_documents([raw_text])\n",
        "\n",
        "for docs in split_docs[:2]:\n",
        "    print(f\"Content: {docs.page_content}\\nMetadata: {docs.metadata}\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558e5fe7",
      "metadata": {},
      "source": [
        "Preprocessing document for **Chroma** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ed0952fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_dosc = vector_store.preprocess_documents(\n",
        "    documents=split_docs,\n",
        "    source=\"The Little Prince\",\n",
        "    author=\"Antoine de Saint-Exupéry\",\n",
        "    chapter=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7d92d3bc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': '57623d83-6989-4288-aef9-4a73a9dd4e15'}, page_content='- we are introduced to the narrator, a pilot, and his ideas about grown-ups'),\n",
              " Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': 'f2828b20-b00f-4771-80dd-13d3b7e76a87'}, page_content='Once when I was six years old I saw a magnificent picture in a book, called True Stories from')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68dfd9a",
      "metadata": {},
      "source": [
        "## Manage Store\n",
        "\n",
        "This section introduces four basic functions.\n",
        "\n",
        "- `add`\n",
        "\n",
        "- `upsert(parallel)`\n",
        "\n",
        "- `query`\n",
        "\n",
        "- `delete`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80a3015",
      "metadata": {},
      "source": [
        "### Add\n",
        "\n",
        "Add the new **Documents** .\n",
        "\n",
        "An error occurs if you have the same **ID** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "725bbe68",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f800cb49",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['57623d83-6989-4288-aef9-4a73a9dd4e15',\n",
              " 'f2828b20-b00f-4771-80dd-13d3b7e76a87']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uids = list(vector_store.unique_ids)\n",
        "uids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "68e78f78",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['57623d83-6989-4288-aef9-4a73a9dd4e15'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['- we are introduced to the narrator, a pilot, and his ideas about grown-ups'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.chroma.get(ids=uids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb0babb",
      "metadata": {},
      "source": [
        "Error occurs when trying to `add` duplicate `ids` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3d57fac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Add of existing embedding ID: 57623d83-6989-4288-aef9-4a73a9dd4e15\n",
            "Add of existing embedding ID: f2828b20-b00f-4771-80dd-13d3b7e76a87\n",
            "Insert of existing embedding ID: 57623d83-6989-4288-aef9-4a73a9dd4e15\n",
            "Insert of existing embedding ID: f2828b20-b00f-4771-80dd-13d3b7e76a87\n"
          ]
        }
      ],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63a62b3",
      "metadata": {},
      "source": [
        "### Upsert(parallel)\n",
        "\n",
        "`Upsert` will `Update` a document or `Add` a new document if the same `ID` exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4a9ca529",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['57623d83-6989-4288-aef9-4a73a9dd4e15',\n",
              "  'f2828b20-b00f-4771-80dd-13d3b7e76a87'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['- we are introduced to the narrator, a pilot, and his ideas about grown-ups',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8edc27d3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': '57623d83-6989-4288-aef9-4a73a9dd4e15'}, page_content='Changed Content')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[0].page_content = \"Changed Content\"\n",
        "pre_dosc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d0086c07",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['57623d83-6989-4288-aef9-4a73a9dd4e15',\n",
              "  'f2828b20-b00f-4771-80dd-13d3b7e76a87'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['Changed Content',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': '57623d83-6989-4288-aef9-4a73a9dd4e15',\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': 'f2828b20-b00f-4771-80dd-13d3b7e76a87',\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.upsert_documents(\n",
        "    documents=pre_dosc[:2],\n",
        ")\n",
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d743a327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# parallel upsert\n",
        "vector_store.upsert_documents_parallel(\n",
        "    documents=pre_dosc,\n",
        "    batch_size=32,\n",
        "    max_workers=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d515ad98",
      "metadata": {},
      "source": [
        "## Query Vector Store\n",
        "\n",
        "There are two ways to **Query** the **LangChain Chroma Vector Store** .\n",
        "\n",
        "- **Directly** : Query the vector store directly using methods like `similarity_search` or `similarity_search_with_score` .\n",
        "\n",
        "- **Turning into retriever** : Convert the vector store into a **retriever** object, which can be used in **LangChain** pipelines or chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37c37fb",
      "metadata": {},
      "source": [
        "### Query\n",
        "\n",
        "This method is created by wrapping the methods of the `langchain-chroma` .\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `query:str` - Query text to search for.\n",
        "\n",
        "- `k:int = DEFAULT_K` - Number of results to return. Defaults to 4.\n",
        "\n",
        "- `filter: Dict[str, str] | None = None` - Filter by metadata. Defaults to None.\n",
        "\n",
        "- `where_document: Dict[str, str] | None = None` - dict used to filter by the documents. E.g. {$contains: {\"text\": \"hello\"}}.\n",
        "\n",
        "- `**kwargs:Any` : Additional keyword arguments to pass to Chroma collection query.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "- `List[Document]` - List of documents most similar to the query text and distance in float for each. Lower score represents more similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99da0a7b",
      "metadata": {},
      "source": [
        "**Simple Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b77b3f7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 62dbbc31-f4be-4d11-bbbc-282e8c1e522b\n",
            "Chapter: 7\n",
            "Page Content: prince disturbed my thoughts.\n",
            "\n",
            "ID: 09f7204d-ccc7-4f2a-b767-440fe3c68ee5\n",
            "Chapter: 6\n",
            "Page Content: Oh, little prince! Bit by bit I came to understand the secrets of your sad little life... For a\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vector_store.query(query=\"Prince\", top_k=2)\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.metadata[\"id\"])\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03207993",
      "metadata": {},
      "source": [
        "**Filtering Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a63c9365",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: b38c0471-f74b-4fa6-a9c9-872b01fd87bb\n",
            "Chapter: 20\n",
            "Page Content: snow, the little prince at last came upon a road. And all roads lead to the abodes of men.\n",
            "\n",
            "ID: 02092b04-eeb3-496e-885c-174e0f864a80\n",
            "Chapter: 20\n",
            "Page Content: extinct forever... that doesn‘t make me a very great prince...\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vector_store.query(query=\"Prince\", top_k=2, filters={\"chapter\": 20})\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.metadata[\"id\"])\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ad001e",
      "metadata": {},
      "source": [
        "**Cosine Similarity Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "04e592b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: b38c0471-f74b-4fa6-a9c9-872b01fd87bb\n",
            "Chapter: 20\n",
            "Page Content: snow, the little prince at last came upon a road. And all roads lead to the abodes of men.\n",
            "Similarity Score: 60.0%\n",
            "\n",
            "ID: 02092b04-eeb3-496e-885c-174e0f864a80\n",
            "Chapter: 20\n",
            "Page Content: extinct forever... that doesn‘t make me a very great prince...\"\n",
            "Similarity Score: 54.0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cosine Similarity\n",
        "results = vector_store.query(query=\"Prince\", top_k=2, cs=True, filters={\"chapter\": 20})\n",
        "\n",
        "for doc, score in results:\n",
        "    print(\"ID:\", doc.metadata[\"id\"])\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print(f\"Similarity Score: {round(score,2)*100:.1f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3de5b67",
      "metadata": {},
      "source": [
        "### as_retriever()\n",
        "\n",
        "The `as_retriever()` method converts a `VectorStore` object into a `Retriever` object.\n",
        "\n",
        "A `Retriever` is an interface used in `LangChain` to query a vector store and retrieve relevant documents.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `search_type:Optional[str]` - Defines the type of search that the Retriever should perform. Can be `similarity` (default), `mmr` , or `similarity_score_threshold`\n",
        "\n",
        "- `search_kwargs:Optional[Dict]` - Keyword arguments to pass to the search function. \n",
        "\n",
        "    Can include things like:\n",
        "\n",
        "    `k` : Amount of documents to return (Default: 4)\n",
        "\n",
        "    `score_threshold` : Minimum relevance threshold for similarity_score_threshold\n",
        "\n",
        "    `fetch_k` : Amount of documents to pass to `MMR` algorithm(Default: 20)\n",
        "        \n",
        "    `lambda_mult` : Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "\n",
        "    `filter` : Filter by document metadata\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `VectorStoreRetriever` - Retriever class for VectorStore.\n",
        "\n",
        "\n",
        "### invoke()\n",
        "\n",
        "Invoke the retriever to get relevant documents.\n",
        "\n",
        "Main entry point for synchronous retriever invocations.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `input:str` - The query string.\n",
        "- `config:RunnableConfig | None = None` - Configuration for the retriever. Defaults to None.\n",
        "- `**kwargs:Any` - Additional arguments to pass to the retriever.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `List[Document]` : List of relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d8e2e184",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "client = Chroma(\n",
        "    collection_name=\"test\",\n",
        "    persist_directory=\"data/chroma_text\",\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "    embedding_function=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "600b208a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 63c0f702-49d4-411e-beab-e18dbf2543ff\n",
            "Chapter: 5\n",
            "Page Content: Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets--\n",
            "\n",
            "ID: 2d4abb40-f615-4f22-8183-066e8a43f317\n",
            "Chapter: 5\n",
            "Page Content: Now there were some terrible seeds on the planet that was the home of the little prince; and these\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retriever = client.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "docs = retriever.invoke(\"Prince\", filter={\"chapter\": 5})\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.id)\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388f5512",
      "metadata": {},
      "source": [
        "### Delete\n",
        "\n",
        "`Delete` the Documents.\n",
        "\n",
        "You can use with `filter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a3c4af1e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1317"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6cc8e842",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len([docs for docs in pre_dosc if docs.metadata[\"chapter\"] == 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a3c99973",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 43 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(\n",
        "    unique_ids=list(vector_store.unique_ids), filters={\"chapter\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7349a83b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1274"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "70991cfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 1274 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(unique_ids=list(vector_store.unique_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b53c861e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f53e7c",
      "metadata": {},
      "source": [
        "Remove a **Huggingface Cache** , `vector_store` , `embeddings` and `client` .\n",
        "\n",
        "If you created a **vectordb** directory, please **remove** it at the end of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1ffecd99",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeleteCacheStrategy(expected_freed_size=0, blobs=frozenset(), refs=frozenset(), repos=frozenset(), snapshots=frozenset())"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import scan_cache_dir\n",
        "\n",
        "del embeddings\n",
        "del vector_store\n",
        "del client\n",
        "scan = scan_cache_dir()\n",
        "scan.delete_revisions()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-B290FrwJ-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
